{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "from ast import literal_eval\n",
    "\n",
    "# For Python2 this have to be done\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Original Data\n",
    "\n",
    "dataset_name = \"beer\"\n",
    "# dataset_name = \"CDsVinyl\"\n",
    "# df_train = pd.read_csv('../../data/'+ dataset_name +'/Train.csv')\n",
    "# df_valid = pd.read_csv('../../data/'+ dataset_name +'/Valid.csv')\n",
    "# df_test = pd.read_csv('../../data/'+ dataset_name +'/Test.csv')\n",
    "key_phrase = pd.read_csv('../data/'+ dataset_name +'/KeyPhrases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load U-I Data \n",
    "rtrain = load_npz(\"../../\"+ dataset_name +\"/Rtrain.npz\")\n",
    "# rvalid = load_npz(\"../../data/\"+ dataset_name +\"/Rvalid.npz\")\n",
    "# rtest = load_npz(\"../../data/\"+ dataset_name +\"/Rtest.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train, model = \"Cosine_similarity\"):\n",
    "    if model == \"Cosine_similarity\":\n",
    "        similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, row_name = 'ItemIndex', shape = (3668,75)):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        key_vector = literal_eval(df['keyVector'][i])\n",
    "        rows.extend([df[row_name][i]]*len(key_vector)) ## Item index\n",
    "        cols.extend(key_vector) ## Keyword Index\n",
    "#         if binary:\n",
    "        vals.extend(np.array([1]*len(key_vector)))\n",
    "#         else:\n",
    "#             vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    \"\"\"\n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    \"\"\"\n",
    "    prediction_scores = []\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores to all users\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "\n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        vector_u = prediction_score[user_index]\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "\n",
    "    vector_u = vector_u\n",
    "\n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    return vector_u[:topK]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        \"R-Precision\": r_precision,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = results[name]\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (np.average(results[name]),\n",
    "                                                              1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = results[name]\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (np.average(results[name]), 1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explain(R,W2,k, model = \"Cosine_similarity\"):\n",
    "    \"\"\"\n",
    "    R: Rating Matrix with size U*I\n",
    "    r_ij: observed rating with user i and item j \n",
    "    s_ij: explanation vector with user i and item j \n",
    "    Z: Joint Embedding/Latent Space with size U*U, generate r_ij and s_ij\n",
    "    W2: Reconstruction matrix with size U*K \n",
    "    S: Output explanation prediction matrix with size U*K\n",
    "    \"\"\"\n",
    "    Z = train(R) # Cosine similarity as default\n",
    "    S = predict(W2, k, Z) \n",
    "    return normalize(S)\n",
    "def  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
