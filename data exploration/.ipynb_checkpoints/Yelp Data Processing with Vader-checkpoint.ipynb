{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Data Packages\n",
    "import math\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Counter\n",
    "from collections import Counter\n",
    "\n",
    "#Operation\n",
    "import operator\n",
    "\n",
    "#Natural Language Processing Packages\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "## Download Resources\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "\n",
    "## Machine Learning\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use vader to evaluated sentiment of reviews (vader score for each entire review)\n",
    "def evalSentences(sentences, to_df=False, columns=[]):\n",
    "    #Instantiate an instance to access SentimentIntensityAnalyzer class\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    pdlist = []\n",
    "    if to_df:\n",
    "        for sentence in tqdm(sentences):\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            pdlist.append([sentence]+[ss['compound']])\n",
    "        reviewDf = pandas.DataFrame(pdlist)\n",
    "        reviewDf.columns = columns\n",
    "        return reviewDf\n",
    "    \n",
    "    else:\n",
    "        for sentence in tqdm(sentences):\n",
    "            print(sentence)\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            for k in sorted(ss):\n",
    "                print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.0, 'neg': 0.0, 'neu': 1.0, 'pos': 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'quick service'\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "ss = sid.polarity_scores(sentence)\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur_df = pd.read_csv('../../data/yelp/Data.csv', index_col=0,  encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = cur_df['review_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 157038/157038 [04:47<00:00, 546.65it/s]\n"
     ]
    }
   ],
   "source": [
    "reviewDF = evalSentences(reviews, to_df=True, columns=['reviewCol','vader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_df['Binary_vader'] = (reviewDF['vader']> 0.05)*[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save\n",
    "reviewDF.to_csv('../../data/yelp/reviewDF.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load reviewDF\n",
    "reviewDF = pd.read_csv('../../data/yelp/reviewDF.csv', index_col=0,  encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewCol</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I ordered the lemon mango slush and the lemon ...</td>\n",
       "      <td>0.9758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Came here on a Sunday afternoon, it wasn't bus...</td>\n",
       "      <td>0.9763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{Grapefruit &amp; Yakult Green Tea with Aloe Jelly...</td>\n",
       "      <td>0.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saw this newly opened bubble tea shop and want...</td>\n",
       "      <td>0.9898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy Lemon has become my new favourite place ...</td>\n",
       "      <td>0.9922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reviewCol   vader\n",
       "0  I ordered the lemon mango slush and the lemon ...  0.9758\n",
       "1  Came here on a Sunday afternoon, it wasn't bus...  0.9763\n",
       "2  {Grapefruit & Yakult Green Tea with Aloe Jelly...  0.9170\n",
       "3  Saw this newly opened bubble tea shop and want...  0.9898\n",
       "4  Happy Lemon has become my new favourite place ...  0.9922"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_df['vader'] = reviewDF['vader']\n",
    "# Get vader binary values\n",
    "cur_df['Binary_vader'] = (reviewDF['vader'] > 0.05)*[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find most sentiment words by Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocess(text):\n",
    "\n",
    "    text = text.replace('.',' ').replace('/t',' ').replace('\\t',' ').replace('/',' ').replace('-',' ')\n",
    "    \n",
    "    # Tokenize\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = [w.lower() for w in text]\n",
    "\n",
    "    # Remove Punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text]\n",
    "\n",
    "    # Remove tokens that are not alphabetic\n",
    "    text = [w for w in text if w.isalpha()]\n",
    "\n",
    "    # Remove Stopwords\n",
    "    # Get english stopwords\n",
    "    en_stopwords = set(stopwords.words('english'))\n",
    "    en_stopwords.remove('off')\n",
    "    text = [w for w in text if w not in en_stopwords]\n",
    "    \n",
    "    # Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "    text = \" \" + \" \".join(str(x) for x in text) + \" \"\n",
    "\n",
    "    text = text.replace('whitish', 'white')\n",
    "    text = text.replace('bisquity', ' biscuit ')\n",
    "    text = text.replace('carmel', ' caramel ')\n",
    "    text = text.replace('flower', ' floral ')\n",
    "    text = text.replace('piny', ' pine ')\n",
    "    text = text.replace('off white', 'offwhite')\n",
    "    text = text.replace('goden', 'gold')\n",
    "    text = text.replace('yello', 'yellow')\n",
    "    text = text.replace('reddish', ' red ') \n",
    "    text = text.replace('favorite', 'favourite ') \n",
    "    \n",
    "\n",
    "    # Reset to token\n",
    "    text = nltk.word_tokenize(text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text]\n",
    "    text = [w for w in text if w.isalpha()]\n",
    "#     en_stopwords = set(stopwords.words('english'))\n",
    "    text = [w for w in text if w not in en_stopwords]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(w) for w in text]\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    return ' '.join(str(x) for x in preprocess(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Mutual Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Top K mutual information terms from the dataframe\n",
    "def getMI(topk, df, label_column='groundTruth'):\n",
    "    miScore = []\n",
    "    for word in topk:\n",
    "        miScore.append([word[0]]+[metrics.mutual_info_score(finaldf[label_column], finaldf[word[0]])])\n",
    "    miScoredf = pandas.DataFrame(miScore).sort_values(1,ascending=0)\n",
    "    miScoredf.columns = ['Word','MI Score']\n",
    "    return miScoredf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"\"\"The Buddha, the Godhead, resides quite as comfortably in the circuits of a digital\n",
    "computer or the gears of a cycle transmission as he does at the top of a mountain\n",
    "or in the petals of a flower. To think otherwise is to demean the Buddha...which is\n",
    "to demean oneself.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Buddha', 'NNP'),\n",
       " ('the', 'DT'),\n",
       " ('Godhead', 'NNP'),\n",
       " ('resides', 'VBZ'),\n",
       " ('quite', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('comfortably', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('circuits', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('digital', 'JJ'),\n",
       " ('computer', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('gears', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('cycle', 'NN'),\n",
       " ('transmission', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('does', 'VBZ'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('top', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('mountain', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('petals', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('flower', 'NN'),\n",
       " ('To', 'TO'),\n",
       " ('think', 'VB'),\n",
       " ('otherwise', 'RB'),\n",
       " ('is', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('demean', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Buddha', 'NNP'),\n",
       " ('which', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('demean', 'VB'),\n",
       " ('oneself', 'PRP')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = PerceptronTagger()\n",
    "# Part of Speech Tagging\n",
    "# Google: https://en.wikipedia.org/wiki/Part-of-speech_tagging\n",
    "pos_tag = tagger.tag\n",
    "taggedToks = pos_tag(re.findall(r'\\w+', text))\n",
    "taggedToks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This grammar is described in the paper by S. N. Kim,\n",
    "# T. Baldwin, and M.-Y. Kan.\n",
    "# Evaluating n-gram based evaluation metrics for automatic\n",
    "# keyphrase extraction.\n",
    "# Technical report, University of Melbourne, Melbourne 2010.\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create phrase tree\n",
    "chunker = nltk.RegexpParser(grammar)\n",
    "tree= chunker.parse(taggedToks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Noun Phrase Extraction Support Functions\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "# generator, generate leaves one by one\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP' or t.label()=='JJ' or t.label()=='RB'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "# stemming, lematizing, lower case... \n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = stemmer.stem(word)\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "# stop-words and length control\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "# generator, create item once a time\n",
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "#         term = [normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        term = [w for w,t in leaf if acceptable_word(w) ] # without normalise\n",
    "        # Phrase only\n",
    "        if len(term)>1:\n",
    "            yield term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['digital', 'computer'], ['cycle', 'transmission']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traverse tree and get noun phrases\n",
    "npTokenList = [word for word in get_terms(tree)]\n",
    "\n",
    "npTokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten phrase lists to get tokens for analysis\n",
    "def flatten(npTokenList):\n",
    "    finalList =[]\n",
    "    for phrase in npTokenList:\n",
    "        token = ''\n",
    "        for word in phrase:\n",
    "            token += word + ' '\n",
    "#         finalList.append(token.rstrip())\n",
    "        finalList.append(token)\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalList = flatten(npTokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['digital computer ', 'cycle transmission ']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "# Revise the previous dataframe transform function...\n",
    "def newDataFrameTransformation(hotelDf, reviewDF, reviewcol_name = 'conca_review',k= 1000):\n",
    "#     reviews = reviewDF[reviewcol_name].values\n",
    "    reviews = hotelDf[reviewcol_name].values\n",
    "    \n",
    "    # Top-k frequent terms\n",
    "    counter = Counter()\n",
    "    for review in tqdm(reviews):\n",
    "        counter.update(flatten([word\n",
    "                            for word \n",
    "                            in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+', review)))) \n",
    "                            ]))\n",
    "    topk = counter.most_common(k)        \n",
    "    \n",
    "    #Find out if a particular review has the word from topk list\n",
    "    freqReview = []\n",
    "    for i in tqdm(range(len(reviews))):\n",
    "        tempCounter = Counter(flatten([word \n",
    "                                       for word \n",
    "                                       in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+',reviews[i]))))]))\n",
    "        topkinReview = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "        freqReview.append(topkinReview)\n",
    "         \n",
    "    #Prepare freqReviewDf\n",
    "    freqReviewDf = pandas.DataFrame(freqReview)\n",
    "    dfName = []\n",
    "    for c in topk:\n",
    "        dfName.append(c[0])\n",
    "    freqReviewDf.columns = dfName\n",
    "    finalreviewDf = reviewDF.join(freqReviewDf)\n",
    "    finaldf = hotelDf[['business_id','rating','Binary','review','conca_review','UserIndex','ItemIndex']].join(finalreviewDf)\n",
    "    return topk, finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topk_phrase' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-9ee19ff72457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Compute PMI for all terms and all possible labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mpmiForAllCal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_column\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Binary'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopk_phrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m#Try calculate all the pmi for top k and store them into one pmidf dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpmilist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpmiposlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'topk_phrase' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute PMI for all terms and all possible labels\n",
    "def pmiForAllCal(df, label_column='Binary', topk=topk_phrase):\n",
    "    #Try calculate all the pmi for top k and store them into one pmidf dataframe\n",
    "    pmilist = []\n",
    "    pmiposlist = []\n",
    "    pmineglist = []\n",
    "    for word in tqdm(topk):\n",
    "#         pmilist.append([word[0]]+[pmiCal(df,word[0])])\n",
    "        pmiposlist.append([word[0]]+[pmiIndivCal(df,word[0],1,label_column)])\n",
    "        pmineglist.append([word[0]]+[pmiIndivCal(df,word[0],0,label_column)])\n",
    "    pmidf = pandas.DataFrame(pmilist)\n",
    "    pmiposlist = pandas.DataFrame(pmiposlist)\n",
    "    pmineglist = pandas.DataFrame(pmineglist)\n",
    "    pmiposlist.columns = ['word','pmi']\n",
    "    pmineglist.columns = ['word','pmi']\n",
    "#     pmidf.columns = ['word','pmi']\n",
    "#     return pmiposlist, pmineglist, pmidf\n",
    "    return pmiposlist, pmineglist\n",
    "\n",
    "def pmiIndivCal(df,x,gt, label_column='Binary'):\n",
    "    px = sum(df[label_column]==gt)/len(df)\n",
    "    py = sum(df[x]==1)/len(df)\n",
    "    pxy = len(df[(df[label_column]==gt) & (df[x]==1)])/len(df)\n",
    "    if pxy==0:#Log 0 cannot happen\n",
    "        pmi = math.log((pxy+0.0001)/(px*py))\n",
    "    else:\n",
    "        pmi = math.log(pxy/(px*py))\n",
    "    return pmi\n",
    "\n",
    "# Simple example of getting pairwise mutual information of a term\n",
    "def pmiCal(df, x):\n",
    "    pmilist=[]\n",
    "    for i in [1,0]:\n",
    "        for j in [0,1]:\n",
    "            px = sum(df['Binary']==i)/len(df)\n",
    "            py = sum(df[x]==j)/len(df)\n",
    "            pxy = len(df[(df['Binary']==i) & (df[x]==j)])/len(df)\n",
    "            if pxy==0:#Log 0 cannot happen\n",
    "                pmi = math.log((pxy+0.0001)/(px*py))\n",
    "            else:\n",
    "                pmi = math.log(pxy/(px*py))\n",
    "            pmilist.append([i]+[j]+[px]+[py]+[pxy]+[pmi])\n",
    "    pmidf = pandas.DataFrame(pmilist)\n",
    "    pmidf.columns = ['x','y','px','py','pxy','pmi']\n",
    "    return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topk_phrase, finaldf_phrase = newDataFrameTransformation(cur_df, reviewDF, k = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topk_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PMI based on ground-truth rating\n",
    "pmiposlist, pmineglist = pmiForAllCal(finaldf_phrase, topk=topk_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PMI based on vader sentiment analysis on review texts. (entire review)\n",
    "pmiposlist_vader, pmineglist_vader = pmiForAllCal(finaldf_phrase, topk=topk_phrase,label_column='Binary_vader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save \n",
    "import pickle as pkl\n",
    "with open(\"pmineglist\", \"wb\") as output_file:\n",
    "    pkl.dump(pmineglist, output_file)\n",
    "with open(\"pmiposlist\", \"wb\") as output_file:\n",
    "    pkl.dump(pmiposlist, output_file)\n",
    "with open(\"pmineglist_vader\", \"wb\") as output_file:\n",
    "    pkl.dump(pmineglist_vader, output_file)\n",
    "with open(\"pmiposlist_vader\", \"wb\") as output_file:\n",
    "    pkl.dump(pmiposlist_vader, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# topk_phrase, finaldf_phrase\n",
    "with open(\"finaldf_phrase\", \"wb\") as output_file:\n",
    "    pkl.dump(finaldf_phrase, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top positive/negative keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_rating = pmineglist.sort_values('pmi',ascending=0)\n",
    "neg_rating = neg_rating['word'][:50].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad place',\n",
       " 'okay noth',\n",
       " 'decent place',\n",
       " 'ok noth',\n",
       " 'decent food',\n",
       " 'second chanc',\n",
       " 'terribl servic',\n",
       " 'mediocr food',\n",
       " 'decent servic',\n",
       " 'eye contact',\n",
       " 'sub par',\n",
       " 'slow servic',\n",
       " 'high hope',\n",
       " 'dri side',\n",
       " 'bit bland',\n",
       " 'separ bill',\n",
       " 'high price',\n",
       " 'empti tabl',\n",
       " 'poor servic',\n",
       " 'room temperatur',\n",
       " 'littl bland',\n",
       " 'good dish',\n",
       " 'bad tast',\n",
       " 'averag price',\n",
       " 'asian legend',\n",
       " 'quick meal',\n",
       " 'good overal',\n",
       " 'bad servic',\n",
       " 'salti side',\n",
       " 'high side',\n",
       " 'swiss chalet',\n",
       " 'non exist',\n",
       " 'plu side',\n",
       " 'extra star',\n",
       " 'wow factor',\n",
       " 'long wait time',\n",
       " 'bad day',\n",
       " 'dim sum place',\n",
       " 'bit pricey',\n",
       " 'instant noodl',\n",
       " 'chicken piec',\n",
       " 'good locat',\n",
       " 'small portion',\n",
       " 'beef noodl',\n",
       " 'good place',\n",
       " 'much sauc',\n",
       " 'decent portion',\n",
       " 'good noth',\n",
       " 'deer garden',\n",
       " 'la carnita']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_rating = pmiposlist.sort_values('pmi',ascending=0)\n",
    "pos_rating = pos_rating['word'][:50].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amaz experi',\n",
       " 'favourit restaur',\n",
       " 'amaz servic',\n",
       " 'wonder experi',\n",
       " 'amaz food',\n",
       " 'great recommend',\n",
       " 'favorit place',\n",
       " 'hidden gem',\n",
       " 'hair cut',\n",
       " 'perfect balanc',\n",
       " 'favourit place',\n",
       " 'littl gem',\n",
       " 'much fun',\n",
       " 'favourit spot',\n",
       " 'escap room',\n",
       " 'super help',\n",
       " 'great coffe',\n",
       " 'real deal',\n",
       " 'great custom servic',\n",
       " 'person favourit',\n",
       " 'great qualiti',\n",
       " 'top notch',\n",
       " 'tast menu',\n",
       " 'delici food',\n",
       " 'great job',\n",
       " 'super nice',\n",
       " 'awesom place',\n",
       " 'great experi',\n",
       " 'delici meal',\n",
       " 'excel servic',\n",
       " 'perfect amount',\n",
       " 'singl time',\n",
       " 'mani flavour',\n",
       " 'great staff',\n",
       " 'nail salon',\n",
       " 'farmer market',\n",
       " 'bang bang',\n",
       " 'sure everyth',\n",
       " 'perfect place',\n",
       " 'sushi bar',\n",
       " 'favourit thing',\n",
       " 'great servic',\n",
       " 'vietnames coffe',\n",
       " 'special occas',\n",
       " 'great price',\n",
       " 'good reason',\n",
       " 'foie gra',\n",
       " 'friendli staff',\n",
       " 'board game',\n",
       " 'filet mignon']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect-Oriented Table for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos_rating' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-16bfc67f451d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Where does the keyword rank in the pos/neg keyphrase list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtemp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_rating\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fast'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpos_rating\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_rating' is not defined"
     ]
    }
   ],
   "source": [
    "# Where does the keyword rank in the pos/neg keyphrase list\n",
    "temp1 = np.where(pos_rating['word'].str.contains('fast'))\n",
    "pos_rating.loc[temp1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>business_id</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>ghost</th>\n",
       "      <th>img_dsc</th>\n",
       "      <th>img_url</th>\n",
       "      <th>nr</th>\n",
       "      <th>photo_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>...</th>\n",
       "      <th>Day</th>\n",
       "      <th>Binary</th>\n",
       "      <th>review</th>\n",
       "      <th>conca_review</th>\n",
       "      <th>keyVector</th>\n",
       "      <th>keyphrases_indices_length</th>\n",
       "      <th>UserIndex</th>\n",
       "      <th>ItemIndex</th>\n",
       "      <th>Binary_vader</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>637</td>\n",
       "      <td>637</td>\n",
       "      <td>gISrxk4A5dfrjDivkC-L-Q</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>293.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>['douce', 'france', 'nice', 'french', 'place',...</td>\n",
       "      <td>douce france nice french place decoration bea...</td>\n",
       "      <td>[22, 38, 77, 150, 167, 172, 179]</td>\n",
       "      <td>7</td>\n",
       "      <td>102</td>\n",
       "      <td>5150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>693</td>\n",
       "      <td>693</td>\n",
       "      <td>gISrxk4A5dfrjDivkC-L-Q</td>\n",
       "      <td>53</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Douce France - Toronto, ON, Canada'...</td>\n",
       "      <td>['https://s3-media2.fl.yelpcdn.com/bphoto/yPXm...</td>\n",
       "      <td>False</td>\n",
       "      <td>602.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>['went', 'brunch', 'sunday', 'pre', 'set', 'sn...</td>\n",
       "      <td>went brunch sunday pre set snack came croissa...</td>\n",
       "      <td>[31, 38, 47, 49, 53, 67, 148, 172]</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>5150</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>773</td>\n",
       "      <td>773</td>\n",
       "      <td>5C57zUQdzvNrCus8JBawmQ</td>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Magic Noodle - North York, ON, Cana...</td>\n",
       "      <td>['https://s3-media4.fl.yelpcdn.com/bphoto/QJdO...</td>\n",
       "      <td>False</td>\n",
       "      <td>471.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['build', 'noodle', 'availablecons', 'broth', ...</td>\n",
       "      <td>build noodle availablecons broth way saltyif ...</td>\n",
       "      <td>[0, 4, 9, 52, 54, 77, 111, 172, 175, 206]</td>\n",
       "      <td>10</td>\n",
       "      <td>650</td>\n",
       "      <td>716</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1578</td>\n",
       "      <td>1578</td>\n",
       "      <td>PxH02Eu2Z4MUycBLU80D8g</td>\n",
       "      <td>175</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>['ate', 'due', 'grupon', 'coupon', 'purchased'...</td>\n",
       "      <td>ate due grupon coupon purchased bit far west ...</td>\n",
       "      <td>[5, 19, 21, 56, 83, 94, 105, 111, 153, 172, 17...</td>\n",
       "      <td>13</td>\n",
       "      <td>1745</td>\n",
       "      <td>3105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>1608</td>\n",
       "      <td>1608</td>\n",
       "      <td>PxH02Eu2Z4MUycBLU80D8g</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['went', 'restaurant', 'winterlicious', 'pleas...</td>\n",
       "      <td>went restaurant winterlicious pleasantly surp...</td>\n",
       "      <td>[17, 19, 56, 59, 112, 158, 166, 167, 172, 187]</td>\n",
       "      <td>10</td>\n",
       "      <td>261</td>\n",
       "      <td>3105</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.1  Unnamed: 0.1.1             business_id  friend_count  \\\n",
       "147           637             637  gISrxk4A5dfrjDivkC-L-Q            50   \n",
       "163           693             693  gISrxk4A5dfrjDivkC-L-Q            53   \n",
       "189           773             773  5C57zUQdzvNrCus8JBawmQ           120   \n",
       "303          1578            1578  PxH02Eu2Z4MUycBLU80D8g           175   \n",
       "304          1608            1608  PxH02Eu2Z4MUycBLU80D8g             1   \n",
       "\n",
       "     ghost                                            img_dsc  \\\n",
       "147  False                                                 []   \n",
       "163  False  ['Photo of Douce France - Toronto, ON, Canada'...   \n",
       "189  False  ['Photo of Magic Noodle - North York, ON, Cana...   \n",
       "303  False                                                 []   \n",
       "304  False                                                 []   \n",
       "\n",
       "                                               img_url     nr  photo_count  \\\n",
       "147                                                 []  False        293.0   \n",
       "163  ['https://s3-media2.fl.yelpcdn.com/bphoto/yPXm...  False        602.0   \n",
       "189  ['https://s3-media4.fl.yelpcdn.com/bphoto/QJdO...  False        471.0   \n",
       "303                                                 []  False         22.0   \n",
       "304                                                 []  False         24.0   \n",
       "\n",
       "     rating   ...    Day Binary  \\\n",
       "147     3.0   ...     11      0   \n",
       "163     4.0   ...      3      0   \n",
       "189     3.0   ...      1      0   \n",
       "303     5.0   ...      5      1   \n",
       "304     5.0   ...      2      1   \n",
       "\n",
       "                                                review  \\\n",
       "147  ['douce', 'france', 'nice', 'french', 'place',...   \n",
       "163  ['went', 'brunch', 'sunday', 'pre', 'set', 'sn...   \n",
       "189  ['build', 'noodle', 'availablecons', 'broth', ...   \n",
       "303  ['ate', 'due', 'grupon', 'coupon', 'purchased'...   \n",
       "304  ['went', 'restaurant', 'winterlicious', 'pleas...   \n",
       "\n",
       "                                          conca_review  \\\n",
       "147   douce france nice french place decoration bea...   \n",
       "163   went brunch sunday pre set snack came croissa...   \n",
       "189   build noodle availablecons broth way saltyif ...   \n",
       "303   ate due grupon coupon purchased bit far west ...   \n",
       "304   went restaurant winterlicious pleasantly surp...   \n",
       "\n",
       "                                             keyVector  \\\n",
       "147                   [22, 38, 77, 150, 167, 172, 179]   \n",
       "163                 [31, 38, 47, 49, 53, 67, 148, 172]   \n",
       "189          [0, 4, 9, 52, 54, 77, 111, 172, 175, 206]   \n",
       "303  [5, 19, 21, 56, 83, 94, 105, 111, 153, 172, 17...   \n",
       "304     [17, 19, 56, 59, 112, 158, 166, 167, 172, 187]   \n",
       "\n",
       "    keyphrases_indices_length UserIndex ItemIndex  Binary_vader   vader  \n",
       "147                         7       102      5150             1  0.8706  \n",
       "163                         8         9      5150             1  0.9818  \n",
       "189                        10       650       716             1  0.9742  \n",
       "303                        13      1745      3105             1  0.9860  \n",
       "304                        10       261      3105             1  0.9769  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First find the reviews that contains the word \n",
    "key_word = 'comfortable'\n",
    "target_df = cur_df['review_text'].loc[np.where(cur_df['review_text'].str.contains(key_word))]\n",
    "target_df_index = np.where(cur_df['review_text'].str.contains(key_word))[0]\n",
    "cur_df.loc[target_df_index].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This bakery cafe definitely gets an ooh la la! Simply the best croissants in the city. So what if they're not made locally? They're made in France with French flour and butter. You can't get better than that! Baked fresh every day, so delicious! Their coffee is excellent \\xa0and their hot chocolate Angelina is to die for. Also tried their Parisian sandwich made with a real demi-baguette, ham, and brie; simple and good.Inside the cafe you feel transported to France because of the decor and because the staff speaks French. So many goodies to try! I will have to come back. Merci!!\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check one of the review text from above\n",
    "cur_df['review_text'][164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-571892bcb225>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# How many reviews contains this keyword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_df_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
     ]
    }
   ],
   "source": [
    "# How many reviews contains this keyword\n",
    "len(target_df_index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It took 12 min after sitting down at the table in a normal not so busy restaurant before the server came through to ask for H2O or order. 12 min. The server was quite good after but 12min! C'mon. Great ambiance. the Baileys Horchata quite nice, the Old Fashioned wasn't an old fashioned. Bartender mistake. Gave me a Manhattan instead. Old fashioned made again. Decent. But why the small ice cubes!! \\xa0Sacrilege diluting it all. The breakfast burger was not bad but the Shashuka was quite quite nice. Overall. Not a bad experience once the initial hiccup was done.\""
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First find the original text\n",
    "cur_df.loc[1790]['review_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It took 12 min after sitting down at the table in a normal not so busy restaurant before the server came through to ask for H2O or order.',\n",
       " '12 min.',\n",
       " 'The server was quite good after but 12min!',\n",
       " \"C'mon.\",\n",
       " 'Great ambiance.',\n",
       " \"the Baileys Horchata quite nice, the Old Fashioned wasn't an old fashioned.\",\n",
       " 'Bartender mistake.',\n",
       " 'Gave me a Manhattan instead.',\n",
       " 'Old fashioned made again.',\n",
       " 'Decent.',\n",
       " 'But why the small ice cubes!!',\n",
       " 'Sacrilege diluting it all.',\n",
       " 'The breakfast burger was not bad but the Shashuka was quite quite nice.',\n",
       " 'Overall.',\n",
       " 'Not a bad experience once the initial hiccup was done.']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the sentences of the orignal text\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = cur_df.loc[1790]['review_text']\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'suppose admirable chain restaurant channeling craft beer goodness using lure horde relatively clueless tourist white shirt friend know better come along anyway unfortunately beer underwhelming rather hit miss usually representative style implied name occasionally downright bad'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the sentence for the keyword to match\n",
    "preprocess_sentence(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order next find inevitably despite better judgement finally service typically attentive flavourless people kinda job well minimum personality injected experience']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the sentence contains the keyphrase\n",
    "key_word = 'experience'\n",
    "sentences = sent_tokenize(text)\n",
    "# ind = np.where([key_word in sentence.lower() for sentence in sentences]) # we can do a preprocess here for each sentence\n",
    "ind = np.where([key_word in preprocess_sentence(sentence) for sentence in sentences])\n",
    "target_sentences = [preprocess_sentence(sentences[ind[0][i]]) for i in range(len(ind[0]))]\n",
    "target_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4754, -0.0332]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find relative positive or negative\n",
    "sentence = target_sentences\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "vader_scores = [sid.polarity_scores(sentence)['compound'] for sentence in sentence]\n",
    "vader_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.1492]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find relative positive or negative\n",
    "sentence = target_sentences\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "vader_scores = [sid.polarity_scores(sentence)['compound'] for sentence in sentence]\n",
    "vader_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final function for generating the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def get_vader_scores_for_sentences_containing_the_keyword(key_word,cur_df,original_review = 'review_text',processed_review = 'conca_review'):\n",
    "    # First find the reviews that contains the word \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    target_df_index = np.where(cur_df[processed_review].str.contains(key_word))[0]\n",
    "    # for each review, get the vader scores \n",
    "    vader_scores = []\n",
    "    for i in target_df_index:\n",
    "        \n",
    "        # For each review first find the original text\n",
    "#         text = cur_df.loc[i][original_review]\n",
    "        text = cur_df[original_review][i]\n",
    "        \n",
    "        # Get the sentences of the orignal text\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Find the sentence containing the key word\n",
    "        ind = np.where([key_word in preprocess_sentence(sentence) for sentence in sentences])\n",
    "        target_sentences = [preprocess_sentence(sentences[ind[0][i]]) for i in range(len(ind[0]))]\n",
    "        \n",
    "        # find relative positive or negative vader scores\n",
    "        sentence = target_sentences\n",
    "        vader_score = [sid.polarity_scores(sentence)['compound'] for sentence in sentence]\n",
    "        vader_scores.append(vader_score)\n",
    "        \n",
    "    return vader_scores\n",
    "\n",
    "def find_dataframe_for_item(cur_df, ItemIndex, ItemIndex_col = 'ItemIndex'):\n",
    "    \"\"\"\n",
    "    Return the dataframe that only contains the reviews \n",
    "    \"\"\"\n",
    "    new_df = cur_df.loc[np.where(cur_df[ItemIndex_col] == ItemIndex)]\n",
    "    new_df = new_df.reset_index()\n",
    "    return new_df\n",
    "\n",
    "def count_pos_neg(vader_result, threshold = 0):\n",
    "    \"\"\"\n",
    "    From the vader score lists, get the positive/negative list  \n",
    "    \"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    if len(vader_result) == 0:\n",
    "        return [0,0]\n",
    "    for i in range(len(vader_result)):\n",
    "        for j in range(len(vader_result[i])):\n",
    "            if vader_result[i][j] > threshold:\n",
    "                pos_count += 1\n",
    "            else:\n",
    "                neg_count += 1\n",
    "    return [pos_count,neg_count]\n",
    "\n",
    "def get_aspect_ratio(cur_df, ItemIndex, aspect_keywords, ItemIndex_col = 'ItemIndex', threshold = 0, original_review = 'review_text',processed_review = 'conca_review'):\n",
    "    target_df = find_dataframe_for_item(cur_df,ItemIndex)\n",
    "    vader_scores = get_vader_scores_for_sentences_containing_the_keyword(aspect_keywords,target_df)\n",
    "    ratio = count_pos_neg(vader_scores,threshold=threshold)\n",
    "    return ratio\n",
    "\n",
    "def get_category_count(cur_df, ItemIndex, keywords):\n",
    "    res = [0, 0]\n",
    "    for keyword in keywords:\n",
    "        a,b = get_aspect_ratio(cur_df, ItemIndex, keyword,\n",
    "                ItemIndex_col = 'ItemIndex', threshold = 0, original_review = 'review_text',processed_review = 'conca_review')\n",
    "        res[0]+= a\n",
    "        res[1]+= b\n",
    "    return res\n",
    "\n",
    "def get_item_aspect_table(cur_df, ItemIndex, categories):\n",
    "    \n",
    "    for category in categories:\n",
    "        keywords = categories[category]\n",
    "        count = get_category_count(cur_df, ItemIndex, keywords)\n",
    "        print ('Category ',category,' has the count: ', count, ' and ratio is ', count[0]/count[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_aspect_ratio(cur_df, 8010, 'experience',\n",
    "                ItemIndex_col = 'ItemIndex', threshold = 0, original_review = 'review_text',processed_review = 'conca_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mannually selected key word for each category \n",
    "food_quality = ['eating','taste','quality','food','dessert',\n",
    "                'texture','fresh', 'meat','vegetable','seafood','dish']\n",
    "drink = ['coffee','tea','beer','drink']\n",
    "service = ['service','care','friendly','customer','custom']\n",
    "price = ['cheap','price','pricy','expensive']\n",
    "ambiance = ['vibe','ambiance','atmosphere','room']\n",
    "location = ['location','parking','street','shop','subway']\n",
    "other = ['bathroom','washroom','table','seat','place','experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expanded key words for each category \n",
    "food_key = ['taco', 'curry', 'potato', 'crispy', 'shrimp', 'bread',  'ramen', 'pizza',  'sandwich', \n",
    "            'sushi', 'egg', 'fish',  'burger', 'cheese', 'salad', 'pork', 'beef', 'noodle',\n",
    "           'meat', 'chicken', 'dim sum', 'squid','tempura','tapioca','olive',\n",
    "            'octopus','croissant','honey','scallop','congee',\n",
    "           'skewer','miso','lettuce','avocado','calamari','kimchi','patty',\n",
    "           'sesame','tart','four','crepe','tuna','wrap','vegan','coconut','corn','poutine','toast','belly',\n",
    "           'oyster','sausage','duck','tofu','sashimi', 'lamb','mango','bacon','tomato'\n",
    "            ,'lobster','rib','waffle','bun','wing','dumpling','bean','steak','salmon',\n",
    "           'pasta','fried chicken','pork belly','spring roll','fried rice',\n",
    "            'pork bone soup']\n",
    "drink_key = ['beer','coffee','cocktail', 'tea', 'espresso','pop','juice','bubble tea','latte','wine','milk','milk tea','green tea']\n",
    "dessert_key = ['chocolate','cake','ice cream','donut','cookie','cone','cheesecake','matcha','pancake']\n",
    "friut_key = ['apple','lemon','strawberry','banana','fruit','grape','mango','watermelon','peach','pear']\n",
    "seasoning=['sugar','oil','soy','leaf','spice','butter','ginger','pepper','peanut','garlic']\n",
    "infrastruture_key = [ 'parking', 'store','shopping','nail','theatre','movie','washroom','bathroom',\n",
    "                    'window','station','chair','table','seat','plaza','market', 'mall', 'booth']\n",
    "service_key = ['quick', 'clean', 'busy',  'friendly','convenient','refill','soggy','greeted','bright','crowded','overpriced',\n",
    "              'cheaper','immediately','dog','quiet','efficient','spacious','pleasant','fair','complaint','disappointing','fancy',\n",
    "             'comfortable', 'dark','cozy','helpful','tax','nicely','honestly', 'pricey','yummy','music','chip','attentive',\n",
    "              'reasonable','wait']\n",
    "taste_key = ['traditional', 'fresh','spicy','flavorful','fluffy','smooth','frozen','sweetness','mayo','gravy','healthy','rare',\n",
    "            'refreshing','crunchy','chili','crust','stick','steamed','greasy','dip','gelato','salt','stuffed','topped','smoked',\n",
    "            'roasted','seasoned','chewy','pot','solid','sour', 'baked', 'juicy','creamy','deep fried']\n",
    "category_key = ['chinese', 'fast', 'thai', 'bar', 'fry', 'fried', 'dessert', 'dinner', 'lunch', 'soup', \n",
    "                'mexico', 'italian','mexican','vietnamese','buffet','takeout','casual','pub','bakery','indian','classic',\n",
    "               'modern','french','asian','birthday', 'vegetarian', 'downtown', 'bbq','japanese','breakfast','seafood',\n",
    "               'brunch'] \n",
    "\n",
    "\n",
    "food_quality = ['eating','taste','quality','food',\n",
    "                'texture', 'dish','cuisine']+food_key + taste_key + seasoning\n",
    "dessert_quality = ['dessert']+ dessert_key\n",
    "drink = drink_key\n",
    "friut_quality = friut_key \n",
    "service = ['service','care','customer','custom'] + service_key\n",
    "price = ['cheap','price','pricy','expensive']\n",
    "ambiance = ['vibe','ambiance','atmosphere','room']\n",
    "location = ['location','parking','street','shop','subway']\n",
    "infrastructure = ['insfrastructure'] + infrastruture_key\n",
    "recommend_to_others = ['girlfriend','boyfriend','wife','husband','child','friend','recommend','parent','grandparent','kid',\n",
    "                       'family','brother','sister']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[girlfriend, boyfriend, wife, husband, child, friend, recommend, parent, grandparent, kid, family, brother, sister]\n"
     ]
    }
   ],
   "source": [
    "print ('[%s]' % ', '.join(map(str, recommend_to_others)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = {'food_quality':food_quality,\n",
    "              'dessert':dessert_quality,\n",
    "              'drink': drink,\n",
    "              'friut':friut_quality,\n",
    "              'service': service,\n",
    "              'price': price,\n",
    "              'ambiance': ambiance,\n",
    "              'location': location,\n",
    "              'infrastructure':infrastructure,\n",
    "              'recommend_to_others': recommend_to_others}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category  ambiance  has the count:  [12, 8]  and ratio is  1.5\n",
      "Category  recommend_to_others  has the count:  [45, 17]  and ratio is  2.6470588235294117\n",
      "Category  infrastructure  has the count:  [21, 16]  and ratio is  1.3125\n",
      "Category  location  has the count:  [12, 6]  and ratio is  2.0\n",
      "Category  drink  has the count:  [12, 5]  and ratio is  2.4\n",
      "Category  food_quality  has the count:  [378, 212]  and ratio is  1.7830188679245282\n",
      "Category  dessert  has the count:  [19, 10]  and ratio is  1.9\n",
      "Category  price  has the count:  [7, 5]  and ratio is  1.4\n",
      "Category  service  has the count:  [105, 38]  and ratio is  2.763157894736842\n",
      "Category  friut  has the count:  [6, 1]  and ratio is  6.0\n"
     ]
    }
   ],
   "source": [
    "get_item_aspect_table(cur_df, 7312, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category  ambiance  has the count:  [31, 10]  and ratio is  3.1\n",
      "Category  recommend_to_others  has the count:  [59, 14]  and ratio is  4.214285714285714\n",
      "Category  infrastructure  has the count:  [57, 30]  and ratio is  1.9\n",
      "Category  location  has the count:  [15, 5]  and ratio is  3.0\n",
      "Category  drink  has the count:  [46, 24]  and ratio is  1.9166666666666667\n",
      "Category  food_quality  has the count:  [666, 238]  and ratio is  2.7983193277310923\n",
      "Category  dessert  has the count:  [18, 7]  and ratio is  2.5714285714285716\n",
      "Category  price  has the count:  [54, 36]  and ratio is  1.5\n",
      "Category  service  has the count:  [158, 73]  and ratio is  2.164383561643836\n",
      "Category  friut  has the count:  [10, 1]  and ratio is  10.0\n"
     ]
    }
   ],
   "source": [
    "get_item_aspect_table(cur_df, 8517, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vader_scores_for_sentences_containing_the_keyword(key_word,cur_df,original_review = 'review_text',processed_review = 'conca_review'):\n",
    "    # First find the reviews that contains the word \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    target_df_index = np.where(cur_df[processed_review].str.contains(key_word))[0]\n",
    "    # for each review, get the vader scores \n",
    "    vader_scores = []\n",
    "    for i in target_df_index:\n",
    "        \n",
    "        # For each review first find the original text\n",
    "#         text = cur_df.loc[i][original_review]\n",
    "        text = cur_df[original_review][i]\n",
    "        \n",
    "        # Get the sentences of the orignal text\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Find the sentence containing the key word\n",
    "        ind = np.where([key_word in preprocess_sentence(sentence) for sentence in sentences])\n",
    "        target_sentences = [preprocess_sentence(sentences[ind[0][i]]) for i in range(len(ind[0]))]\n",
    "        \n",
    "        # find relative positive or negative vader scores\n",
    "        sentence = target_sentences\n",
    "        vader_score = [sid.polarity_scores(sentence)['compound'] for sentence in sentence]\n",
    "        vader_scores.append(vader_score)\n",
    "        \n",
    "    return vader_scores\n",
    "\n",
    "def print_examples_for_sentences_containing_the_keyword(key_word,cur_df,original_review = 'review_text',processed_review = 'conca_review'):\n",
    "    # First find the reviews that contains the word \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    target_df_index = np.where(cur_df[processed_review].str.contains(key_word))[0]\n",
    "    # for each review, get the vader scores \n",
    "    vader_scores = []\n",
    "    all_sentences = []\n",
    "    res = {}\n",
    "    for i in target_df_index:\n",
    "        \n",
    "        # For each review first find the original text\n",
    "#         text = cur_df.loc[i][original_review]\n",
    "        text = cur_df[original_review][i]\n",
    "        \n",
    "        # Get the sentences of the orignal text\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Find the sentence containing the key word\n",
    "        ind = np.where([key_word in preprocess_sentence(sentence) for sentence in sentences])\n",
    "        target_sentences = [preprocess_sentence(sentences[ind[0][i]]) for i in range(len(ind[0]))]\n",
    "\n",
    "        # find relative positive or negative vader scores\n",
    "        sentence = target_sentences\n",
    "        vader_score = [sid.polarity_scores(sentence)['compound'] for sentence in sentence]\n",
    "        for sentence in sentence:\n",
    "            ind_vader_score = sid.polarity_scores(sentence)['compound']\n",
    "            res[sentence] = ind_vader_score\n",
    "        \n",
    "#         print (vader_score,sentence)\n",
    "#         res[sentence[0]] = vader_score \n",
    "#         vader_scores.append(vader_score)\n",
    "#         all_sentences.append(sentence)\n",
    "        \n",
    "#     return all_sentences, vader_scores\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_word = 'recommend'\n",
    "ItemIndex = 7312\n",
    "example_df = find_dataframe_for_item(cur_df, ItemIndex, ItemIndex_col = 'ItemIndex')\n",
    "res = print_examples_for_sentences_containing_the_keyword(key_word,example_df,original_review = 'review_text',processed_review = 'conca_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always great service meal filling delicious': 0.8316,\n",
       " 'best service bar none red lobster go ask patrick': 0.6369,\n",
       " 'coming location year service food always fantastic came night craving crab leg': 0.5574,\n",
       " 'droolservice good food came quick server attentive bringing u refill drink even empty': 0.2732,\n",
       " 'enjoyed every last bite overall decent meal great attentive service everyone left stuffed satisfied': 0.8807,\n",
       " 'establishment appears clean service good': 0.6808,\n",
       " 'food average though service excellent price relatively cheap especially lunch around average': 0.5719,\n",
       " 'food service good gave tip best scrumptious shrimp ever': 0.8807,\n",
       " 'great customer service always': 0.6249,\n",
       " 'never disappointed service red lobster': 0.3724,\n",
       " 'often disappointed terrible service restaurant': -0.7351,\n",
       " 'ordered alfredo shrimp pasta came clam chowder dessert place close service excellent even though nt many customer staff pretty hardworking liked': 0.8658,\n",
       " 'please rave impeccable service': 0.3182,\n",
       " 'receiving food drink approached make sure good say amazing service': 0.8402,\n",
       " 'server friendly service bit slow side especially lunch hour': 0.4939,\n",
       " 'server trained provide good customer service tend need making experience even amazing': 0.7717,\n",
       " 'service': 0.0,\n",
       " 'service amazing': 0.5859,\n",
       " 'service decent': 0.0,\n",
       " 'service decent waiter nt really check u often liked': 0.4703,\n",
       " 'service good cheese biscuit delicious': 0.765,\n",
       " 'service good server friendly thoughtful': 0.8271,\n",
       " 'service great': 0.6249,\n",
       " 'service incredible waiter happy go menu checking frequently make sure everything okay': 0.7845,\n",
       " 'service like chain restaurant': 0.3612,\n",
       " 'service okay': 0.2263,\n",
       " 'service quick courteous': 0.5106,\n",
       " 'service quick friendly': 0.4939,\n",
       " 'shrimp linguini alfredo thick filling want maximize endless offer would skip option addition breaded shrimp service hit miss especially first minute left without server': -0.0772,\n",
       " 'staff handled situation great customer service comped meal since nt satisfied food served': 0.7845,\n",
       " 'start off service excellent': 0.5719,\n",
       " 'term food service definitely experienced better': 0.6808,\n",
       " 'time last couple month say service amazing': 0.5859,\n",
       " 'wo nt disappointed least service': -0.4767,\n",
       " 'worst service food red lobster chain ever experienced': -0.6249}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'definitely recommend restaurant casual date night family celebration craving seafood': 0.7184,\n",
       " 'highly recommended': 0.2716,\n",
       " 'per recommendation shrimp linguine last glad': 0.4588,\n",
       " 'prior u ordering friendly gave u recommendation since first time': 0.4939,\n",
       " 'really enjoyed maple bacon shrimp highly recommend trying new menu item': 0.7496,\n",
       " 'something waitress recommended would thought would made better': 0.5719,\n",
       " 'stuffed maine lobster great highly recommendthe food size huge great big stomach': 0.8885,\n",
       " 'waiter joseph really pleasant gave u recommendation appetizer choose appetizer orderedred lobster signature pizza crisp thin crust pizza topped maritime lobster meat melted cheese fresh tomato sweet basil': 0.8357,\n",
       " 'waitress kind gave u recommendation based told': 0.5267,\n",
       " 'would nt really recommend average place': 0.4201,\n",
       " 'would recommend coming food portion great': 0.765}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['start off service excellent'],\n",
       " ['service good cheese biscuit delicious'],\n",
       " ['service decent waiter nt really check u often liked'],\n",
       " ['term food service definitely experienced better'],\n",
       " ['service like chain restaurant', 'great customer service always'],\n",
       " ['enjoyed every last bite overall decent meal great attentive service everyone left stuffed satisfied'],\n",
       " ['always great service meal filling delicious'],\n",
       " ['droolservice good food came quick server attentive bringing u refill drink even empty'],\n",
       " ['time last couple month say service amazing'],\n",
       " ['service great'],\n",
       " ['coming location year service food always fantastic came night craving crab leg',\n",
       "  'service quick friendly'],\n",
       " ['service'],\n",
       " ['establishment appears clean service good'],\n",
       " ['service decent',\n",
       "  'server friendly service bit slow side especially lunch hour'],\n",
       " ['please rave impeccable service',\n",
       "  'often disappointed terrible service restaurant',\n",
       "  'never disappointed service red lobster',\n",
       "  'server trained provide good customer service tend need making experience even amazing',\n",
       "  'receiving food drink approached make sure good say amazing service',\n",
       "  'wo nt disappointed least service'],\n",
       " ['shrimp linguini alfredo thick filling want maximize endless offer would skip option addition breaded shrimp service hit miss especially first minute left without server'],\n",
       " ['service okay'],\n",
       " ['best service bar none red lobster go ask patrick'],\n",
       " ['food service good gave tip best scrumptious shrimp ever',\n",
       "  'service amazing'],\n",
       " ['food average though service excellent price relatively cheap especially lunch around average',\n",
       "  'service incredible waiter happy go menu checking frequently make sure everything okay'],\n",
       " ['ordered alfredo shrimp pasta came clam chowder dessert place close service excellent even though nt many customer staff pretty hardworking liked'],\n",
       " ['service good server friendly thoughtful'],\n",
       " ['staff handled situation great customer service comped meal since nt satisfied food served'],\n",
       " ['service quick courteous'],\n",
       " ['worst service food red lobster chain ever experienced']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_word = 'food'\n",
    "ItemIndex = 8517\n",
    "example_df = find_dataframe_for_item(cur_df, ItemIndex, ItemIndex_col = 'ItemIndex')\n",
    "res = print_examples_for_sentences_containing_the_keyword(key_word,example_df,original_review = 'review_text',processed_review = 'conca_review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'also loved sticky meat dumpling deep fried shrimp roll mayo pineapple sticky rice good flaky taro pastry good bbq pork pastry delicious every time come since though food get slightly le le good': 0.9493,\n",
       " 'although nt make trip sure could nt tell nt looking around room clientele well good crown princess feel much cleaner chinese restaurant downtown even serious style important food darn good': 0.886,\n",
       " 'anyway back food': 0.0,\n",
       " 'anyways pleasantly surprised quality food crown princess': 0.6124,\n",
       " 'bill came tax tip food fed hungry female like think average appetite': 0.3612,\n",
       " 'bit pricey compared place worth food always fresh hot since order piece paper restaurant always packed make reservation': 0.4939,\n",
       " 'ca nt really go wrong thisordered baked bbq pork puff pastry personally nt like dish find dry one sister favourite dishordered pan fried snowpea leaf ordered deep fried wonton sour sauce dish big joke thought would real wonton filling basically bowl sweet sour sauce deep fried dumpling skin much sauce nt even know sauce seafood since drenched sweet sour sauce nt tasty would order summary love location service food mediocre worth extra dollar two per dish': 0.9156,\n",
       " 'clear crown princess caters mostly people go restaurant show off wealth people socialize rather care food food': 0.8402,\n",
       " 'delicious food upscale ambience though brightly lit': 0.7351,\n",
       " 'dinner food good necessarily better joint offer similar fare half price fresh scallop undercooked': 0.7964,\n",
       " 'disappointment lack cart pushed around definitely forgotten soon saw fantastic outfit server wear fine china teapot got green tea definitely got eat food thing stand octopus cooked perfectly yummy sauce dumpling also pork bun': 0.9062,\n",
       " 'dish around depending expensive ingredient food fantastic': 0.5574,\n",
       " 'enjoy food talk friend happily food delicious fresh': 0.9432,\n",
       " 'food breakdown good jumbo size sticky rice': 0.4404,\n",
       " 'food decent service good outfit questionable lot seating accommodate large group high list vendor lunch': 0.1779,\n",
       " 'food delivers though pricier usual chinatown dim sum far cheaper expect especially make time take advantage deal': 0.25,\n",
       " 'food either bland super salty': 0.5994,\n",
       " 'food elegant fresh feel attention paid every dish': 0.6597,\n",
       " 'food everything perfect': 0.5719,\n",
       " 'food good': 0.4404,\n",
       " 'food good anything ordinary': 0.4404,\n",
       " 'food grungy gorgeous atmosphere come good': 0.7845,\n",
       " 'food high price point': 0.0,\n",
       " 'food nt taste better': 0.4404,\n",
       " 'food quality taste sister restaurant markham north york better presentation': 0.4404,\n",
       " 'food quite good particularly enjoyed taro croquette': 0.7935,\n",
       " 'food really good': 0.4927,\n",
       " 'food similar average dim sum joint definitely outstanding term flavour': 0.7717,\n",
       " 'food similarly beautiful dumpling ordered swirl dye skin looked like little floral steamer basket pork bun ordered billed piggy piggy bun fluffy wheat bun shaped like pig filled char sieu fact drawback price presentation food ambience amazing food nt really taste better dim sum cheapie place scarborough': 0.9458,\n",
       " 'food sister restaurant uptown crown princess serf high quality dim sum without loud hustle bustle usually find chinese restaurant': 0.0,\n",
       " 'food size great ingredient fresh': 0.7506,\n",
       " 'food wise lot choice something different normal dim sum fare like milk tart bird nest roast duck dumpling': 0.7184,\n",
       " 'food worthy repeat visit kind amazed picture decor easily upstage everything else': 0.8979,\n",
       " 'gaugy tacky oppulent made dining experience bright one overall food great everything fresh distinct flavour': 0.8519,\n",
       " 'group wait longer since nt share table seems came sunday waited almost minute food good': 0.5908,\n",
       " 'har gow leek peanut like peanut food may enjoyed nt care crunchy texture har gow': 0.8402,\n",
       " 'har gow steamed perfection bbq pork bun pastry sweet savoury shrimp roll spinach divine different dim sum restaurant chicken foot wondeful flavourful succulent right off bone mediocre thing food turnip cake although tasty flaky': 0.8834,\n",
       " 'hate greasy fried food greasy': -0.5719,\n",
       " 'helping u order bringing food maid outfit another person serve food u': 0.296,\n",
       " 'hmm little food court level least meat batter': 0.0,\n",
       " 'however find pretentious taste food average': 0.0,\n",
       " 'hubby checked crown princess first time past weekend decor top quality food great bbq pork pastry jammed full filling yum': 0.7096,\n",
       " 'insane number food order people': -0.34,\n",
       " 'instead cart la carte food brought waitress french maid outfit': 0.0,\n",
       " 'interior bit extravagant compared dimsum place food always important ambiance': 0.2023,\n",
       " 'keep mind chinese food one least favourite food': 0.0,\n",
       " 'let start food': 0.0,\n",
       " 'like wanted accent food touch euro french fru fru style': 0.3612,\n",
       " 'lot u restaurant decided go crazy dim sum ordering well peking duck lunch know ordered roasted peking duck course peking duck actually someone stand behind table skin roasted duck well skilled minute favourite part course peking duck skin wrap green onion hoisin sauce always use green onion ask moreordered shark fin seafood dumpling soup nt particularly enjoy shark fin soup decided order cause controversy fact would nt able order anymore crave soup clear quite refreshing break open dumpling eat soup decent amount shark fin soupordered shrimp dumpling har gow many time talk har gow blog': 0.8439,\n",
       " 'love hate kind food': 0.5994,\n",
       " 'main room nicest imhothe food presented nicely excellent preparation': 0.8689,\n",
       " 'mean easy get good food service decor chinese faire': 0.7003,\n",
       " 'mind amongst two dining companion ordered worth food stuffed end meal decor really fancy refer photo tea pot plate supposedly take resos dim sum seated right away without asked name recommendation fried rice heap seafood chicken pork jumbo fried shrimp wrapped like spring roll mayo japanese style dumplingsa solid star dim sum': 0.6124,\n",
       " 'need pay cash want early special food nt disappoint service excellent': 0.5574,\n",
       " 'nt afraid expand food horizon order unusual item': 0.3182,\n",
       " 'nt eaten said food month': 0.0,\n",
       " 'offer original fusion item foie gras caviar siu maii nt find anything truly omg though food solid service spot': 0.7003,\n",
       " 'otherwise better place le food much better': 0.7003,\n",
       " 'overall food excellent really tasty good amount favouring': 0.8473,\n",
       " 'overall food quite acceptable overwhelming either': 0.3804,\n",
       " 'overall food true chinese cuisine': 0.4215,\n",
       " 'place kind euro french upscale style seems bit place still quite pleasure thing matter restaurant importantly food good': 0.9179,\n",
       " 'plate dinnerware dirty crusted food glassware greasy food came decent pace missing ingredient key auspicious food new year eve': -0.6249,\n",
       " 'portion bit smaller place food tad le greasy salty restaurant': 0.0,\n",
       " 'pricy especially given downscale version available nearby decent amount well prepared food full dinner menu go full sinophile treasure sea organ meat': 0.6369,\n",
       " 'quality food satisfactory': 0.3612,\n",
       " 'restaurant crown princess fine diningcuisine dimsumlocation downtownaddress bay stprice range poor ok good good excellentfood service ambiance nt want check restaurant called crown princess': 0.7184,\n",
       " 'sadly nt take advatange special nobody got early enough full night post halloween drink fest anyways food great one best downtown dim sum sure': 0.8885,\n",
       " 'satin faux marble purple velvet scream opulence dinner food ok though': -0.128,\n",
       " 'service always top notch food fast': 0.2023,\n",
       " 'service good far food go really liked': 0.7178,\n",
       " 'service good food better dim sum': 0.7003,\n",
       " 'service great take little get food come': 0.6249,\n",
       " 'service impeccable really saying alot chinese restaurant food slightly innovative take dim sum portion price reasonable loved siu mai foie top caviar instead red roe': 0.8078,\n",
       " 'service spot usual dim sum experience food quite nice although could better': 0.7178,\n",
       " 'still end day nt think ornate decor translate ridiculously jacked price unless quality food impressive': 0.2263,\n",
       " 'sure probably go back food good good quality even inexpensive': 0.7964,\n",
       " 'table agreed probably best chinese food ate outside china': 0.743,\n",
       " 'tell good restaurant treat staple food': 0.6808,\n",
       " 'trumpet sound crown favourite dim sum spot hood compared dynasty bloor price crown princess bit cheaper food better nt go dynasty ever open large wooden door step elegantly decorated dining room': 0.7003,\n",
       " 'waited hour get seat saturday taste food bad': -0.5423,\n",
       " 'whistle dim sum happy tune rest fellow reviewer great refined dim sum twist u downtowners unique dish quality food fancy schmancy dish': 0.8316,\n",
       " 'wow good thing place good food compensate decor look like elton john designed crack without glass excited try place review yelp': 0.8769}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
