{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Data Packages\n",
    "import math\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Counter\n",
    "from collections import Counter\n",
    "\n",
    "#Operation\n",
    "import operator\n",
    "\n",
    "#Natural Language Processing Packages\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "## Download Resources\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "\n",
    "## Machine Learning\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use vader to evaluated sentiment of reviews\n",
    "def evalSentences(sentences, to_df=False, columns=[]):\n",
    "    #Instantiate an instance to access SentimentIntensityAnalyzer class\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    pdlist = []\n",
    "    if to_df:\n",
    "        for sentence in tqdm(sentences):\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            pdlist.append([sentence]+[ss['compound']])\n",
    "        reviewDf = pandas.DataFrame(pdlist)\n",
    "        reviewDf.columns = columns\n",
    "        return reviewDf\n",
    "    \n",
    "    else:\n",
    "        for sentence in tqdm(sentences):\n",
    "            print(sentence)\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            for k in sorted(ss):\n",
    "                print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cur_df = pd.read_csv('../../data/yelp/Data.csv', index_col=0,  encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = cur_df['review_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 169961/169961 [05:04<00:00, 557.57it/s]\n"
     ]
    }
   ],
   "source": [
    "reviewDF = evalSentences(reviews, to_df=True, columns=['reviewCol','vader'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewCol</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I ordered the lemon mango slush and the lemon ...</td>\n",
       "      <td>0.9758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Came here on a Sunday afternoon, it wasn't bus...</td>\n",
       "      <td>0.9763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{Grapefruit &amp; Yakult Green Tea with Aloe Jelly...</td>\n",
       "      <td>0.9170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saw this newly opened bubble tea shop and want...</td>\n",
       "      <td>0.9898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Happy Lemon has become my new favourite place ...</td>\n",
       "      <td>0.9922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reviewCol   vader\n",
       "0  I ordered the lemon mango slush and the lemon ...  0.9758\n",
       "1  Came here on a Sunday afternoon, it wasn't bus...  0.9763\n",
       "2  {Grapefruit & Yakult Green Tea with Aloe Jelly...  0.9170\n",
       "3  Saw this newly opened bubble tea shop and want...  0.9898\n",
       "4  Happy Lemon has become my new favourite place ...  0.9922"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_df['Binary_vader'] = (reviewDF['vader']> 0.05)*[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Save\n",
    "# reviewDF.to_csv('../../data/yelp/reviewDF.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find most sentiment words by Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: You may want to use an NLTK tokenizer instead of a regular expression in the following\n",
    "def dataFrameTransformation(cur_df, reviewDF, k=500):\n",
    "    text = reviewDF['reviewCol'].values\n",
    "    \n",
    "\n",
    "#     text = text.replace('.',' ').replace('/t',' ').replace('\\t',' ').replace('/',' ').replace('-',' ')\n",
    "    text = np.char.replace(text, '.',' ')\n",
    "    \n",
    "    # Tokenize\n",
    "#     text = nltk.word_tokenize(reviews)\n",
    "    # Lowercase\n",
    "    text = [w.lower() for w in text]\n",
    "    # Remove Punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text]\n",
    "    # Remove tokens that are not alphabetic\n",
    "    text = [w for w in text if w.isalpha()]\n",
    "    \n",
    "    # Remove Stopwords\n",
    "    # Get english stopwords\n",
    "    en_stopwords = set(stopwords.words('english'))\n",
    "    en_stopwords.remove('off')\n",
    "    text = [w for w in text if w not in en_stopwords]\n",
    "    \n",
    "#     # Lemmatizing\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     text = [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "#     text = \" \" + \" \".join(str(x) for x in text) + \" \"\n",
    "\n",
    "#     text = text.replace('whitish', 'white')\n",
    "#     text = text.replace('bisquity', ' biscuit ')\n",
    "#     text = text.replace('carmel', ' caramel ')\n",
    "#     text = text.replace('flower', ' floral ')\n",
    "#     text = text.replace('piny', ' pine ')\n",
    "#     text = text.replace('off white', 'offwhite')\n",
    "#     text = text.replace('goden', 'gold')\n",
    "#     text = text.replace('yello', 'yellow')\n",
    "#     text = text.replace('reddish', ' red ') \n",
    "    \n",
    "#     # Reset to token\n",
    "#     text = nltk.word_tokenize(text)\n",
    "#     table = str.maketrans('', '', string.punctuation)\n",
    "#     text = [w.translate(table) for w in text]\n",
    "#     text = [w for w in text if w.isalpha()]\n",
    "# #     en_stopwords = set(stopwords.words('english'))\n",
    "#     text = [w for w in text if w not in en_stopwords]\n",
    "    \n",
    "    # Top-k frequent terms\n",
    "    counter = Counter()\n",
    "    reviews = text\n",
    "    for review in reviews:\n",
    "            counter.update([word.lower() \n",
    "                            for word \n",
    "                            in re.findall(r'\\w+', review) \n",
    "                            if word.lower() not in stop and len(word) > 2])\n",
    "    topk = counter.most_common(k)        \n",
    "    \n",
    "    #Find out if a particular review has the word from topk list\n",
    "    freqReview = []\n",
    "    for i in range(len(reviews)):\n",
    "        tempCounter = Counter([word.lower() for word in re.findall(r'\\w+',reviews[i])])\n",
    "        topkinReview = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "        freqReview.append(topkinReview)\n",
    "        \n",
    "        \n",
    "    #Prepare freqReviewDf\n",
    "    freqReviewDf = pandas.DataFrame(freqReview)\n",
    "    dfName = []\n",
    "    for c in topk:\n",
    "        dfName.append(c[0])\n",
    "    freqReviewDf.columns = dfName\n",
    "    finalreviewDf = reviewDF.join(freqReviewDf)\n",
    "    finaldf = hotelDf[['hotelName','ratingScore','groundTruth']].join(finalreviewDf)\n",
    "    return topk, finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: You may want to use an NLTK tokenizer instead of a regular expression in the following\n",
    "def dataFrameTransformation(hotelDf, reviewDF, k=500):\n",
    "    reviews = reviewDF['reviewCol'].values\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    # Lowercase\n",
    "    reviews = [w.lower() for w in reviews]\n",
    "    \n",
    "    # Top-k frequent terms\n",
    "    counter = Counter()\n",
    "    for review in tqdm(reviews):\n",
    "            counter.update([word.lower() \n",
    "                            for word \n",
    "                            in re.findall(r'\\w+', review) \n",
    "                            if word.lower() not in stop and len(word) > 2])\n",
    "    topk = counter.most_common(k)        \n",
    "    \n",
    "    #Find out if a particular review has the word from topk list\n",
    "    freqReview = []\n",
    "    for i in tqdm(range(len(reviews))):\n",
    "        tempCounter = Counter([word.lower() for word in re.findall(r'\\w+',reviews[i])])\n",
    "        topkinReview = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "        freqReview.append(topkinReview)\n",
    "        \n",
    "        \n",
    "    #Prepare freqReviewDf\n",
    "    freqReviewDf = pandas.DataFrame(freqReview)\n",
    "    dfName = []\n",
    "    for c in topk:\n",
    "        dfName.append(c[0])\n",
    "    freqReviewDf.columns = dfName\n",
    "    finalreviewDf = reviewDF.join(freqReviewDf)\n",
    "    finaldf = hotelDf.join(finalreviewDf)\n",
    "    return topk, finaldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topk, finaldf = dataFrameTransformation(cur_df, reviewDF, k=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Mutual Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Top K mutual information terms from the dataframe\n",
    "def getMI(topk, df, label_column='groundTruth'):\n",
    "    miScore = []\n",
    "    for word in topk:\n",
    "        miScore.append([word[0]]+[metrics.mutual_info_score(finaldf[label_column], finaldf[word[0]])])\n",
    "    miScoredf = pandas.DataFrame(miScore).sort_values(1,ascending=0)\n",
    "    miScoredf.columns = ['Word','MI Score']\n",
    "    return miScoredf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noun Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"\"\"The Buddha, the Godhead, resides quite as comfortably in the circuits of a digital\n",
    "computer or the gears of a cycle transmission as he does at the top of a mountain\n",
    "or in the petals of a flower. To think otherwise is to demean the Buddha...which is\n",
    "to demean oneself.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Buddha', 'NNP'),\n",
       " ('the', 'DT'),\n",
       " ('Godhead', 'NNP'),\n",
       " ('resides', 'VBZ'),\n",
       " ('quite', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('comfortably', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('circuits', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('digital', 'JJ'),\n",
       " ('computer', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('gears', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('cycle', 'NN'),\n",
       " ('transmission', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('does', 'VBZ'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('top', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('mountain', 'NN'),\n",
       " ('or', 'CC'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('petals', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('flower', 'NN'),\n",
       " ('To', 'TO'),\n",
       " ('think', 'VB'),\n",
       " ('otherwise', 'RB'),\n",
       " ('is', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('demean', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('Buddha', 'NNP'),\n",
       " ('which', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('to', 'TO'),\n",
       " ('demean', 'VB'),\n",
       " ('oneself', 'PRP')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = PerceptronTagger()\n",
    "# Part of Speech Tagging\n",
    "# Google: https://en.wikipedia.org/wiki/Part-of-speech_tagging\n",
    "pos_tag = tagger.tag\n",
    "taggedToks = pos_tag(re.findall(r'\\w+', text))\n",
    "taggedToks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This grammar is described in the paper by S. N. Kim,\n",
    "# T. Baldwin, and M.-Y. Kan.\n",
    "# Evaluating n-gram based evaluation metrics for automatic\n",
    "# keyphrase extraction.\n",
    "# Technical report, University of Melbourne, Melbourne 2010.\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create phrase tree\n",
    "chunker = nltk.RegexpParser(grammar)\n",
    "tree= chunker.parse(taggedToks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Noun Phrase Extraction Support Functions\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "# generator, generate leaves one by one\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP' or t.label()=='JJ' or t.label()=='RB'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "# stemming, lematizing, lower case... \n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = stemmer.stem(word)\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "# stop-words and length control\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "# generator, create item once a time\n",
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "        term = [normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        # Phrase only\n",
    "        if len(term)>1:\n",
    "            yield term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['digit', 'comput'], ['cycl', 'transmiss']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traverse tree and get noun phrases\n",
    "npTokenList = [word for word in get_terms(tree)]\n",
    "\n",
    "npTokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten phrase lists to get tokens for analysis\n",
    "def flatten(npTokenList):\n",
    "    finalList =[]\n",
    "    for phrase in npTokenList:\n",
    "        token = ''\n",
    "        for word in phrase:\n",
    "            token += word + ' '\n",
    "        finalList.append(token.rstrip())\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalList = flatten(npTokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['digit comput', 'cycl transmiss']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "# Revise the previous dataframe transform function...\n",
    "def newDataFrameTransformation(hotelDf, reviewDF, reviewcol_name = 'reviewCol',k= 1000):\n",
    "    reviews = reviewDF[reviewcol_name].values\n",
    "    \n",
    "    \n",
    "    # Top-k frequent terms\n",
    "    counter = Counter()\n",
    "    for review in tqdm(reviews):\n",
    "            counter.update(flatten([word\n",
    "                            for word \n",
    "                            in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+', review)))) \n",
    "                            ]))\n",
    "    topk = counter.most_common(k)        \n",
    "    \n",
    "    #Find out if a particular review has the word from topk list\n",
    "    freqReview = []\n",
    "    for i in tqdm(range(len(reviews))):\n",
    "        tempCounter = Counter(flatten([word \n",
    "                                       for word \n",
    "                                       in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+',reviews[i]))))]))\n",
    "        topkinReview = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "        freqReview.append(topkinReview)\n",
    "         \n",
    "    #Prepare freqReviewDf\n",
    "    freqReviewDf = pandas.DataFrame(freqReview)\n",
    "    dfName = []\n",
    "    for c in topk:\n",
    "        dfName.append(c[0])\n",
    "    freqReviewDf.columns = dfName\n",
    "    finalreviewDf = reviewDF.join(freqReviewDf)\n",
    "    finaldf = hotelDf[['business_id','rating','Binary','review','conca_review','UserIndex','ItemIndex']].join(finalreviewDf)\n",
    "    return topk, finaldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topk_phrase, finaldf_phrase = newDataFrameTransformation(cur_df, reviewDF, k = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topk_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute PMI for all terms and all possible labels\n",
    "def pmiForAllCal(df, label_column='Binary', topk=topk_phrase):\n",
    "    #Try calculate all the pmi for top k and store them into one pmidf dataframe\n",
    "    pmilist = []\n",
    "    pmiposlist = []\n",
    "    pmineglist = []\n",
    "    for word in tqdm(topk):\n",
    "#         pmilist.append([word[0]]+[pmiCal(df,word[0])])\n",
    "        pmiposlist.append([word[0]]+[pmiIndivCal(df,word[0],1,label_column)])\n",
    "        pmineglist.append([word[0]]+[pmiIndivCal(df,word[0],0,label_column)])\n",
    "    pmidf = pandas.DataFrame(pmilist)\n",
    "    pmiposlist = pandas.DataFrame(pmiposlist)\n",
    "    pmineglist = pandas.DataFrame(pmineglist)\n",
    "    pmiposlist.columns = ['word','pmi']\n",
    "    pmineglist.columns = ['word','pmi']\n",
    "#     pmidf.columns = ['word','pmi']\n",
    "#     return pmiposlist, pmineglist, pmidf\n",
    "    return pmiposlist, pmineglist\n",
    "\n",
    "def pmiIndivCal(df,x,gt, label_column='Binary'):\n",
    "    px = sum(df[label_column]==gt)/len(df)\n",
    "    py = sum(df[x]==1)/len(df)\n",
    "    pxy = len(df[(df[label_column]==gt) & (df[x]==1)])/len(df)\n",
    "    if pxy==0:#Log 0 cannot happen\n",
    "        pmi = math.log((pxy+0.0001)/(px*py))\n",
    "    else:\n",
    "        pmi = math.log(pxy/(px*py))\n",
    "    return pmi\n",
    "\n",
    "# Simple example of getting pairwise mutual information of a term\n",
    "def pmiCal(df, x):\n",
    "    pmilist=[]\n",
    "    for i in [1,0]:\n",
    "        for j in [0,1]:\n",
    "            px = sum(df['Binary']==i)/len(df)\n",
    "            py = sum(df[x]==j)/len(df)\n",
    "            pxy = len(df[(df['Binary']==i) & (df[x]==j)])/len(df)\n",
    "            if pxy==0:#Log 0 cannot happen\n",
    "                pmi = math.log((pxy+0.0001)/(px*py))\n",
    "            else:\n",
    "                pmi = math.log(pxy/(px*py))\n",
    "            pmilist.append([i]+[j]+[px]+[py]+[pxy]+[pmi])\n",
    "    pmidf = pandas.DataFrame(pmilist)\n",
    "    pmidf.columns = ['x','y','px','py','pxy','pmi']\n",
    "    return pmidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pmiposlist, pmineglist = pmiForAllCal(finaldf_phrase, topk=topk_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pmiposlist_vader, pmineglist_vader = pmiForAllCal(finaldf_phrase, topk=topk_phrase,label_column='vader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>pmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first time</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ice cream</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great place</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>next time</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>littl bit</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>long time</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>last time</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>good thing</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>good place</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dim sum</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>portion size</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>custom servic</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hot sauc</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friday night</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spring roll</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>larg group</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>side dish</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pad thai</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>good food</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wait time</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>right amount</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>big fan</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>second time</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>food court</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>saturday night</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bubbl tea</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>fri chicken</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>great servic</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mani peopl</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>mani time</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>mani seat</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>nice atmospher</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>fun time</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>mother day</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>bamboo shoot</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>extra noodl</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>half price</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>breakfast place</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>noth fanci</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>ga station</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>veggi option</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>uber eat</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>vegetarian dish</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>second chanc</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>sour sauc</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>black sesam ice cream</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>tofu soup</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>mediocr food</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>high rate</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>nice varieti</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>ethiopian food</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>signatur dish</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>raini day</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>larg window</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>good portion size</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>meat eater</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>soft drink</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>enough food</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>thursday even</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>sukho thai</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word  pmi\n",
       "0               first time  inf\n",
       "1                ice cream  inf\n",
       "2              great place  inf\n",
       "3                next time  inf\n",
       "4                littl bit  inf\n",
       "5                long time  inf\n",
       "6                last time  inf\n",
       "7               good thing  inf\n",
       "8               good place  inf\n",
       "9                  dim sum  inf\n",
       "10            portion size  inf\n",
       "11           custom servic  inf\n",
       "12                hot sauc  inf\n",
       "13            friday night  inf\n",
       "14             spring roll  inf\n",
       "15              larg group  inf\n",
       "16               side dish  inf\n",
       "17                pad thai  inf\n",
       "18               good food  inf\n",
       "19               wait time  inf\n",
       "20            right amount  inf\n",
       "21                 big fan  inf\n",
       "22             second time  inf\n",
       "23              food court  inf\n",
       "24          saturday night  inf\n",
       "25               bubbl tea  inf\n",
       "26             fri chicken  inf\n",
       "27            great servic  inf\n",
       "28              mani peopl  inf\n",
       "29               mani time  inf\n",
       "..                     ...  ...\n",
       "970              mani seat  inf\n",
       "971         nice atmospher  inf\n",
       "972               fun time  inf\n",
       "973             mother day  inf\n",
       "974           bamboo shoot  inf\n",
       "975            extra noodl  inf\n",
       "976             half price  inf\n",
       "977        breakfast place  inf\n",
       "978             noth fanci  inf\n",
       "979             ga station  inf\n",
       "980           veggi option  inf\n",
       "981               uber eat  inf\n",
       "982        vegetarian dish  inf\n",
       "983           second chanc  inf\n",
       "984              sour sauc  inf\n",
       "985  black sesam ice cream  inf\n",
       "986              tofu soup  inf\n",
       "987           mediocr food  inf\n",
       "988              high rate  inf\n",
       "989           nice varieti  inf\n",
       "990         ethiopian food  inf\n",
       "991          signatur dish  inf\n",
       "992              raini day  inf\n",
       "993            larg window  inf\n",
       "994      good portion size  inf\n",
       "995             meat eater  inf\n",
       "996             soft drink  inf\n",
       "997            enough food  inf\n",
       "998          thursday even  inf\n",
       "999             sukho thai  inf\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmiposlist_vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"pmineglist\", \"wb\") as output_file:\n",
    "    pkl.dump(pmineglist, output_file)\n",
    "with open(\"pmiposlist\", \"wb\") as output_file:\n",
    "    pkl.dump(pmiposlist, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# topk_phrase, finaldf_phrase\n",
    "with open(\"finaldf_phrase\", \"wb\") as output_file:\n",
    "    pkl.dump(finaldf_phrase, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyphrase for positive and negative rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_rating = pmineglist.sort_values('pmi',ascending=0)\n",
    "neg_rating = neg_rating['word'][:50].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad place',\n",
       " 'okay noth',\n",
       " 'decent place',\n",
       " 'ok noth',\n",
       " 'decent food',\n",
       " 'second chanc',\n",
       " 'terribl servic',\n",
       " 'mediocr food',\n",
       " 'decent servic',\n",
       " 'eye contact',\n",
       " 'sub par',\n",
       " 'slow servic',\n",
       " 'high hope',\n",
       " 'dri side',\n",
       " 'bit bland',\n",
       " 'separ bill',\n",
       " 'high price',\n",
       " 'empti tabl',\n",
       " 'poor servic',\n",
       " 'room temperatur',\n",
       " 'littl bland',\n",
       " 'good dish',\n",
       " 'bad tast',\n",
       " 'averag price',\n",
       " 'asian legend',\n",
       " 'quick meal',\n",
       " 'good overal',\n",
       " 'bad servic',\n",
       " 'salti side',\n",
       " 'high side',\n",
       " 'swiss chalet',\n",
       " 'non exist',\n",
       " 'plu side',\n",
       " 'extra star',\n",
       " 'wow factor',\n",
       " 'long wait time',\n",
       " 'bad day',\n",
       " 'dim sum place',\n",
       " 'bit pricey',\n",
       " 'instant noodl',\n",
       " 'chicken piec',\n",
       " 'good locat',\n",
       " 'small portion',\n",
       " 'beef noodl',\n",
       " 'good place',\n",
       " 'much sauc',\n",
       " 'decent portion',\n",
       " 'good noth',\n",
       " 'deer garden',\n",
       " 'la carnita']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bad place, okay noth, decent place, ok noth, decent food, second chanc, terribl servic, mediocr food, decent servic, eye contact, sub par, slow servic, high hope, dri side, bit bland, separ bill, high price, empti tabl, poor servic, room temperatur, littl bland, good dish, bad tast, averag price, asian legend, quick meal, good overal, bad servic, salti side, high side, swiss chalet, non exist, plu side, extra star, wow factor, long wait time, bad day, dim sum place, bit pricey, instant noodl, chicken piec, good locat, small portion, beef noodl, good place, much sauc, decent portion, good noth, deer garden, la carnita]\n"
     ]
    }
   ],
   "source": [
    "print ('[%s]' % ', '.join(map(str, neg_rating)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_rating = pmiposlist.sort_values('pmi',ascending=0)\n",
    "pos_rating = pos_rating['word'][:50].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[amaz experi, favourit restaur, amaz servic, wonder experi, amaz food, great recommend, favorit place, hidden gem, hair cut, perfect balanc, favourit place, littl gem, much fun, favourit spot, escap room, super help, great coffe, real deal, great custom servic, person favourit, great qualiti, top notch, tast menu, delici food, great job, super nice, awesom place, great experi, delici meal, excel servic, perfect amount, singl time, mani flavour, great staff, nail salon, farmer market, bang bang, sure everyth, perfect place, sushi bar, favourit thing, great servic, vietnames coffe, special occas, great price, good reason, foie gra, friendli staff, board game, filet mignon]\n"
     ]
    }
   ],
   "source": [
    "print ('[%s]' % ', '.join(map(str, pos_rating)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amaz experi',\n",
       " 'favourit restaur',\n",
       " 'amaz servic',\n",
       " 'wonder experi',\n",
       " 'amaz food',\n",
       " 'great recommend',\n",
       " 'favorit place',\n",
       " 'hidden gem',\n",
       " 'hair cut',\n",
       " 'perfect balanc',\n",
       " 'favourit place',\n",
       " 'littl gem',\n",
       " 'much fun',\n",
       " 'favourit spot',\n",
       " 'escap room',\n",
       " 'super help',\n",
       " 'great coffe',\n",
       " 'real deal',\n",
       " 'great custom servic',\n",
       " 'person favourit',\n",
       " 'great qualiti',\n",
       " 'top notch',\n",
       " 'tast menu',\n",
       " 'delici food',\n",
       " 'great job',\n",
       " 'super nice',\n",
       " 'awesom place',\n",
       " 'great experi',\n",
       " 'delici meal',\n",
       " 'excel servic',\n",
       " 'perfect amount',\n",
       " 'singl time',\n",
       " 'mani flavour',\n",
       " 'great staff',\n",
       " 'nail salon',\n",
       " 'farmer market',\n",
       " 'bang bang',\n",
       " 'sure everyth',\n",
       " 'perfect place',\n",
       " 'sushi bar',\n",
       " 'favourit thing',\n",
       " 'great servic',\n",
       " 'vietnames coffe',\n",
       " 'special occas',\n",
       " 'great price',\n",
       " 'good reason',\n",
       " 'foie gra',\n",
       " 'friendli staff',\n",
       " 'board game',\n",
       " 'filet mignon']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vader binary values\n",
    "finaldf_phrase['Binary_vader'] = (finaldf_phrase['vader'] > 0.05)*[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|                                                                                 | 1/1000 [00:00<15:14,  1.09it/s]\n",
      "\n",
      "  0%|▏                                                                                | 2/1000 [00:01<14:45,  1.13it/s]\n",
      "\n",
      "  0%|▏                                                                                | 3/1000 [00:02<14:20,  1.16it/s]\n",
      "\n",
      "  0%|▎                                                                                | 4/1000 [00:03<14:02,  1.18it/s]\n",
      "\n",
      "  0%|▍                                                                                | 5/1000 [00:04<13:48,  1.20it/s]\n",
      "\n",
      "  1%|▍                                                                                | 6/1000 [00:04<13:37,  1.22it/s]\n",
      "\n",
      "  1%|▌                                                                                | 7/1000 [00:05<13:29,  1.23it/s]\n",
      "\n",
      "  1%|▋                                                                                | 8/1000 [00:06<13:25,  1.23it/s]\n",
      "\n",
      "  1%|▋                                                                                | 9/1000 [00:07<13:17,  1.24it/s]\n",
      "\n",
      "  1%|▊                                                                               | 10/1000 [00:08<13:12,  1.25it/s]\n",
      "\n",
      "  1%|▉                                                                               | 11/1000 [00:08<13:09,  1.25it/s]\n",
      "\n",
      "  1%|▉                                                                               | 12/1000 [00:09<13:05,  1.26it/s]\n",
      "\n",
      "  1%|█                                                                               | 13/1000 [00:10<12:57,  1.27it/s]\n",
      "\n",
      "  1%|█                                                                               | 14/1000 [00:11<12:59,  1.26it/s]\n",
      "\n",
      "  2%|█▏                                                                              | 15/1000 [00:12<12:55,  1.27it/s]\n",
      "\n",
      "  2%|█▎                                                                              | 16/1000 [00:12<12:59,  1.26it/s]\n",
      "\n",
      "  2%|█▎                                                                              | 17/1000 [00:13<12:53,  1.27it/s]\n",
      "\n",
      "  2%|█▍                                                                              | 18/1000 [00:14<12:48,  1.28it/s]\n",
      "\n",
      "  2%|█▌                                                                              | 19/1000 [00:15<12:42,  1.29it/s]\n",
      "\n",
      "  2%|█▌                                                                              | 20/1000 [00:15<12:43,  1.28it/s]\n",
      "\n",
      "  2%|█▋                                                                              | 21/1000 [00:16<12:40,  1.29it/s]\n",
      "\n",
      "  2%|█▊                                                                              | 22/1000 [00:17<12:44,  1.28it/s]\n",
      "\n",
      "  2%|█▊                                                                              | 23/1000 [00:18<12:51,  1.27it/s]\n",
      "\n",
      "  2%|█▉                                                                              | 24/1000 [00:19<12:47,  1.27it/s]\n",
      "\n",
      "  2%|██                                                                              | 25/1000 [00:19<12:48,  1.27it/s]\n",
      "\n",
      "  3%|██                                                                              | 26/1000 [00:20<12:51,  1.26it/s]\n",
      "\n",
      "  3%|██▏                                                                             | 27/1000 [00:21<12:42,  1.28it/s]\n",
      "\n",
      "  3%|██▏                                                                             | 28/1000 [00:22<12:40,  1.28it/s]\n",
      "\n",
      "  3%|██▎                                                                             | 29/1000 [00:23<12:39,  1.28it/s]\n",
      "\n",
      "  3%|██▍                                                                             | 30/1000 [00:23<12:43,  1.27it/s]\n",
      "\n",
      "  3%|██▍                                                                             | 31/1000 [00:24<12:48,  1.26it/s]\n",
      "\n",
      "  3%|██▌                                                                             | 32/1000 [00:25<12:40,  1.27it/s]\n",
      "\n",
      "  3%|██▋                                                                             | 33/1000 [00:26<12:33,  1.28it/s]\n",
      "\n",
      "  3%|██▋                                                                             | 34/1000 [00:26<12:29,  1.29it/s]\n",
      "\n",
      "  4%|██▊                                                                             | 35/1000 [00:27<12:25,  1.29it/s]\n",
      "\n",
      "  4%|██▉                                                                             | 36/1000 [00:28<12:22,  1.30it/s]\n",
      "\n",
      "  4%|██▉                                                                             | 37/1000 [00:29<12:21,  1.30it/s]\n",
      "\n",
      "  4%|███                                                                             | 38/1000 [00:29<12:19,  1.30it/s]\n",
      "\n",
      "  4%|███                                                                             | 39/1000 [00:30<12:21,  1.30it/s]\n",
      "\n",
      "  4%|███▏                                                                            | 40/1000 [00:31<12:20,  1.30it/s]\n",
      "\n",
      "  4%|███▎                                                                            | 41/1000 [00:32<12:30,  1.28it/s]\n",
      "\n",
      "  4%|███▎                                                                            | 42/1000 [00:33<12:31,  1.27it/s]\n",
      "\n",
      "  4%|███▍                                                                            | 43/1000 [00:33<12:40,  1.26it/s]\n",
      "\n",
      "  4%|███▌                                                                            | 44/1000 [00:34<12:46,  1.25it/s]\n",
      "\n",
      "  4%|███▌                                                                            | 45/1000 [00:35<12:49,  1.24it/s]\n",
      "\n",
      "  5%|███▋                                                                            | 46/1000 [00:36<12:52,  1.24it/s]\n",
      "\n",
      "  5%|███▊                                                                            | 47/1000 [00:37<12:47,  1.24it/s]\n",
      "\n",
      "  5%|███▊                                                                            | 48/1000 [00:38<12:54,  1.23it/s]\n",
      "\n",
      "  5%|███▉                                                                            | 49/1000 [00:38<12:56,  1.22it/s]\n",
      "\n",
      "  5%|████                                                                            | 50/1000 [00:39<12:53,  1.23it/s]\n",
      "\n",
      "  5%|████                                                                            | 51/1000 [00:40<12:50,  1.23it/s]\n",
      "\n",
      "  5%|████▏                                                                           | 52/1000 [00:41<12:48,  1.23it/s]\n",
      "\n",
      "  5%|████▏                                                                           | 53/1000 [00:42<12:41,  1.24it/s]\n",
      "\n",
      "  5%|████▎                                                                           | 54/1000 [00:42<12:35,  1.25it/s]\n",
      "\n",
      "  6%|████▍                                                                           | 55/1000 [00:43<12:35,  1.25it/s]\n",
      "\n",
      "  6%|████▍                                                                           | 56/1000 [00:44<12:37,  1.25it/s]\n",
      "\n",
      "  6%|████▌                                                                           | 57/1000 [00:45<12:39,  1.24it/s]\n",
      "\n",
      "  6%|████▋                                                                           | 58/1000 [00:46<12:40,  1.24it/s]\n",
      "\n",
      "  6%|████▋                                                                           | 59/1000 [00:46<12:35,  1.25it/s]\n",
      "\n",
      "  6%|████▊                                                                           | 60/1000 [00:47<12:34,  1.25it/s]\n",
      "\n",
      "  6%|████▉                                                                           | 61/1000 [00:48<12:36,  1.24it/s]\n",
      "\n",
      "  6%|████▉                                                                           | 62/1000 [00:49<12:38,  1.24it/s]\n",
      "\n",
      "  6%|█████                                                                           | 63/1000 [00:50<12:32,  1.25it/s]\n",
      "\n",
      "  6%|█████                                                                           | 64/1000 [00:50<12:34,  1.24it/s]\n",
      "\n",
      "  6%|█████▏                                                                          | 65/1000 [00:51<12:38,  1.23it/s]\n",
      "\n",
      "  7%|█████▎                                                                          | 66/1000 [00:52<12:34,  1.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▎                                                                          | 67/1000 [00:53<12:22,  1.26it/s]\n",
      "\n",
      "  7%|█████▍                                                                          | 68/1000 [00:54<12:20,  1.26it/s]\n",
      "\n",
      "  7%|█████▌                                                                          | 69/1000 [00:54<12:17,  1.26it/s]\n",
      "\n",
      "  7%|█████▌                                                                          | 70/1000 [00:55<12:18,  1.26it/s]\n",
      "\n",
      "  7%|█████▋                                                                          | 71/1000 [00:56<12:15,  1.26it/s]\n",
      "\n",
      "  7%|█████▊                                                                          | 72/1000 [00:57<12:06,  1.28it/s]\n",
      "\n",
      "  7%|█████▊                                                                          | 73/1000 [00:57<11:58,  1.29it/s]\n",
      "\n",
      "  7%|█████▉                                                                          | 74/1000 [00:58<12:00,  1.29it/s]\n",
      "\n",
      "  8%|██████                                                                          | 75/1000 [00:59<11:59,  1.29it/s]\n",
      "\n",
      "  8%|██████                                                                          | 76/1000 [01:00<12:00,  1.28it/s]\n",
      "\n",
      "  8%|██████▏                                                                         | 77/1000 [01:01<12:00,  1.28it/s]\n",
      "\n",
      "  8%|██████▏                                                                         | 78/1000 [01:01<12:01,  1.28it/s]\n",
      "\n",
      "  8%|██████▎                                                                         | 79/1000 [01:02<11:58,  1.28it/s]\n",
      "\n",
      "  8%|██████▍                                                                         | 80/1000 [01:03<11:58,  1.28it/s]\n",
      "\n",
      "  8%|██████▍                                                                         | 81/1000 [01:04<12:05,  1.27it/s]\n",
      "\n",
      "  8%|██████▌                                                                         | 82/1000 [01:05<12:05,  1.27it/s]\n",
      "\n",
      "  8%|██████▋                                                                         | 83/1000 [01:05<11:59,  1.27it/s]\n",
      "\n",
      "  8%|██████▋                                                                         | 84/1000 [01:06<11:58,  1.28it/s]\n",
      "\n",
      "  8%|██████▊                                                                         | 85/1000 [01:07<12:03,  1.26it/s]\n",
      "\n",
      "  9%|██████▉                                                                         | 86/1000 [01:08<12:03,  1.26it/s]\n",
      "\n",
      "  9%|██████▉                                                                         | 87/1000 [01:09<12:04,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "pmiposlist_vader, pmineglist_vader = pmiForAllCal(finaldf_phrase, topk=topk_phrase,label_column='Binary_vader')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
