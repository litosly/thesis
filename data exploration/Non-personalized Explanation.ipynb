{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "from ast import literal_eval\n",
    "\n",
    "# For Python2 this have to be done\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Original Data\n",
    "\n",
    "dataset_name = \"beer\"\n",
    "# dataset_name = \"CDsVinyl\"\n",
    "\n",
    "# df_train = pd.read_csv('../../data/'+ dataset_name +'/Train.csv')\n",
    "# df_valid = pd.read_csv('../../data/'+ dataset_name +'/Valid.csv')\n",
    "# df_test = pd.read_csv('../../data/'+ dataset_name +'/Test.csv')\n",
    "key_phrase = pd.read_csv('../data/'+ dataset_name +'/KeyPhrases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load U-I Data \n",
    "rtrain = load_npz(\"../../\"+ dataset_name +\"/Rtrain.npz\")\n",
    "# rvalid = load_npz(\"../../data/\"+ dataset_name +\"/Rvalid.npz\")\n",
    "# rtest = load_npz(\"../../data/\"+ dataset_name +\"/Rtest.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load U-K and I-K Data\n",
    "Normalize = True\n",
    "\n",
    "if Normalize == True:\n",
    "    U_K = normalize(load_npz(\"../../\"+ dataset_name +\"/U_K.npz\"))\n",
    "    I_K = normalize(load_npz(\"../../\"+ dataset_name +\"/I_K.npz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train, model = \"Cosine_similarity\"):\n",
    "    if model == \"Cosine_similarity\":\n",
    "        similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, row_name = 'ItemIndex', shape = (3668,75)):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        key_vector = literal_eval(df['keyVector'][i])\n",
    "        rows.extend([df[row_name][i]]*len(key_vector)) ## Item index\n",
    "        cols.extend(key_vector) ## Keyword Index\n",
    "#         if binary:\n",
    "        vals.extend(np.array([1]*len(key_vector)))\n",
    "#         else:\n",
    "#             vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    \"\"\"\n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    \"\"\"\n",
    "    prediction_scores = []\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores to all users\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "\n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        vector_u = prediction_score[user_index]\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "\n",
    "def sub_routine(vector_u, vector_train, topK=50):\n",
    "    \"\"\"\n",
    "    vector_u: predicted user vector\n",
    "    vector_train: true user vector\n",
    "    topK: top k items in vector\n",
    "    vector_u: top k items predicted\n",
    "    \"\"\"\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    return vector_u[:topK]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        \"R-Precision\": r_precision,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = results[name]\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (np.average(results[name]),\n",
    "                                                              1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = results[name]\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (np.average(results[name]), 1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(R,W2,k, model = \"Cosine_similarity\", item_similarity_en = True):\n",
    "    \"\"\"\n",
    "    k: knn's hyperparameter k\n",
    "    R: Rating Matrix with size U*I\n",
    "    r_ij: observed rating with user i and item j \n",
    "    s_ij: explanation vector with user i and item j \n",
    "    Z: Joint Embedding/Latent Space with size U*U, generate r_ij and s_ij\n",
    "    W2: Reconstruction matrix with size U*K \n",
    "    S: Output explanation prediction matrix with size U*K (dense numpy ndarray)\n",
    "    \"\"\"\n",
    "    Z = train(R) # Cosine similarity as default\n",
    "    S = predict(W2, k, Z, item_similarity_en=item_similarity_en) \n",
    "    if normalize_en == True:       \n",
    "        return normalize(S) # prediction score\n",
    "    return S\n",
    "\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False, normalize_en = False):\n",
    "    \"\"\"\n",
    "    matrix_train: Rating Matrix with size U*I\n",
    "    k: knn's hyperparameter k\n",
    "    similarity: Joint Embedding/Latent Space with size U*U or I*I\n",
    "    \n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    \n",
    "    r_ij: observed rating with user i and item j \n",
    "    s_ij: explanation vector with user i and item j \n",
    "    \"\"\"\n",
    "    prediction_scores = []\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores to all users\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "\n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    if normalize_en:\n",
    "        res = normalize(res)\n",
    "    return res\n",
    "\n",
    "def explain_prediction(prediction_score, topK, matrix_Train):\n",
    "    \"\"\"\n",
    "    output prediction res of the  top K items/keyphrase/whatever\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        vector_u = prediction_score[user_index]\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine_explain(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "    return np.vstack(prediction)\n",
    "#     return prediction\n",
    "\n",
    "def sub_routine_explain(vector_u, vector_train, topK=30):\n",
    "    \"\"\"\n",
    "    vector_u: predicted user vector\n",
    "    vector_train: true user vector\n",
    "    topK: top k items in vector\n",
    "    vector_u: top k items predicted\n",
    "    \"\"\"\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "#     candidate_index = np.argpartition(-vector_u, topK+75)[:topK+75] #  10 here to make res consistent\n",
    "    candidate_index = np.argpartition(-vector_u, 74)[:topK]\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "#     vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "    \n",
    "    return vector_u[:topK]\n",
    "\n",
    "def predict_explanation(explanation_scores, top_keyphrase = 10):\n",
    "    explanation = []\n",
    "    for explanation_score in tqdm(explanation_scores):\n",
    "        explanation.append(np.argsort(explanation_score)[::-1][:top_keyphrase])\n",
    "    return np.array(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_explanation(df_predict, df_test, metric_names, atK, user_col,\n",
    "                         item_col, rating_col, keyphrase_vector_col):\n",
    "    df_test = df_test[(df_test[rating_col] == 1) & (df_test[keyphrase_vector_col] != '[]')] # Remove negatives reviews and reviews without keyphrases matched\n",
    "    df_test = df_test[[user_col, item_col, keyphrase_vector_col]]\n",
    "    df_test[keyphrase_vector_col] = df_test[keyphrase_vector_col].apply(lambda x: eval(x))\n",
    "    df_predict = df_predict[[user_col, item_col, 'ExplanIndex']]\n",
    "    res = pd.merge(df_test, df_predict, how='inner', on=[user_col, item_col])\n",
    "\n",
    "    global_metrics = {\n",
    "        # \"R-Precision\": r_precision,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_interactionss = len(res)\n",
    "\n",
    "    for k in atK:\n",
    "        res['hits'] = res.apply(lambda x: list(np.isin(x['ExplanIndex'][:k], x[keyphrase_vector_col])), axis=1)\n",
    "\n",
    "        global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "\n",
    "        for metric in tqdm(global_metric_names):\n",
    "            res[metric] = res.apply(lambda x: global_metrics[metric](vector_true_dense=x[keyphrase_vector_col],\n",
    "                                                                     vector_predict=x['ExplanIndex'][:k],\n",
    "                                                                     hits=x['hits']),\n",
    "                                    axis=1)\n",
    "        results_summary = dict()\n",
    "        for name in global_metric_names:\n",
    "            results_summary['{0}@{1}'.format(name, k)] = (np.average(res[name]),\n",
    "                                                          1.96 * np.std(res[name]) / np.sqrt(num_interactionss))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U_I*I_U = U*U\n",
    "U*K*U_U = U*K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similarity = train(rtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6370/6370 [00:04<00:00, 1294.72it/s]\n"
     ]
    }
   ],
   "source": [
    "explanation_scores = predict(U_K, 100, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 6370/6370 [00:00<00:00, 113750.01it/s]\n"
     ]
    }
   ],
   "source": [
    "explanation =  predict_explanation(explanation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Should compare the results here with U_K/I_K in validation/test set for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6370/6370 [00:01<00:00, 3590.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 6370/6370 [00:02<00:00, 2502.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.97862585034013616, 0.0017923416440049878),\n",
       " 'NDCG': (0.35438898459015533, 0.0017954792542171048),\n",
       " 'Precision@10': (0.96654631083202525, 0.0020995579999948662),\n",
       " 'R-Precision': (0.20753293870580852, 0.0016496100715197373),\n",
       " 'Recall@10': (0.20755536525884735, 0.0016531752929639524)}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(explanation, U_K, atK=[10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "I_U*U_I = I*I \n",
    "I_K*I_I = I*K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U_I*I_U = U*U\n",
    "U_I*U_U = U_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
