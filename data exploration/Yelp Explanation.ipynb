{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "from ast import literal_eval\n",
    "\n",
    "# For Python2 this have to be done\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/yelp/Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse_matrix(df, num_user, num_item, user_col, item_col, rating_col):\n",
    "\n",
    "    dok = df[[user_col, item_col, rating_col]].copy()\n",
    "    dok = dok.values\n",
    "    dok = dok[dok[:, 2] > 0]\n",
    "    shape = [num_user, num_item]\n",
    "\n",
    "    return sparse.csr_matrix((dok[:, 2].astype(np.float32), (dok[:, 0], dok[:, 1])), shape=shape)\n",
    "\n",
    "def leave_one_out_split(df, user_col, ratio, random_state=None):\n",
    "    grouped = df.groupby(user_col, as_index=False)\n",
    "    valid = grouped.apply(lambda x: x.sample(frac=ratio, random_state=random_state))\n",
    "    train = df.loc[~df.index.isin([x[1] for x in valid.index])]\n",
    "    return train, valid\n",
    "\n",
    "def time_ordered_split(df, ratio, user_col = None, random_state=None):\n",
    "    # Sort data based on timestamp\n",
    "    argsort = np.argsort(df['timestamp'])\n",
    "    df_ordered = df.reindex(argsort)\n",
    "    train_offset = int((1-ratio)*len(df_ordered))\n",
    "    \n",
    "    train = df_ordered[:train_offset]\n",
    "    valid = df_ordered[train_offset:]\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def main(enable_validation = False, time_ordered_split_en = True, implicit_en = False):\n",
    "    df = pd.read_csv('../../data/yelp/' + 'Data.csv')\n",
    "\n",
    "    num_users = df['UserIndex'].nunique()\n",
    "    num_items = df['ItemIndex'].nunique()\n",
    "\n",
    "    # Get timestamp \n",
    "    date_time_df = df[['Day','Month','Year']]\n",
    "    date_time_df.rename(columns={'Year': 'year', 'Month': 'day', 'Day':'month'}, inplace=True)\n",
    "    date_time = pd.to_datetime(date_time_df)\n",
    "    df['timestamp'] = date_time\n",
    "\n",
    "    rating_col = 'rating'\n",
    "    if implicit_en == True:\n",
    "        rating_col = 'Binary'\n",
    "    \n",
    "    if time_ordered_split_en:\n",
    "        df_train, df_test = time_ordered_split(df, 0.2)\n",
    "    else:\n",
    "        df_train, df_test = leave_one_out_split(df, 'UserIndex', 0.2, random_state=8292)\n",
    "\n",
    "    if enable_validation:\n",
    "        if time_ordered_split_en:\n",
    "            df_train, df_valid = time_ordered_split(df_train, 0.2)\n",
    "        else:\n",
    "            df_train, df_valid = leave_one_out_split(df_train, 'UserIndex', 0.2, random_state=8292)\n",
    "        \n",
    "        # Clean empty rows\n",
    "        df_valid = df_valid.dropna().reset_index(drop = True)\n",
    "        \n",
    "        # Save\n",
    "        df_valid.to_csv('../../data/yelp/' + 'Valid.csv')\n",
    "        R_valid = to_sparse_matrix(df_valid, num_users, num_items, 'UserIndex','ItemIndex', rating_col)\n",
    "        sparse.save_npz('../../data/yelp/' + 'Rvalid.npz', R_valid)\n",
    "    \n",
    "    # Clean empty rows\n",
    "    df_train = df_train.dropna().reset_index(drop = True)\n",
    "    df_test = df_test.dropna().reset_index(drop = True)\n",
    "    \n",
    "    # Save\n",
    "    df_train.to_csv('../../data/yelp/'  + 'Train.csv')\n",
    "    R_train = to_sparse_matrix(df_train, num_users, num_items, 'UserIndex', 'ItemIndex', rating_col)\n",
    "    sparse.save_npz('../../data/yelp/' + 'Rtrain.npz', R_train)\n",
    "\n",
    "    df_test.to_csv('../../data/yelp/' + 'Test.csv')\n",
    "    R_test = to_sparse_matrix(df_test, num_users, num_items, 'UserIndex', 'ItemIndex', rating_col)\n",
    "    sparse.save_npz('../../data/yelp/' + 'Rtest.npz', R_test)\n",
    "    \n",
    "def date_to_timestamp(date, **not_used):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'Unnamed: 0.1', u'Unnamed: 0.1.1', u'business_id',\n",
       "       u'friend_count', u'ghost', u'img_dsc', u'img_url', u'nr',\n",
       "       u'photo_count', u'rating', u'review_count', u'review_date',\n",
       "       u'review_id', u'review_language', u'review_text', u'ufc', u'user_id',\n",
       "       u'user_loc', u'vote_count', u'Updated', u'Year', u'Month', u'Day',\n",
       "       u'Binary', u'review', u'conca_review', u'keyVector',\n",
       "       u'keyphrases_indices_length', u'UserIndex', u'ItemIndex'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main(enable_validation = True, time_ordered_split_en = False, implicit_en = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Original Data\n",
    "df_train = pd.read_csv('../../data/yelp/Train.csv')\n",
    "df_valid = pd.read_csv('../../data/yelp/Valid.csv')\n",
    "df_test = pd.read_csv('../../data/yelp/Test.csv')\n",
    "keyphrases = pd.read_csv('../../data/yelp/KeyPhrases.csv')['Phrases'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ItemIndex = pd.read_csv('../../data/yelp/ItemIndex.csv')\n",
    "ItemIndex = ItemIndex.sort_values('ItemIndex').drop_duplicates(subset=['ItemIndex', 'business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load U-I Data \n",
    "rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")\n",
    "rvalid = load_npz(\"../../data/yelp/Rvalid.npz\")\n",
    "rtest = load_npz(\"../../data/yelp/Rtest.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2473x10282 sparse matrix of type '<type 'numpy.float32'>'\n",
       "\twith 102741 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 102741/102741 [00:06<00:00, 17012.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 102741/102741 [00:05<00:00, 17493.79it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 32133/32133 [00:01<00:00, 17636.11it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 32133/32133 [00:01<00:00, 17665.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate U_K and I_K\n",
    "# For validation set\n",
    "U_K = get_I_K(df_train, row_name = 'UserIndex', shape = (2473, 237))\n",
    "I_K = get_I_K(df_train, row_name = 'ItemIndex', shape = (10282, 237))\n",
    "# For test set\n",
    "U_K_test = get_I_K(df_test, row_name = 'UserIndex', shape = (2473, 237))\n",
    "I_K_test = get_I_K(df_test, row_name = 'ItemIndex', shape = (10282, 237))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "save_npz('../../data/yelp/U_K.npz',U_K)\n",
    "save_npz('../../data/yelp/I_K.npz',I_K)\n",
    "save_npz( '../../data/yelp/U_K_test.npz',U_K_test)\n",
    "save_npz('../../data/yelp/I_K_test.npz',I_K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load \n",
    "U_K = load_npz('../../data/yelp/U_K.npz')\n",
    "I_K = load_npz('../../data/yelp/I_K.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, row_name = 'ItemIndex', shape = (3668,75)):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        key_vector = literal_eval(df['keyVector'][i])\n",
    "        rows.extend([df[row_name][i]]*len(key_vector)) ## Item index\n",
    "        cols.extend(key_vector) ## Keyword Index\n",
    "#         if binary:\n",
    "        vals.extend(np.array([1]*len(key_vector)))\n",
    "#         else:\n",
    "#             vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    \"\"\"\n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    \"\"\"\n",
    "    prediction_scores = []\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores to all users\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "\n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        vector_u = prediction_score[user_index]\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "\n",
    "    vector_u = vector_u\n",
    "\n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    return vector_u[:topK]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evluation \n",
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        \"R-Precision\": r_precision,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = results[name]\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (np.average(results[name]),\n",
    "                                                              1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = results[name]\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (np.average(results[name]), 1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output\n",
    "\n",
    "def explain_evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        \"R-Precision\": r_precision,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "#                 vector_true = matrix_Test[user_index]\n",
    "                vector_true = np.ravel(matrix_Test.todense()[0])\n",
    "                vector_true_dense = np.argsort(vector_true)[::-1][:k]\n",
    "#                 vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = results[name]\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (np.average(results[name]),\n",
    "                                                              1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "#             vector_true = matrix_Test[user_index]\n",
    "#             vector_true_dense = vector_true.nonzero()[1]\n",
    "            vector_true = np.ravel(matrix_Test.todense()[0])\n",
    "            vector_true_dense = np.argsort(vector_true)[::-1][:k]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = results[name]\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (np.average(results[name]), 1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explain(R,W2,k, model = \"Cosine_similarity\", item_similarity_en = True):\n",
    "    \"\"\"\n",
    "    k: knn's hyperparameter k\n",
    "    R: Rating Matrix with size U*I\n",
    "    r_ij: observed rating with user i and item j \n",
    "    s_ij: explanation vector with user i and item j \n",
    "    Z: Joint Embedding/Latent Space with size U*U, generate r_ij and s_ij\n",
    "    W2: Reconstruction matrix with size U*K \n",
    "    S: Output explanation prediction matrix with size U*K (dense numpy ndarray)\n",
    "    \"\"\"\n",
    "    Z = train(R) # Cosine similarity as default\n",
    "    S = predict(W2, k, Z, item_similarity_en=item_similarity_en) \n",
    "    if normalize_en == True:       \n",
    "        return normalize(S) # prediction score\n",
    "    return S\n",
    "\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False, normalize_en = False):\n",
    "    \"\"\"\n",
    "    matrix_train: Rating Matrix with size U*I\n",
    "    k: knn's hyperparameter k\n",
    "    similarity: Joint Embedding/Latent Space with size U*U or I*I\n",
    "    \n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    \n",
    "    r_ij: observed rating with user i and item j \n",
    "    s_ij: explanation vector with user i and item j \n",
    "    \"\"\"\n",
    "    prediction_scores = []\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores to all users\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "\n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    if normalize_en:\n",
    "        res = normalize(res)\n",
    "    return res\n",
    "\n",
    "def explain_prediction(prediction_score, topK, matrix_Train):\n",
    "    \"\"\"\n",
    "    output prediction res of the  top K items/keyphrase/whatever\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        vector_u = prediction_score[user_index]\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine_explain(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "    return np.vstack(prediction)\n",
    "#     return prediction\n",
    "\n",
    "def sub_routine_explain(vector_u, vector_train, topK=30):\n",
    "    \"\"\"\n",
    "    vector_u: predicted user vector\n",
    "    vector_train: true user vector\n",
    "    topK: top k items in vector\n",
    "    vector_u: top k items predicted\n",
    "    \"\"\"\n",
    "#     train_index = vector_train.nonzero()[1]\n",
    "#     candidate_index = np.argpartition(-vector_u, topK+75)[:topK+75] #  10 here to make res consistent\n",
    "#     candidate_index = np.argpartition(-vector_u, 74)[:topK]\n",
    "#     vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "#     vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    candidate_index = np.argsort(vector_u)[::-1][:topK]\n",
    "    return candidate_index\n",
    "\n",
    "def predict_pilot_explanation(explanation_scores, top_keyphrase = 10):\n",
    "    \"\"\"\n",
    "    Used for retrieve the 1st row of prediction scores, used for pilot test\n",
    "    \"\"\"\n",
    "    explanation = []\n",
    "    for explanation_score in tqdm(explanation_scores):\n",
    "        explanation.append(np.argsort(explanation_score)[::-1][:top_keyphrase])\n",
    "    return np.array(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explain(R,W2,k):\n",
    "    \"\"\"\n",
    "    R: Rating Matrix with size U*I\n",
    "    r_ij: observed rating with user i and item j \n",
    "    s_ij: explanation vector with user i and item j \n",
    "    Z: Joint Embedding/Latent Space with size U*U, generate r_ij and s_ij\n",
    "    W2: Reconstruction matrix with size U*K \n",
    "    S: Output explanation prediction matrix with size U*K\n",
    "    \"\"\"\n",
    "    Z = train(R)\n",
    "    S = predict(W2, k, Z)\n",
    "    return normalize(S)\n",
    "\n",
    "# Evaluation Model\n",
    "def recall(vector_true_dense, vector_true_predict):\n",
    "    \"\"\"\n",
    "    The fraction of relevant instances that have been retrieved over the total amount of relevant instances\n",
    "    The length of vector_true_dense and vector_true_predict has to be the same\n",
    "    Out put recall\n",
    "    \"\"\"\n",
    "    hits = len(np.isin(vector_true_predict, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "# Total Recall\n",
    "def recall_all(true_matrix, predict_matrix, topK = 20):\n",
    "    res = []\n",
    "    for i in tqdm(range(len(Explanation_res1))):\n",
    "        true_vector = np.argsort(np.ravel(normalize(true_matrix).todense()[i]))[-topK:]\n",
    "        predict_vector = np.argsort(predict_matrix[i])[-topK:]\n",
    "        res.append(recall(true_vector,predict_vector))\n",
    "    return sum(res)/len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explicit with time-ordered split (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:01<00:00, 1840.03it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:00<00:00, 7562.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# ( U_I * I_U ) * U_K\n",
    "similarity = train(rtrain)\n",
    "explanation_scores = predict(U_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 100, U_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 932.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 920.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 925.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 943.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 861.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.25586715994268083, 0.001353590654717463),\n",
       " 'MAP@20': (0.3366685134557135, 0.0011824646421380184),\n",
       " 'MAP@5': (0.0031510416666666666, 0.0004847510631302727),\n",
       " 'MAP@50': (0.5610333179971825, 0.0011967631690340385),\n",
       " 'NDCG': (0.6087433758971473, 0.0013926192605863022),\n",
       " 'Precision@10': (0.22005208333333334, 0.001578073907642935),\n",
       " 'Precision@20': (0.27421874999999996, 0.0012901125154105365),\n",
       " 'Precision@5': (0.013802083333333333, 0.0019980380038774437),\n",
       " 'Precision@50': (0.47991319444444447, 0.0014071444362840457),\n",
       " 'R-Precision': (0.47991319444444447, 0.0014071444362840457),\n",
       " 'Recall@10': (0.22005208333333334, 0.001578073907642935),\n",
       " 'Recall@20': (0.27421874999999996, 0.0012901125154105365),\n",
       " 'Recall@5': (0.013802083333333333, 0.0019980380038774437),\n",
       " 'Recall@50': (0.47991319444444447, 0.0014071444362840457)}"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, U_K_test, atK=[5,10,20,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explicit with time-ordered split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:01<00:00, 1809.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 7414.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# ( U_I * I_U ) * U_K\n",
    "similarity = train(rtrain)\n",
    "explanation_scores = predict(U_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 100, U_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 1053.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 1079.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 1062.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 1019.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 1018.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.49012323675471464, 0.001752167730460859),\n",
       " 'MAP@30': (0.5868575706982433, 0.0011499213639099082),\n",
       " 'MAP@5': (0.6690339091490721, 0.0027927379124820764),\n",
       " 'MAP@50': (0.5560703129913159, 0.0008912113533278475),\n",
       " 'NDCG': (0.606270925862621, 0.0007611411843798994),\n",
       " 'Precision@10': (0.28968330134356995, 0.0012569661306957034),\n",
       " 'Precision@30': (0.42738323736404354, 0.0012872793817335507),\n",
       " 'Precision@5': (0.4001919385796545, 0.0005016998301376581),\n",
       " 'Precision@50': (0.39292706333973126, 0.000860039642773129),\n",
       " 'R-Precision': (0.39292706333973126, 0.000860039642773129),\n",
       " 'Recall@10': (0.28968330134356995, 0.0012569661306957034),\n",
       " 'Recall@30': (0.42738323736404354, 0.0012872793817335507),\n",
       " 'Recall@5': (0.4001919385796545, 0.0005016998301376581),\n",
       " 'Recall@50': (0.39292706333973126, 0.000860039642773129)}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, U_K_test, atK=[5,10,30,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit without time-ordered split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:01<00:00, 2128.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 7607.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# ( U_I * I_U ) * U_K\n",
    "similarity = train(rtrain)\n",
    "explanation_scores = predict(U_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 100, U_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 939.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 889.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 913.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 882.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@20': (0.45316721230242296, 0.0019350810812115379),\n",
       " 'MAP@40': (0.5842206191206047, 0.002377891946403873),\n",
       " 'MAP@5': (0.623118509034002, 0.0029496286180747486),\n",
       " 'NDCG': (0.7268562260242788, 0.0021141846532503274),\n",
       " 'Precision@20': (0.3027102005975245, 0.0018155475022447942),\n",
       " 'Precision@40': (0.5250213401621853, 0.0021673342673273256),\n",
       " 'Precision@5': (0.3977806231327358, 0.0016873251019215718),\n",
       " 'R-Precision': (0.5250213401621853, 0.0021673342673273256),\n",
       " 'Recall@20': (0.3027102005975245, 0.0018155475022447942),\n",
       " 'Recall@40': (0.5250213401621853, 0.0021673342673273256),\n",
       " 'Recall@5': (0.3977806231327358, 0.0016873251019215718)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, U_K_test, atK=[5,20,40]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-based Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explicit with time-oredered split (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10282/10282 [00:06<00:00, 1666.18it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10282/10282 [00:01<00:00, 7276.72it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity = train(np.transpose(rtrain))\n",
    "explanation_scores = predict(I_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 70, I_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:40<00:00, 252.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:42<00:00, 242.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:43<00:00, 235.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:44<00:00, 233.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:45<00:00, 224.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.6154823600942113, 0.001724659791955),\n",
       " 'MAP@30': (0.5114275408681419, 0.000994026740346734),\n",
       " 'MAP@5': (0.6862275706457504, 0.001730412184158496),\n",
       " 'MAP@50': (0.43082281538800626, 0.0007338316374898346),\n",
       " 'NDCG': (0.4116329054547845, 0.0006277699440078318),\n",
       " 'Precision@10': (0.46250134307510476, 0.0015506019823310372),\n",
       " 'Precision@30': (0.3161419719924071, 0.0007302846569774447),\n",
       " 'Precision@5': (0.4319974212957989, 0.0015513551750021481),\n",
       " 'Precision@50': (0.27950145052111314, 0.0005713366085377778),\n",
       " 'R-Precision': (0.27950145052111314, 0.0005713366085377778),\n",
       " 'Recall@10': (0.46250134307510476, 0.0015506019823310372),\n",
       " 'Recall@30': (0.3161419719924071, 0.0007302846569774447),\n",
       " 'Recall@5': (0.4319974212957989, 0.0015513551750021481),\n",
       " 'Recall@50': (0.27950145052111314, 0.0005713366085377778)}"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, I_K, atK=[5,10,30,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explicit without time-oredered split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:03<00:00, 2005.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:00<00:00, 8034.48it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity = train(np.transpose(rtrain))\n",
    "explanation_scores = predict(I_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 70, I_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:13<00:00, 561.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:13<00:00, 560.94it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:13<00:00, 561.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:13<00:00, 552.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@20': (0.18036523094037374, 0.0006643399155876114),\n",
       " 'MAP@5': (0.18969546954695468, 0.0013948017800600933),\n",
       " 'MAP@50': (0.204786603725808, 0.0005440288534445658),\n",
       " 'NDCG': (0.22373220342456343, 0.0005602887409655968),\n",
       " 'Precision@20': (0.12375112511251127, 0.0007850385182262991),\n",
       " 'Precision@5': (0.19963996399639963, 0.000215195479214176),\n",
       " 'Precision@50': (0.1786993699369937, 0.0006504945026639768),\n",
       " 'R-Precision': (0.1786993699369937, 0.0006504945026639768),\n",
       " 'Recall@20': (0.12375112511251127, 0.0007850385182262991),\n",
       " 'Recall@5': (0.19963996399639963, 0.000215195479214176),\n",
       " 'Recall@50': (0.1786993699369937, 0.0006504945026639768)}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, I_K_test, atK=[5,20,50]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand item-based Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get restaurant names correspond to ItemIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_business_df(path = \"../../data/yelp/business.json\" ):\n",
    "    with open(path) as json_file:\n",
    "        data = json_file.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_restaurant_info(business_df, business_id, name = True, review_count = True, stars = True ):\n",
    "    output_list = {}\n",
    "    row_idx = int(business_df.index[business_df['business_id'] == business_id].tolist()[0])\n",
    "    if name == True:\n",
    "        output_list['name'] = business_df['name'][row_idx].encode('utf-8').strip()\n",
    "    if review_count == True:\n",
    "        output_list['review_count'] = business_df['review_count'][row_idx]\n",
    "    if stars == True:\n",
    "        output_list['stars'] = business_df['stars'][row_idx] \n",
    "    return output_list\n",
    "\n",
    "def get_businessid_from_Itemindex(ItemIndex_list, itemindex):\n",
    "    return ItemIndex_list['business_id'].tolist()[itemindex]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = get_business_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l_uAw0K2lkOsyVJATcnwsA'"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_businessid_from_Itemindex(ItemIndex, 8010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Spicy Mafia', 'review_count': 9, 'stars': 3.0}"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_restaurant_info(business_df, 'l_uAw0K2lkOsyVJATcnwsA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2343x7456 sparse matrix of type '<type 'numpy.float32'>'\n",
       "\twith 75764 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find specific Restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35611     Sushi On Bloor\n",
      "114653    Sushi On Bloor\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['sushi on bloor' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7031                       Spicy Mafia\n",
      "28409                 Caricature Mafia\n",
      "51959               Mafia Mike's Pizza\n",
      "113184    Cakefacemafia Brows & Beauty\n",
      "119974                     Spicy Mafia\n",
      "140192                    Mafia Mike's\n",
      "156211                  La'Bella MAFIA\n",
      "180250              Mafia Mike's PIzza\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['mafia' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74627     Crown Prince Fine Dining & Banquet\n",
      "106186            Crown Princess Fine Dining\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['crown prince' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4501                  Red Lobster\n",
      "9494                  Red Lobster\n",
      "9599                  Red Lobster\n",
      "11793                 Red Lobster\n",
      "15146                 Red Lobster\n",
      "21612                 Red Lobster\n",
      "25103                 Red Lobster\n",
      "37750                 Red Lobster\n",
      "39311                 Red Lobster\n",
      "42883                 Red Lobster\n",
      "46058                 Red Lobster\n",
      "46742                 Red Lobster\n",
      "51418                 Red Lobster\n",
      "51780                 Red Lobster\n",
      "52374                 Red Lobster\n",
      "56591                 Red Lobster\n",
      "66201                 Red Lobster\n",
      "68260                 Red Lobster\n",
      "70318                 Red Lobster\n",
      "79983                 Red Lobster\n",
      "105382                Red Lobster\n",
      "108167                Red Lobster\n",
      "112307                Red Lobster\n",
      "115861                Red Lobster\n",
      "119864                Red Lobster\n",
      "126004                Red Lobster\n",
      "134059                Red Lobster\n",
      "140933                Red Lobster\n",
      "145037                Red Lobster\n",
      "154559                Red Lobster\n",
      "159619                Red Lobster\n",
      "161507                Red Lobster\n",
      "162753                Red Lobster\n",
      "163962                Red Lobster\n",
      "167619    Red Lobster Restaurants\n",
      "176109                Red Lobster\n",
      "179596                Red Lobster\n",
      "179732                Red Lobster\n",
      "184775                Red Lobster\n",
      "185524                Red Lobster\n",
      "185736                Red Lobster\n",
      "188347                Red Lobster\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['red lobster' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73128    The Queen And Beaver Public House\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['queen and beaver' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107476                            Miku\n",
      "116702    Mikush Home Appliance Center\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['miku' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the restaurant exists in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0a2O150ytxrDjDzXNfRWkA'"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df['business_id'][107476]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_itemindex_from_business_id(ItemIndex, '0a2O150ytxrDjDzXNfRWkA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(df.ItemIndex == 8010)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Prediction with Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_itemindex_from_business_id(ItemIndex_list, business_id):\n",
    "    business_id_list = ItemIndex_list['business_id'].tolist()\n",
    "    return business_id_list.index(business_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spicy Mafia (fewer reviews Chinese food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 9, 'name': 'Spicy Mafia', 'stars': 3.0}\n",
      "8010\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, 'l_uAw0K2lkOsyVJATcnwsA')\n",
    "print get_itemindex_from_business_id(ItemIndex, 'l_uAw0K2lkOsyVJATcnwsA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rice', 'noodle', 'wait', 'thai', 'beef', 'art', 'tea', 'pork',\n",
       "       'mall', 'chicken', 'soup', 'spicy', 'quick', 'fresh', 'curry',\n",
       "       'pot', 'meat', 'lunch', 'busy', 'friendly', 'egg', 'dinner',\n",
       "       'fast', 'fried', 'bar', 'salad', 'vietnamese', 'milk', 'pop',\n",
       "       'tart', 'fry', 'kimchi', 'cocktail', 'clean', 'downtown', 'chewy',\n",
       "       'shrimp', 'salt', 'pasta', 'bubble', 'crispy', 'bubble tea',\n",
       "       'coconut', 'belly', 'tapioca', 'spring roll', 'milk tea', 'fair',\n",
       "       'tofu', 'wine', 'cake', 'fish', 'pork belly', 'sour', 'yummy',\n",
       "       'pizza', 'stick', 'chinese', 'sandwich', 'asian', 'fruit', 'bun',\n",
       "       'bean', 'cheese', 'attentive', 'rib', 'chili', 'tuna', 'wing',\n",
       "       'crunchy'], dtype='|S19')"
      ]
     },
     "execution_count": 709,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Predicted\n",
    "test_list = list(map(int, explanation[8010])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rice, noodle, wait, thai, beef, art, tea, pork, mall, chicken, soup, spicy, quick, fresh, curry, pot, meat, lunch, busy, friendly]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tomato', 'noodle', 'egg', 'fish', 'art', 'pork', 'pot', 'lunch',\n",
       "       'soup', 'meat', 'tofu', 'busy', 'tart', 'apple', 'strawberry',\n",
       "       'avocado', 'pop', 'lettuce', 'miso', 'juice', 'skewer', 'scallop',\n",
       "       'congee', 'calamari', 'cone', 'honey', 'cookie', 'banana',\n",
       "       'croissant', 'octopus', 'espresso', 'olive', 'donut', 'booth',\n",
       "       'sesame', 'kimchi', 'oyster', 'bacon', 'mango', 'lamb', 'sashimi',\n",
       "       'duck', 'pancake', 'matcha', 'latte', 'sausage', 'fruit',\n",
       "       'cheesecake', 'cocktail', 'bubble', 'patty', 'belly', 'toast',\n",
       "       'poutine', 'corn', 'coconut', 'vegan', 'lemon', 'wrap', 'tuna',\n",
       "       'crepe', 'four', 'tempura', 'tapioca', 'accept debit', 'squid',\n",
       "       'takeout', 'downtown', 'vegetarian', 'birthday'], dtype='|S19')"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Ground Truth\n",
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[8010].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tomato, noodle, egg, fish, art, pork, pot, lunch, soup, meat, tofu, busy, tart, apple, strawberry, avocado, pop, lettuce, miso, juice]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[np.argsort(np.ravel(I_K[8010].todense()))[::-1][:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hit\n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[8010].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crown Princess Fine Dining (many reviews Chinese food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 235, 'name': 'Crown Princess Fine Dining', 'stars': 3.5}\n",
      "8517\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, 'ovlWOSKVjGecnaPuZLv_OQ')\n",
    "print get_itemindex_from_business_id(ItemIndex, 'ovlWOSKVjGecnaPuZLv_OQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rice', 'art', 'wait', 'thai', 'tea', 'mall', 'fresh', 'soup',\n",
       "       'pot', 'noodle', 'bar', 'chicken', 'beef', 'friendly', 'dumpling',\n",
       "       'dinner', 'curry', 'spicy', 'egg', 'pork', 'busy', 'tart', 'lunch',\n",
       "       'quick', 'fried', 'clean', 'chinese', 'cake', 'fish', 'shrimp',\n",
       "       'sushi', 'rib', 'meat', 'dim sum', 'dessert', 'tuna', 'pancake',\n",
       "       'milk', 'fast', 'fair', 'crispy', 'stick', 'downtown', 'store',\n",
       "       'attentive', 'bun', 'steamed', 'tofu', 'salt', 'salad', 'parking',\n",
       "       'pop', 'coconut', 'seafood', 'wine', 'wing', 'bean', 'sour',\n",
       "       'window', 'asian', 'creamy', 'music', 'fry', 'wrap', 'beer',\n",
       "       'markham', 'comfortable', 'ice cream', 'reasonable', 'scallop'],\n",
       "      dtype='|S19')"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Predicted\n",
    "test_list = list(map(int, explanation[8517])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rice, art, wait, thai, tea, mall, fresh, soup, pot, noodle, bar, chicken, beef, friendly, dumpling, dinner, curry, spicy, egg, pork]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dim sum', 'rice', 'tea', 'art', 'chinese', 'dumpling', 'downtown',\n",
       "       'pork', 'wait', 'shrimp', 'bun', 'pot', 'fried', 'dinner',\n",
       "       'chicken', 'congee', 'egg', 'cake', 'dessert', 'tart', 'bbq',\n",
       "       'mall', 'fancy', 'rib', 'lunch', 'scallop', 'fresh', 'stick',\n",
       "       'noodle', 'pricey', 'busy', 'markham', 'wrap', 'squid', 'clean',\n",
       "       'duck', 'salt', 'crispy', 'bean', 'octopus', 'yummy', 'fair',\n",
       "       'attentive', 'tax', 'french', 'brunch', 'soup', 'refill', 'quick',\n",
       "       'corn', 'steamed', 'greasy', 'fast', 'curry', 'cheaper', 'solid',\n",
       "       'quiet', 'sesame', 'meat', 'disappointing', 'asian', 'parking',\n",
       "       'bar', 'baked', 'reasonable', 'crunchy', 'traditional', 'lobster',\n",
       "       'milk', 'comfortable'], dtype='|S19')"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Ground Truth\n",
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[8517].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dim sum, rice, tea, art, chinese, dumpling, downtown, pork, wait, shrimp, bun, pot, fried, dinner, chicken, congee, egg, cake, dessert, tart]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[np.argsort(np.ravel(I_K[8517].todense()))[::-1][:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hit\n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[8517].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Queen And Beaver Public House (many reviews dessert place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 18, 'name': 'Japango Sushi & Noodle Restaurant', 'stars': 3.5}\n",
      "6672\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, 'd_QHsjv4aPh2BKiHgk_Dcg')\n",
    "print get_itemindex_from_business_id(ItemIndex, 'd_QHsjv4aPh2BKiHgk_Dcg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['art', 'rice', 'mall', 'pot', 'wait', 'bar', 'friendly', 'beer',\n",
       "       'tea', 'fresh', 'chicken', 'pizza', 'beef', 'salad', 'sandwich',\n",
       "       'busy', 'quick', 'lunch', 'tart', 'meat', 'egg', 'store', 'dinner',\n",
       "       'cheese', 'fast', 'pop', 'soup', 'spicy', 'bun', 'pub', 'pork',\n",
       "       'ice cream', 'bread', 'wing', 'fry', 'dessert', 'tofu', 'cake',\n",
       "       'coffee', 'crust', 'fried', 'rib', 'stick', 'clean', 'burger',\n",
       "       'downtown', 'corn', 'roasted', 'cocktail', 'apple', 'tuna', 'bean',\n",
       "       'wine', 'noodle', 'potato', 'tomato', 'chocolate', 'crispy',\n",
       "       'movie', 'comfortable', 'bbq', 'shrimp', 'fair', 'helpful', 'thai',\n",
       "       'salt', 'cozy', 'topped', 'casual', 'chip'], dtype='|S19')"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = list(map(int, explanation[8764])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[art, rice, mall, pot, wait, bar, friendly, beer, tea, fresh, chicken, pizza, beef, salad, sandwich, busy, quick, lunch, tart, meat]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pub', 'beer', 'dinner', 'art', 'pot', 'rice', 'bar', 'burger',\n",
       "       'chip', 'mall', 'fish', 'egg', 'wait', 'tea', 'fry', 'cozy',\n",
       "       'fresh', 'brunch', 'beef', 'salt', 'cheese', 'cocktail', 'meat',\n",
       "       'bacon', 'comfortable', 'downtown', 'friendly', 'potato', 'chair',\n",
       "       'stick', 'pricey', 'pork', 'bun', 'bread', 'rib', 'lunch',\n",
       "       'greasy', 'patty', 'toast', 'traditional', 'fast', 'wing', 'quick',\n",
       "       'salad', 'tomato', 'tart', 'attentive', 'sausage', 'crispy',\n",
       "       'lamb', 'clean', 'gravy', 'pleasant', 'vegetarian', 'fancy',\n",
       "       'solid', 'dark', 'dessert', 'latte', 'breakfast', 'seasoned',\n",
       "       'washroom', 'lemon', 'refreshing', 'bean', 'fried', 'steak',\n",
       "       'rare', 'ice cream', 'pop'], dtype='|S19')"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[8764].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hit\n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[8764].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Lobster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 93, 'name': 'Red Lobster', 'stars': 3.0}\n",
      "7312\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, 'hTdJAjSZtHWwqqh5cCeAfA')\n",
    "print get_itemindex_from_business_id(ItemIndex, 'hTdJAjSZtHWwqqh5cCeAfA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rice', 'tea', 'art', 'chicken', 'mall', 'wait', 'friendly',\n",
       "       'fresh', 'fast', 'pot', 'quick', 'milk', 'clean', 'bubble',\n",
       "       'bubble tea', 'busy', 'rib', 'dessert', 'fry', 'pop', 'cake',\n",
       "       'lunch', 'milk tea', 'egg', 'tart', 'salad', 'breakfast', 'beef',\n",
       "       'cheese', 'dinner', 'soup', 'store', 'noodle', 'fried', 'meat',\n",
       "       'tapioca', 'bar', 'spicy', 'burger', 'fish', 'curry', 'parking',\n",
       "       'steak', 'coffee', 'wing', 'fruit', 'chinese', 'corn', 'potato',\n",
       "       'bread', 'bacon', 'chocolate', 'fair', 'tax', 'mango', 'plaza',\n",
       "       'helpful', 'attentive', 'bean', 'crispy', 'toast', 'stick', 'bun',\n",
       "       'reasonable', 'lemon', 'sour', 'juicy', 'asian', 'salt',\n",
       "       'comfortable'], dtype='|S19')"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted\n",
    "test_list = list(map(int, explanation[7312])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lobster', 'wait', 'seafood', 'shrimp', 'friendly', 'fresh', 'art',\n",
       "       'mall', 'fish', 'pot', 'dinner', 'salad', 'dessert', 'lunch',\n",
       "       'potato', 'rice', 'tea', 'fried', 'refill', 'stuffed', 'four',\n",
       "       'quick', 'busy', 'bread', 'dip', 'crispy', 'pasta', 'bar',\n",
       "       'coconut', 'tart', 'soup', 'fry', 'baked', 'scallop', 'quiet',\n",
       "       'chocolate', 'cheese', 'parking', 'chicken', 'chip', 'bun', 'rib',\n",
       "       'tuna', 'greasy', 'salt', 'creamy', 'classic', 'casual',\n",
       "       'immediately', 'pop', 'pizza', 'crowded', 'beer', 'wing', 'topped',\n",
       "       'cheesecake', 'strawberry', 'roasted', 'markham', 'latte',\n",
       "       'steamed', 'frozen', 'meat', 'juicy', 'deep fried', 'ice cream',\n",
       "       'cookie', 'tomato', 'salmon', 'steak'], dtype='|S19')"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ground Truth\n",
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[7312].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hits \n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[7312].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 604, 'name': 'Miku', 'stars': 4.0}\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, '0a2O150ytxrDjDzXNfRWkA')\n",
    "print get_itemindex_from_business_id(ItemIndex, '0a2O150ytxrDjDzXNfRWkA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rice', 'wait', 'art', 'mall', 'tea', 'fresh', 'cake', 'pot',\n",
       "       'egg', 'meat', 'pork', 'dessert', 'chicken', 'friendly', 'tart',\n",
       "       'dinner', 'cheese', 'soup', 'busy', 'ice cream', 'bar', 'fish',\n",
       "       'noodle', 'quick', 'fried', 'beef', 'lunch', 'spicy', 'salad',\n",
       "       'pop', 'salt', 'sushi', 'matcha', 'japanese', 'rib', 'shrimp',\n",
       "       'fast', 'salmon', 'ramen', 'tuna', 'corn', 'creamy', 'crispy',\n",
       "       'seafood', 'coffee', 'milk', 'latte', 'wing', 'bean', 'oyster',\n",
       "       'waffle', 'clean', 'bbq', 'lobster', 'brunch', 'fair', 'pricey',\n",
       "       'parking', 'belly', 'cheesecake', 'attentive', 'sashimi',\n",
       "       'green tea', 'chocolate', 'cocktail', 'fry', 'pancake', 'yummy',\n",
       "       'bread', 'pork belly'], dtype='|S19')"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted\n",
    "test_list = list(map(int, explanation[273])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sushi', 'rice', 'dessert', 'salmon', 'fish', 'art', 'tea',\n",
       "       'fresh', 'sashimi', 'green tea', 'cake', 'dinner', 'japanese',\n",
       "       'matcha', 'lunch', 'tart', 'ice cream', 'tuna', 'wait',\n",
       "       'chocolate', 'mall', 'scallop', 'miso', 'salad', 'beef', 'latte',\n",
       "       'birthday', 'seafood', 'friendly', 'attentive', 'pot', 'rib',\n",
       "       'modern', 'tofu', 'oyster', 'shrimp', 'bar', 'bean', 'pricey',\n",
       "       'egg', 'cocktail', 'clean', 'nicely', 'lobster', 'fried', 'sesame',\n",
       "       'baked', 'quick', 'busy', 'wine', 'creamy', 'salt', 'calamari',\n",
       "       'spicy', 'traditional', 'crunchy', 'seasoned', 'meat', 'pop',\n",
       "       'dark', 'spacious', 'squid', 'four', 'fruit', 'crispy', 'stick',\n",
       "       'bacon', 'potato', 'bread', 'topped'], dtype='|S19')"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ground Truth\n",
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[273].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hits \n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[273].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top_items items associated with keyphrase_ids \n",
    "def item_associated_with_keyphrase(I_K, keyphrase_ids, top_items = 100):\n",
    "    \"\"\"\n",
    "    I_K: Item Keyphrase Matrix\n",
    "    Keyphrase_ids: top items described by keyphrase_ids\n",
    "    top_n: how many top items described by the keyphrases will be output \n",
    "    output item list (unique)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for keyphrase_id in keyphrase_ids:\n",
    "        res.append(np.argsort(np.ravel(I_K.todense()[:,keyphrase_id]))[::-1][:top_items])\n",
    "    return np.unique(res)\n",
    "\n",
    "# Modify U_U latent Space from U_I\n",
    "def modify_user_preference(U_I, items, user_id = 0):\n",
    "    \"\"\"\n",
    "    TODO: Fix the function s.t. it will not modify the initial matrix\n",
    "    \"\"\"\n",
    "    U_I[user_id,:] = 0\n",
    "    for i in items:\n",
    "        U_I[0,i] = 1\n",
    "    return U_I\n",
    "\n",
    "def clear_user_keyphrase(U_K, user_id = 0):\n",
    "    U_K[user_id,:] = 0\n",
    "\n",
    "def explain_synthetic_user(rtrain, U_K, I_K, keyphrases, top_keyphrase = 20, user_id = 0, k = 100, top_items = 100, **Not_used):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    items = item_associated_with_keyphrase(I_K, keyphrases, top_items = top_items) # 8 is coffee\n",
    "    U_I = modify_user_preference(rtrain, items, user_id=user_id)\n",
    "    modified_U_U = train(U_I)\n",
    "    U_K[user_id, :] = 0\n",
    "    synthetic_user_keyphrase = normalize(predict(U_K, k, modified_U_U))[user_id]\n",
    "    return np.argsort(synthetic_user_keyphrase)[::-1][:top_keyphrase]\n",
    "\n",
    "def modify_user_keyphrase(U_K, keyphrase_ids, normalization = True, keyval = 1, user_id = 0, **Not_Used):\n",
    "    \"\"\"\n",
    "    Change all keyphrase_ids to some fixed number, all others to 0\n",
    "    Return the U_K matrix with user_id row the synthetic user1\n",
    "    \"\"\"\n",
    "    U_K[user_id,:] = 0\n",
    "    for keyphrase_id in keyphrase_ids:\n",
    "        U_K[user_id,keyphrase_id] = keyval\n",
    "    if normalization == True:\n",
    "        return normalize(U_K)\n",
    "    return U_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:19: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "# Modify user preference matrix\n",
    "items = item_associated_with_keyphrase(I_K, [0], top_items = 200) # 'chinese'\n",
    "U_I = modify_user_preference(rtrain, items, user_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:25: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "# make synthetic user1's keyphrase preference all 0\n",
    "clear_user_keyphrase(U_K, user_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2343x233 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 298309 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7456x233 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 351924 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:01<00:00, 2151.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 97624.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get latent user similarity embedding\n",
    "modified_U_U = train(U_I)\n",
    "# predict\n",
    "explanation_scores1 = predict(U_K, 100, modified_U_U)\n",
    "explanation1 =  predict_pilot_explanation(explanation_scores1, top_keyphrase = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50,  60, 155, 205,  57, 160, 197,  61, 164, 226], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_in_prediction(rtrain, U_K, I_K, top_items = 200, keyphrase = 0,user_i = 0):\n",
    "    \"\"\"\n",
    "    Get the rank for user_i with keyphrase\n",
    "    TODO: modify so that no need to reload U_K,I_K\n",
    "    \"\"\"  \n",
    "    U_K = load_npz('../../data/yelp/U_K.npz')\n",
    "    U_K = normalize(U_K)\n",
    "    rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")\n",
    "    \n",
    "    # Modify user preference matrix\n",
    "    items = item_associated_with_keyphrase(I_K, keyphrase, top_items = top_items) # 'raspberry'\n",
    "    U_I = modify_user_preference(rtrain, items, user_id = 0)\n",
    "    \n",
    "    # make synthetic user1's keyphrase preference all 0\n",
    "    clear_user_keyphrase(U_K, user_id = 0)\n",
    "    \n",
    "    # Get latent user similarity embedding\n",
    "    modified_U_U = train(U_I)\n",
    "    # predict\n",
    "    explanation_scores1 = predict(U_K, 100, modified_U_U)\n",
    "    explanation1 =  predict_pilot_explanation(explanation_scores1, top_keyphrase = 230)\n",
    "    return list(explanation1[user_i]).index(keyphrase[0])\n",
    "    \n",
    "def evaluate_pilot_test(rtrain,U_K, I_K,keyphrase_list, top_items = 200, user_i = 0):\n",
    "    # Get the average rank for user_i with keyphrase  \n",
    "    res1 = 0\n",
    "    for i in range(75):\n",
    "        a = rank_in_prediction(rtrain, U_K, I_K, top_items = top_items, keyphrase = [i],user_i = user_i)\n",
    "        print \"keyphrase\", keyphrase_list[i], \"'s rank is \", a\n",
    "        res1+= a\n",
    "    return res1/75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_pilot_test(rtrain,U_K,I_K,keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top_users items associated with keyphrase_ids \n",
    "def users_with_keyphrase_preference(U_K, keyphrase_ids, top_users = 100, norm = True):\n",
    "    \"\"\"\n",
    "    U_K: User Keyphrase Matrix\n",
    "    Keyphrase_ids: top_users who like keyphrase_ids\n",
    "    output item list (unique)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    if norm:\n",
    "        U_K = normalize(U_K)\n",
    "    try:\n",
    "        for keyphrase_id in keyphrase_ids:\n",
    "            res.append(np.argsort(np.ravel(U_K.todense()[:,keyphrase_id]))[::-1][:top_users])\n",
    "    except:\n",
    "        return np.argsort(np.ravel(U_K.todense()[:,keyphrase_ids]))[::-1][:top_users]\n",
    "    return np.unique(res)\n",
    "\n",
    "# Modify I_I latent Space from U_I\n",
    "def modify_item_history(U_I, users, item_id = 0):\n",
    "    \"\"\"\n",
    "    TODO: Fix the function s.t. it will not modify the initial matrix\n",
    "    \"\"\"\n",
    "    U_I[:,item_id] = 0\n",
    "    for i in users:\n",
    "        U_I[i, item_id] = 1\n",
    "    return normalize(U_I)\n",
    "\n",
    "def clear_item_keyphrase(I_K, item_id = 0):\n",
    "    I_K[item_id, :] = 0\n",
    "\n",
    "def explain_synthetic_item(rtrain, U_K, I_K, keyphrases, top_keyphrase = 20, user_id = 0, k = 100, top_users = 100, **Not_used):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    items = item_associated_with_keyphrase(I_K, keyphrases, top_items = top_items) # 8 is coffee\n",
    "    U_I = modify_user_preference(rtrain, items, user_id=user_id)\n",
    "    modified_U_U = train(U_I)\n",
    "    U_K[user_id, :] = 0\n",
    "    synthetic_user_keyphrase = normalize(predict(U_K, k, modified_U_U))[user_id]\n",
    "    return np.argsort(synthetic_user_keyphrase)[::-1][:top_keyphrase]\n",
    "\n",
    "def modify_user_keyphrase(U_K, keyphrase_ids, normalization = True, keyval = 1, user_id = 0, **Not_Used):\n",
    "    \"\"\"\n",
    "    Change all keyphrase_ids to some fixed number, all others to 0\n",
    "    Return the U_K matrix with user_id row the synthetic user1\n",
    "    \"\"\"\n",
    "    U_K[user_id,:] = 0\n",
    "    for keyphrase_id in keyphrase_ids:\n",
    "        U_K[user_id,keyphrase_id] = keyval\n",
    "    if normalization == True:\n",
    "        return normalize(U_K)\n",
    "    return U_K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U_K = load_npz('../../data/yelp/U_K.npz')\n",
    "I_K = load_npz('../../data/yelp/I_K.npz')\n",
    "U_K = normalize(U_K)\n",
    "I_K = normalize(I_K)\n",
    "rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:23: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "# Modify user preference matrix\n",
    "users = users_with_keyphrase_preference(U_K, 70, top_users = 100, norm = True) # 'raspberry'\n",
    "U_I = modify_item_history(rtrain, users, item_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make synthetic item1's keyphrase to all 0\n",
    "clear_item_keyphrase(I_K, item_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:03<00:00, 2184.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 7456/7456 [00:00<00:00, 112969.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get latent item similarity embedding\n",
    "modified_I_I = train(np.transpose(U_I))\n",
    "# predict\n",
    "explanation_scores1 = predict(I_K, 100, modified_I_I)\n",
    "explanation1 =  predict_pilot_explanation(explanation_scores1, top_keyphrase = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_in_prediction_item(rtrain, U_K, I_K, top_users = 200, keyphrase = 70, item_i = 0):\n",
    "    \"\"\"\n",
    "    Get the rank for user_i with keyphrase\n",
    "    TODO: modify so that no need to reload U_K,I_K\n",
    "    \"\"\"  \n",
    "    U_K = load_npz('../../data/yelp/U_K.npz')\n",
    "    I_K = load_npz('../../data/yelp/I_K.npz')\n",
    "    U_K = normalize(U_K)\n",
    "    I_K = normalize(I_K)\n",
    "    rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")\n",
    "    \n",
    "    # Modify user preference matrix\n",
    "    users = users_with_keyphrase_preference(U_K, keyphrase, top_users = top_users, norm = True) # 'raspberry'\n",
    "    U_I = modify_item_history(rtrain, users, item_id = item_i)\n",
    "    \n",
    "    # make synthetic item1's keyphrase to all 0\n",
    "    clear_item_keyphrase(I_K, item_id = item_i)\n",
    "    \n",
    "    # Get latent item similarity embedding\n",
    "    modified_I_I = train(np.transpose(U_I))\n",
    "    # predict\n",
    "    explanation_scores1 = predict(I_K, 100, modified_I_I)\n",
    "    explanation1 =  predict_pilot_explanation(explanation_scores1, top_keyphrase = 230)\n",
    "    return list(explanation1[item_i]).index(keyphrase)\n",
    "\n",
    "    \n",
    "def evaluate_pilot_test_item(rtrain,U_K, I_K,keyphrase_list, top_users = 200, item_i = 0):\n",
    "    # Get the average rank for item_i with keyphrase  \n",
    "    res1 = 0\n",
    "    for i in range(75):\n",
    "        a = rank_in_prediction_item(rtrain, U_K, I_K, top_users = top_users, keyphrase = i, item_i = item_i)\n",
    "        print \"keyphrase\", keyphrase_list[i], \"'s rank is \", a\n",
    "        res1+= a\n",
    "    return res1/75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:23: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:29: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:03<00:00, 2163.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 7456/7456 [00:00<00:00, 112969.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_in_prediction_item(rtrain, U_K, I_K, top_users = 200, keyphrase = 10, item_i = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_pilot_test_item(rtrain,U_K, I_K,keyphrases, top_users = 200, item_i = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
