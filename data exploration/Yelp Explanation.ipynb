{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "from ast import literal_eval\n",
    "\n",
    "# For Python2 this have to be done\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/yelp/Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_sparse_matrix(df, num_user, num_item, user_col, item_col, rating_col):\n",
    "\n",
    "    dok = df[[user_col, item_col, rating_col]].copy()\n",
    "    dok = dok.values\n",
    "    dok = dok[dok[:, 2] > 0]\n",
    "    shape = [num_user, num_item]\n",
    "\n",
    "    return sparse.csr_matrix((dok[:, 2].astype(np.float32), (dok[:, 0], dok[:, 1])), shape=shape)\n",
    "\n",
    "def leave_one_out_split(df, user_col, ratio, random_state=None):\n",
    "    grouped = df.groupby(user_col, as_index=False)\n",
    "    valid = grouped.apply(lambda x: x.sample(frac=ratio, random_state=random_state))\n",
    "    train = df.loc[~df.index.isin([x[1] for x in valid.index])]\n",
    "    return train, valid\n",
    "\n",
    "def time_ordered_split(df, ratio, user_col = None, random_state=None):\n",
    "    # Sort data based on timestamp\n",
    "    argsort = np.argsort(df['timestamp'])\n",
    "    df_ordered = df.reindex(argsort)\n",
    "    train_offset = int((1-ratio)*len(df_ordered))\n",
    "    \n",
    "    train = df_ordered[:train_offset]\n",
    "    valid = df_ordered[train_offset:]\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def main(enable_validation = False, time_ordered_split_en = True, implicit_en = False):\n",
    "    df = pd.read_csv('../../data/yelp/' + 'Data.csv')\n",
    "\n",
    "    num_users = df['UserIndex'].nunique()\n",
    "    num_items = df['ItemIndex'].nunique()\n",
    "\n",
    "    # Get timestamp \n",
    "    date_time_df = df[['Day','Month','Year']]\n",
    "    date_time_df.rename(columns={'Year': 'year', 'Month': 'day', 'Day':'month'}, inplace=True)\n",
    "    date_time = pd.to_datetime(date_time_df)\n",
    "    df['timestamp'] = date_time\n",
    "\n",
    "    rating_col = 'rating'\n",
    "    if implicit_en == True:\n",
    "        rating_col = 'Binary'\n",
    "    \n",
    "    if time_ordered_split_en:\n",
    "        df_train, df_test = time_ordered_split(df, 0.2)\n",
    "    else:\n",
    "        df_train, df_test = leave_one_out_split(df, 'UserIndex', 0.2, random_state=8292)\n",
    "\n",
    "    if enable_validation:\n",
    "        if time_ordered_split_en:\n",
    "            df_train, df_valid = time_ordered_split(df_train, 0.2)\n",
    "        else:\n",
    "            df_train, df_valid = leave_one_out_split(df_train, 'UserIndex', 0.2, random_state=8292)\n",
    "        \n",
    "        # Clean empty rows\n",
    "        df_valid = df_valid.dropna().reset_index(drop = True)\n",
    "        \n",
    "        # Save\n",
    "        df_valid.to_csv('../../data/yelp/' + 'Valid.csv')\n",
    "        R_valid = to_sparse_matrix(df_valid, num_users, num_items, 'UserIndex','ItemIndex', rating_col)\n",
    "        sparse.save_npz('../../data/yelp/' + 'Rvalid.npz', R_valid)\n",
    "    \n",
    "    # Clean empty rows\n",
    "    df_train = df_train.dropna().reset_index(drop = True)\n",
    "    df_test = df_test.dropna().reset_index(drop = True)\n",
    "    \n",
    "    # Save\n",
    "    df_train.to_csv('../../data/yelp/'  + 'Train.csv')\n",
    "    R_train = to_sparse_matrix(df_train, num_users, num_items, 'UserIndex', 'ItemIndex', rating_col)\n",
    "    sparse.save_npz('../../data/yelp/' + 'Rtrain.npz', R_train)\n",
    "\n",
    "    df_test.to_csv('../../data/yelp/' + 'Test.csv')\n",
    "    R_test = to_sparse_matrix(df_test, num_users, num_items, 'UserIndex', 'ItemIndex', rating_col)\n",
    "    sparse.save_npz('../../data/yelp/' + 'Rtest.npz', R_test)\n",
    "    \n",
    "def date_to_timestamp(date, **not_used):\n",
    "    dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "    return time.mktime(dt.timetuple())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'Unnamed: 0.1', u'Unnamed: 0.1.1', u'business_id',\n",
       "       u'friend_count', u'ghost', u'img_dsc', u'img_url', u'nr',\n",
       "       u'photo_count', u'rating', u'review_count', u'review_date',\n",
       "       u'review_id', u'review_language', u'review_text', u'ufc', u'user_id',\n",
       "       u'user_loc', u'vote_count', u'Updated', u'Year', u'Month', u'Day',\n",
       "       u'Binary', u'review', u'conca_review', u'keyVector',\n",
       "       u'keyphrases_indices_length', u'UserIndex', u'ItemIndex'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(enable_validation = True, time_ordered_split_en = False, implicit_en = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Original Data\n",
    "df_train = pd.read_csv('../../data/yelp/Train.csv')\n",
    "df_valid = pd.read_csv('../../data/yelp/Valid.csv')\n",
    "df_test = pd.read_csv('../../data/yelp/Test.csv')\n",
    "# keyphrases = pd.read_csv('../../data/yelp/KeyPhrases.csv')['Phrases'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ItemIndex = pd.read_csv('../../data/yelp/ItemIndex.csv')\n",
    "ItemIndex = ItemIndex.sort_values('ItemIndex').drop_duplicates(subset=['ItemIndex', 'business_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load U-I Data \n",
    "rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")\n",
    "rvalid = load_npz(\"../../data/yelp/Rvalid.npz\")\n",
    "rtest = load_npz(\"../../data/yelp/Rtest.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2473x10282 sparse matrix of type '<type 'numpy.float32'>'\n",
       "\twith 102741 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2473x10282 sparse matrix of type '<type 'numpy.float32'>'\n",
       "\twith 102741 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102741"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 102741/102741 [00:05<00:00, 17790.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 102741/102741 [00:05<00:00, 18116.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate U_K and I_K\n",
    "# For validation set\n",
    "U_K = get_I_K(df_train, row_name = 'UserIndex', shape = (2473, 334))\n",
    "I_K = get_I_K(df_train, row_name = 'ItemIndex', shape = (10282, 334))\n",
    "# For test set\n",
    "# U_K_test = get_I_K(df_test, row_name = 'UserIndex', shape = (2473, 235))\n",
    "# I_K_test = get_I_K(df_test, row_name = 'ItemIndex', shape = (10282, 235))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "save_npz('../../data/yelp/U_K.npz',U_K)\n",
    "save_npz('../../data/yelp/I_K.npz',I_K)\n",
    "# save_npz( '../../data/yelp/U_K_test.npz',U_K_test)\n",
    "# save_npz('../../data/yelp/I_K_test.npz',I_K_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load \n",
    "U_K = load_npz('../../data/yelp/U_K.npz')\n",
    "I_K = load_npz('../../data/yelp/I_K.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2473x235 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 252930 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, row_name = 'ItemIndex', shape = (3668,75)):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        key_vector = literal_eval(df['keyVector'][i])\n",
    "        rows.extend([df[row_name][i]]*len(key_vector)) ## Item index\n",
    "        cols.extend(key_vector) ## Keyword Index\n",
    "#         if binary:\n",
    "        vals.extend(np.array([1]*len(key_vector)))\n",
    "#         else:\n",
    "#             vals.extend(arr[arr.nonzero()])    \n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    \"\"\"\n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    \"\"\"\n",
    "    prediction_scores = []\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores to all users\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "\n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        vector_u = prediction_score[user_index]\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "\n",
    "    vector_u = vector_u\n",
    "\n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    return vector_u[:topK]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evluation \n",
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        \"R-Precision\": r_precision,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = results[name]\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (np.average(results[name]),\n",
    "                                                              1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = results[name]\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (np.average(results[name]), 1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output\n",
    "\n",
    "def explain_evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        \"R-Precision\": r_precision,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "#                 vector_true = matrix_Test[user_index]\n",
    "                vector_true = np.ravel(matrix_Test.todense()[0])\n",
    "                vector_true_dense = np.argsort(vector_true)[::-1][:k]\n",
    "#                 vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = results[name]\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (np.average(results[name]),\n",
    "                                                              1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "#             vector_true = matrix_Test[user_index]\n",
    "#             vector_true_dense = vector_true.nonzero()[1]\n",
    "            vector_true = np.ravel(matrix_Test.todense()[0])\n",
    "            vector_true_dense = np.argsort(vector_true)[::-1][:k]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = results[name]\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (np.average(results[name]), 1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explain(R,W2,k, model = \"Cosine_similarity\", item_similarity_en = True):\n",
    "    \"\"\"\n",
    "    k: knn's hyperparameter k\n",
    "    R: Rating Matrix with size U*I\n",
    "    r_ij: observed rating with user i and item j \n",
    "    s_ij: explanation vector with user i and item j \n",
    "    Z: Joint Embedding/Latent Space with size U*U, generate r_ij and s_ij\n",
    "    W2: Reconstruction matrix with size U*K \n",
    "    S: Output explanation prediction matrix with size U*K (dense numpy ndarray)\n",
    "    \"\"\"\n",
    "    Z = train(R) # Cosine similarity as default\n",
    "    S = predict(W2, k, Z, item_similarity_en=item_similarity_en) \n",
    "    if normalize_en == True:       \n",
    "        return normalize(S) # prediction score\n",
    "    return S\n",
    "\n",
    "def predict(matrix_train, k, similarity, item_similarity_en = False, normalize_en = False):\n",
    "    \"\"\"\n",
    "    matrix_train: Rating Matrix with size U*I\n",
    "    k: knn's hyperparameter k\n",
    "    similarity: Joint Embedding/Latent Space with size U*U or I*I\n",
    "    \n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    \n",
    "    r_ij: observed rating with user i and item j \n",
    "    s_ij: explanation vector with user i and item j \n",
    "    \"\"\"\n",
    "    prediction_scores = []\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores to all users\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "\n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    if normalize_en:\n",
    "        res = normalize(res)\n",
    "    return res\n",
    "\n",
    "def explain_prediction(prediction_score, topK, matrix_Train):\n",
    "    \"\"\"\n",
    "    output prediction res of the  top K items/keyphrase/whatever\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        vector_u = prediction_score[user_index]\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine_explain(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "    return np.vstack(prediction)\n",
    "#     return prediction\n",
    "\n",
    "def sub_routine_explain(vector_u, vector_train, topK=30):\n",
    "    \"\"\"\n",
    "    vector_u: predicted user vector\n",
    "    vector_train: true user vector\n",
    "    topK: top k items in vector\n",
    "    vector_u: top k items predicted\n",
    "    \"\"\"\n",
    "#     train_index = vector_train.nonzero()[1]\n",
    "#     candidate_index = np.argpartition(-vector_u, topK+75)[:topK+75] #  10 here to make res consistent\n",
    "#     candidate_index = np.argpartition(-vector_u, 74)[:topK]\n",
    "#     vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "#     vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    candidate_index = np.argsort(vector_u)[::-1][:topK]\n",
    "    return candidate_index\n",
    "\n",
    "def predict_pilot_explanation(explanation_scores, top_keyphrase = 10):\n",
    "    \"\"\"\n",
    "    Used for retrieve the 1st row of prediction scores, used for pilot test\n",
    "    \"\"\"\n",
    "    explanation = []\n",
    "    for explanation_score in tqdm(explanation_scores):\n",
    "        explanation.append(np.argsort(explanation_score)[::-1][:top_keyphrase])\n",
    "    return np.array(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explain(R,W2,k):\n",
    "    \"\"\"\n",
    "    R: Rating Matrix with size U*I\n",
    "    r_ij: observed rating with user i and item j \n",
    "    s_ij: explanation vector with user i and item j \n",
    "    Z: Joint Embedding/Latent Space with size U*U, generate r_ij and s_ij\n",
    "    W2: Reconstruction matrix with size U*K \n",
    "    S: Output explanation prediction matrix with size U*K\n",
    "    \"\"\"\n",
    "    Z = train(R)\n",
    "    S = predict(W2, k, Z)\n",
    "    return normalize(S)\n",
    "\n",
    "# Evaluation Model\n",
    "def recall(vector_true_dense, vector_true_predict):\n",
    "    \"\"\"\n",
    "    The fraction of relevant instances that have been retrieved over the total amount of relevant instances\n",
    "    The length of vector_true_dense and vector_true_predict has to be the same\n",
    "    Out put recall\n",
    "    \"\"\"\n",
    "    hits = len(np.isin(vector_true_predict, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "# Total Recall\n",
    "def recall_all(true_matrix, predict_matrix, topK = 20):\n",
    "    res = []\n",
    "    for i in tqdm(range(len(Explanation_res1))):\n",
    "        true_vector = np.argsort(np.ravel(normalize(true_matrix).todense()[i]))[-topK:]\n",
    "        predict_vector = np.argsort(predict_matrix[i])[-topK:]\n",
    "        res.append(recall(true_vector,predict_vector))\n",
    "    return sum(res)/len(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explicit with time-ordered split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:01<00:00, 1707.87it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:00<00:00, 7316.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# ( U_I * I_U ) * U_K\n",
    "similarity = train(rtrain)\n",
    "explanation_scores = predict(U_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 100, U_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 907.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 912.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 896.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 876.64it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:03<00:00, 814.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.30572436135912695, 0.0023586693682974545),\n",
       " 'MAP@20': (0.45840955394632565, 0.0018530622660002188),\n",
       " 'MAP@5': (0.0564626736111111, 0.0018899042160193539),\n",
       " 'MAP@50': (0.7432506572809149, 0.0007380546195515702),\n",
       " 'NDCG': (0.7156882409582186, 0.000962427851550532),\n",
       " 'Precision@10': (0.35668402777777775, 0.002292811652556904),\n",
       " 'Precision@20': (0.34036458333333336, 0.0018187403298631716),\n",
       " 'Precision@5': (0.14505208333333333, 0.0035186949203039313),\n",
       " 'Precision@50': (0.5203993055555556, 0.0009750286085532342),\n",
       " 'R-Precision': (0.5203993055555556, 0.0009750286085532342),\n",
       " 'Recall@10': (0.35668402777777775, 0.002292811652556904),\n",
       " 'Recall@20': (0.34036458333333336, 0.0018187403298631716),\n",
       " 'Recall@5': (0.14505208333333333, 0.0035186949203039313),\n",
       " 'Recall@50': (0.5203993055555556, 0.0009750286085532342)}"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, U_K, atK=[5,10,20,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:03<00:00, 636.88it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:03<00:00, 637.70it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:03<00:00, 640.51it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:03<00:00, 642.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:03<00:00, 627.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.3057171275628307, 0.002359294896992973),\n",
       " 'MAP@20': (0.45840593704817756, 0.0018534890803013125),\n",
       " 'MAP@5': (0.0564626736111111, 0.0018899042160193539),\n",
       " 'MAP@50': (0.7096259471824808, 0.001151950788477201),\n",
       " 'NDCG': (0.7122241082228672, 0.001110137717433586),\n",
       " 'Precision@10': (0.35668402777777775, 0.002292811652556904),\n",
       " 'Precision@20': (0.34036458333333336, 0.0018187403298631716),\n",
       " 'Precision@5': (0.14505208333333333, 0.0035186949203039313),\n",
       " 'Precision@50': (0.5081076388888889, 0.0011381668693386043),\n",
       " 'R-Precision': (0.5081076388888889, 0.0011381668693386043),\n",
       " 'Recall@10': (0.35668402777777775, 0.002292811652556904),\n",
       " 'Recall@20': (0.34036458333333336, 0.0018187403298631716),\n",
       " 'Recall@5': (0.14505208333333333, 0.0035186949203039313),\n",
       " 'Recall@50': (0.5081076388888889, 0.0011381668693386043)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With Vader Keyphrase\n",
    "explain_evaluate(explanation, U_K, atK=[5,10,20,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implicit with time-ordered split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:01<00:00, 1967.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:00<00:00, 7585.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# ( U_I * I_U ) * U_K\n",
    "similarity = train(rtrain)\n",
    "explanation_scores = predict(U_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 100, U_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 905.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 882.27it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 844.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:02<00:00, 890.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:03<00:00, 813.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.311798011739418, 0.0031561433837630068),\n",
       " 'MAP@30': (0.6306166965447901, 0.0031362038353130012),\n",
       " 'MAP@5': (0.0601345486111111, 0.002815488470400913),\n",
       " 'MAP@50': (0.7330563022535985, 0.003249815201998381),\n",
       " 'NDCG': (0.7132432268933633, 0.001959402705468077),\n",
       " 'Precision@10': (0.3548177083333333, 0.0028022116113549708),\n",
       " 'Precision@30': (0.4426070601851852, 0.0020332865390623677),\n",
       " 'Precision@5': (0.13671875, 0.003680709859694958),\n",
       " 'Precision@50': (0.5121440972222222, 0.0020030788652286792),\n",
       " 'R-Precision': (0.5121440972222222, 0.0020030788652286792),\n",
       " 'Recall@10': (0.3548177083333333, 0.0028022116113549708),\n",
       " 'Recall@30': (0.4426070601851852, 0.0020332865390623677),\n",
       " 'Recall@5': (0.13671875, 0.003680709859694958),\n",
       " 'Recall@50': (0.5121440972222222, 0.0020030788652286792)}"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, U_K, atK=[5,10,20,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit without time-ordered split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:01<00:00, 2128.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 7607.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# ( U_I * I_U ) * U_K\n",
    "similarity = train(rtrain)\n",
    "explanation_scores = predict(U_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 100, U_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 939.08it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 889.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 913.09it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:02<00:00, 882.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@20': (0.45316721230242296, 0.0019350810812115379),\n",
       " 'MAP@40': (0.5842206191206047, 0.002377891946403873),\n",
       " 'MAP@5': (0.623118509034002, 0.0029496286180747486),\n",
       " 'NDCG': (0.7268562260242788, 0.0021141846532503274),\n",
       " 'Precision@20': (0.3027102005975245, 0.0018155475022447942),\n",
       " 'Precision@40': (0.5250213401621853, 0.0021673342673273256),\n",
       " 'Precision@5': (0.3977806231327358, 0.0016873251019215718),\n",
       " 'R-Precision': (0.5250213401621853, 0.0021673342673273256),\n",
       " 'Recall@20': (0.3027102005975245, 0.0018155475022447942),\n",
       " 'Recall@40': (0.5250213401621853, 0.0021673342673273256),\n",
       " 'Recall@5': (0.3977806231327358, 0.0016873251019215718)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, U_K_test, atK=[5,20,40]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-based Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explicit with time-oredered split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10282/10282 [00:06<00:00, 1582.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10282/10282 [00:01<00:00, 7445.33it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity = train(np.transpose(rtrain))\n",
    "explanation_scores = predict(I_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 70, I_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:44<00:00, 232.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:43<00:00, 236.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:42<00:00, 244.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:42<00:00, 243.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:44<00:00, 233.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.28026686614955226, 0.0020168575717200354),\n",
       " 'MAP@30': (0.37520300707679344, 0.0013170483773279748),\n",
       " 'MAP@5': (0.0017209636821287305, 0.0003842673027413293),\n",
       " 'MAP@50': (0.33210548732975054, 0.0009162156264124513),\n",
       " 'NDCG': (0.3364941611204058, 0.0008128281214049484),\n",
       " 'Precision@10': (0.24431499460625677, 0.0013591829513627755),\n",
       " 'Precision@30': (0.27102840704782455, 0.0008001989770720488),\n",
       " 'Precision@5': (0.002761596548004315, 0.0005082563467914027),\n",
       " 'Precision@50': (0.24768500539374325, 0.0005864023566947201),\n",
       " 'R-Precision': (0.24768500539374325, 0.0005864023566947201),\n",
       " 'Recall@10': (0.24431499460625677, 0.0013591829513627755),\n",
       " 'Recall@30': (0.27102840704782455, 0.0008001989770720488),\n",
       " 'Recall@5': (0.002761596548004315, 0.0005082563467914027),\n",
       " 'Recall@50': (0.24768500539374325, 0.0005864023566947201)}"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, I_K, atK=[5,10,30,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:00<00:00, 168.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:01<00:00, 167.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:58<00:00, 176.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:57<00:00, 178.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:58<00:00, 176.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.30612538076616214, 0.0022405766064059534),\n",
       " 'MAP@30': (0.3778614515602291, 0.0013079230179825654),\n",
       " 'MAP@5': (0.12744735048254585, 0.0030434484506194712),\n",
       " 'MAP@50': (0.34609231953227454, 0.0009031312108072494),\n",
       " 'NDCG': (0.3879557787232804, 0.0007765785785828622),\n",
       " 'Precision@10': (0.20611344311699498, 0.0010204111243237357),\n",
       " 'Precision@30': (0.2785670720769203, 0.0007951049318394071),\n",
       " 'Precision@5': (0.12123560434829406, 0.0019859885794326995),\n",
       " 'Precision@50': (0.28675277149930045, 0.0005913220980868425),\n",
       " 'R-Precision': (0.28675277149930045, 0.0005913220980868425),\n",
       " 'Recall@10': (0.20611344311699498, 0.0010204111243237357),\n",
       " 'Recall@30': (0.2785670720769203, 0.0007951049318394071),\n",
       " 'Recall@5': (0.12123560434829406, 0.0019859885794326995),\n",
       " 'Recall@50': (0.28675277149930045, 0.0005913220980868425)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With Vader\n",
    "explain_evaluate(explanation, I_K, atK=[5,10,30,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implicit without time-oredered split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10282/10282 [00:05<00:00, 1837.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10282/10282 [00:01<00:00, 7765.86it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity = train(np.transpose(rtrain))\n",
    "explanation_scores = predict(I_K, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 70, I_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:40<00:00, 251.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:40<00:00, 256.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:40<00:00, 255.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:39<00:00, 261.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [00:40<00:00, 251.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.1931789138884608, 0.0031984714634543656),\n",
       " 'MAP@30': (0.28915286485493175, 0.003933334833419286),\n",
       " 'MAP@5': (0.17321610931319667, 0.003704519739578873),\n",
       " 'MAP@50': (0.39472789043500867, 0.001113353205085209),\n",
       " 'NDCG': (0.3966737410561124, 0.0009858754594146636),\n",
       " 'Precision@10': (0.15645091693635382, 0.002387857507926541),\n",
       " 'Precision@30': (0.22858683926645093, 0.0030535196896196216),\n",
       " 'Precision@5': (0.12323624595469257, 0.0021489295200967933),\n",
       " 'Precision@50': (0.34073354908306364, 0.0012682162198424524),\n",
       " 'R-Precision': (0.34073354908306364, 0.0012682162198424524),\n",
       " 'Recall@10': (0.15645091693635382, 0.002387857507926541),\n",
       " 'Recall@30': (0.22858683926645093, 0.0030535196896196216),\n",
       " 'Recall@5': (0.12323624595469257, 0.0021489295200967933),\n",
       " 'Recall@50': (0.34073354908306364, 0.0012682162198424524)}"
      ]
     },
     "execution_count": 815,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, I_K_test, atK=[5,10,30,50]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand item-based Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get restaurant names correspond to ItemIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_business_df(path = \"../../data/yelp/business.json\" ):\n",
    "    with open(path) as json_file:\n",
    "        data = json_file.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_restaurant_info(business_df, business_id, name = True, review_count = True, stars = True ):\n",
    "    output_list = {}\n",
    "    row_idx = int(business_df.index[business_df['business_id'] == business_id].tolist()[0])\n",
    "    if name == True:\n",
    "        output_list['name'] = business_df['name'][row_idx].encode('utf-8').strip()\n",
    "    if review_count == True:\n",
    "        output_list['review_count'] = business_df['review_count'][row_idx]\n",
    "    if stars == True:\n",
    "        output_list['stars'] = business_df['stars'][row_idx] \n",
    "    return output_list\n",
    "\n",
    "def get_businessid_from_Itemindex(ItemIndex_list, itemindex):\n",
    "    return ItemIndex_list['business_id'].tolist()[itemindex]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_df = get_business_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l_uAw0K2lkOsyVJATcnwsA'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_businessid_from_Itemindex(ItemIndex, 8010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Spicy Mafia', 'review_count': 9, 'stars': 3.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_restaurant_info(business_df, 'l_uAw0K2lkOsyVJATcnwsA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2473x10282 sparse matrix of type '<type 'numpy.float32'>'\n",
       "\twith 102741 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find specific Restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35611     Sushi On Bloor\n",
      "114653    Sushi On Bloor\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['sushi on bloor' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7031                       Spicy Mafia\n",
      "28409                 Caricature Mafia\n",
      "51959               Mafia Mike's Pizza\n",
      "113184    Cakefacemafia Brows & Beauty\n",
      "119974                     Spicy Mafia\n",
      "140192                    Mafia Mike's\n",
      "156211                  La'Bella MAFIA\n",
      "180250              Mafia Mike's PIzza\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['mafia' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74627     Crown Prince Fine Dining & Banquet\n",
      "106186            Crown Princess Fine Dining\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['crown prince' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4501                  Red Lobster\n",
      "9494                  Red Lobster\n",
      "9599                  Red Lobster\n",
      "11793                 Red Lobster\n",
      "15146                 Red Lobster\n",
      "21612                 Red Lobster\n",
      "25103                 Red Lobster\n",
      "37750                 Red Lobster\n",
      "39311                 Red Lobster\n",
      "42883                 Red Lobster\n",
      "46058                 Red Lobster\n",
      "46742                 Red Lobster\n",
      "51418                 Red Lobster\n",
      "51780                 Red Lobster\n",
      "52374                 Red Lobster\n",
      "56591                 Red Lobster\n",
      "66201                 Red Lobster\n",
      "68260                 Red Lobster\n",
      "70318                 Red Lobster\n",
      "79983                 Red Lobster\n",
      "105382                Red Lobster\n",
      "108167                Red Lobster\n",
      "112307                Red Lobster\n",
      "115861                Red Lobster\n",
      "119864                Red Lobster\n",
      "126004                Red Lobster\n",
      "134059                Red Lobster\n",
      "140933                Red Lobster\n",
      "145037                Red Lobster\n",
      "154559                Red Lobster\n",
      "159619                Red Lobster\n",
      "161507                Red Lobster\n",
      "162753                Red Lobster\n",
      "163962                Red Lobster\n",
      "167619    Red Lobster Restaurants\n",
      "176109                Red Lobster\n",
      "179596                Red Lobster\n",
      "179732                Red Lobster\n",
      "184775                Red Lobster\n",
      "185524                Red Lobster\n",
      "185736                Red Lobster\n",
      "188347                Red Lobster\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['red lobster' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73128    The Queen And Beaver Public House\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['queen and beaver' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107476                            Miku\n",
      "116702    Mikush Home Appliance Center\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['miku' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470                              Fernando's\n",
      "600                         Shenandoah Mill\n",
      "4672                                 Nandos\n",
      "8284                        Fernando's Food\n",
      "18596                  Nando's Mexican Cafe\n",
      "28152         Nando's Flame Grilled Chicken\n",
      "39568                   Hernando's Hideaway\n",
      "51248                               Nando's\n",
      "57684                   Nando's Chickenland\n",
      "59320                               Nando's\n",
      "60928                   Fernando's Hideaway\n",
      "66834                               Nando's\n",
      "72563                               Nando's\n",
      "78219                    Barry Fernando, MD\n",
      "78984               San Fernando Apartments\n",
      "85788                         Viva Fernando\n",
      "88200                               Nando's\n",
      "100964               Nando Milano Trattoria\n",
      "110806                   Rene Fernando, DDS\n",
      "111236              Fernando's Tree Service\n",
      "122556                      Fernando's Cafe\n",
      "127748                              Nando's\n",
      "131533        Nando's Flame Grilled Chicken\n",
      "141393                              Nando's\n",
      "144310                      Nandos Danforth\n",
      "144317                 Nando's Mexican Cafe\n",
      "152427                              Nando's\n",
      "152829        Nando's Flame Grilled Chicken\n",
      "156540    Wayne Newton's Casa de Shenandoah\n",
      "159183           Fernando's Metal Polishing\n",
      "173541                              Nando's\n",
      "175347                              Nando's\n",
      "177828        Nando's Flame Grilled Chicken\n",
      "184616                  Fernando's Hideaway\n",
      "189818                 Nando's Mexican Cafe\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['nando' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64440    Gyubee Japanese BBQ - Downtown\n",
      "65285     Gyubee Japanese BBQ - Markham\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "x = ['gyube' in business_df['name'][i].lower() for i in range(len(business_df))]\n",
    "for i in np.array(x).nonzero():\n",
    "    print business_df['name'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If the restaurant exists in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_itemindex_from_business_id(ItemIndex_list, business_id):\n",
    "    business_id_list = ItemIndex_list['business_id'].tolist()\n",
    "    return business_id_list.index(business_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'l3rDLV3OrQVlLn53q-fYRA'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df['business_id'][4672]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'l3rDLV3OrQVlLn53q-fYRA' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-ea76a37be400>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_itemindex_from_business_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mItemIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'l3rDLV3OrQVlLn53q-fYRA'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-68-508b78a94a1c>\u001b[0m in \u001b[0;36mget_itemindex_from_business_id\u001b[1;34m(ItemIndex_list, business_id)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_itemindex_from_business_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mItemIndex_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbusiness_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mbusiness_id_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mItemIndex_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'business_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbusiness_id_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbusiness_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: 'l3rDLV3OrQVlLn53q-fYRA' is not in list"
     ]
    }
   ],
   "source": [
    "get_itemindex_from_business_id(ItemIndex, 'l3rDLV3OrQVlLn53q-fYRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(df.ItemIndex == 7312)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Prediction with Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spicy Mafia (fewer reviews Chinese food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 9, 'name': 'Spicy Mafia', 'stars': 3.0}\n",
      "8010\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, 'l_uAw0K2lkOsyVJATcnwsA')\n",
    "print get_itemindex_from_business_id(ItemIndex, 'l_uAw0K2lkOsyVJATcnwsA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['noodle', 'wait', 'thai', 'beef', 'tea', 'pork', 'mall', 'chicken',\n",
       "       'soup', 'spicy', 'quick', 'fresh', 'curry', 'pot', 'meat', 'lunch',\n",
       "       'busy', 'friendly', 'egg', 'dinner', 'fast', 'fried', 'bar',\n",
       "       'salad', 'vietnamese', 'milk', 'pop', 'tart', 'fry', 'kimchi',\n",
       "       'cocktail', 'clean', 'downtown', 'chewy', 'shrimp', 'salt',\n",
       "       'pasta', 'bubble', 'crispy', 'bubble tea', 'coconut', 'belly',\n",
       "       'tapioca', 'spring roll', 'milk tea', 'fair', 'tofu', 'wine',\n",
       "       'cake', 'fish', 'pork belly', 'sour', 'yummy', 'pizza', 'stick',\n",
       "       'chinese', 'sandwich', 'asian', 'fruit', 'bun', 'bean', 'cheese',\n",
       "       'attentive', 'rib', 'chili', 'tuna', 'wing', 'crunchy',\n",
       "       'reasonable', 'creamy'], dtype='|S19')"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Predicted TF\n",
    "test_list = list(map(int, explanation[8010])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wait', 'fresh', 'mall', 'friendly', 'pot', 'tea', 'chicken',\n",
       "       'bar', 'cone', 'egg', 'cheese', 'salad', 'beef', 'meat',\n",
       "       'croissant', 'cocktail', 'sashimi', 'gravy', 'busy', 'quick',\n",
       "       'plaza', 'congee', 'patty', 'tart', 'english muffin', 'pork',\n",
       "       'matcha', 'wild boar', 'fast', 'pale ale', 'fried', 'dessert',\n",
       "       'dinner', 'lunch', 'soup', 'mexico', 'noodle', 'cake', 'clean',\n",
       "       'salmon', 'dim sum', 'taco', 'octopus', 'smoked', 'brunch',\n",
       "       'waffle', 'booth', 'steak', 'north york', 'cheesecake', 'spicy',\n",
       "       'gelato', 'scallop', 'duck', 'pasta', 'cookie', 'general tao',\n",
       "       'pizza', 'bacon', 'public transit', 'cocktail', 'poutine', 'donut',\n",
       "       'indian', 'mexican', 'theatre', 'baked', 'bread', 'burger',\n",
       "       'pork belly'], dtype='|S19')"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Predicted TFIDF\n",
    "test_list = list(map(int, explanation[8010])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[noodle, wait, thai, beef, tea, pork, mall, chicken, soup, spicy, quick, fresh, curry, pot, meat, lunch, busy, friendly, egg, dinner]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wait, fresh, mall, friendly, pot, tea, chicken, bar, cone, egg, cheese, salad, beef, meat, croissant, cocktail, sashimi, gravy, busy, quick]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tomato', 'soup', 'meat', 'noodle', 'pork', 'fish', 'egg', 'busy',\n",
       "       'pot', 'tart', 'lunch', 'tofu', 'juice', 'avocado', 'apple',\n",
       "       'strawberry', 'pop', 'lettuce', 'miso', 'skewer', 'congee',\n",
       "       'sashimi', 'scallop', 'cone', 'honey', 'cookie', 'lamb', 'banana',\n",
       "       'croissant', 'mango', 'bacon', 'octopus', 'espresso', 'olive',\n",
       "       'donut', 'duck', 'booth', 'matcha', 'calamari', 'latte', 'sausage',\n",
       "       'fruit', 'cheesecake', 'cocktail', 'oyster', 'bubble', 'belly',\n",
       "       'toast', 'pancake', 'poutine', 'corn', 'coconut', 'vegan', 'lemon',\n",
       "       'wrap', 'tuna', 'crepe', 'four', 'tempura', 'sesame', 'patty',\n",
       "       'kimchi', 'tapioca', 'accept debit', 'squid', 'takeout',\n",
       "       'downtown', 'vegetarian', 'birthday', 'asian'], dtype='|S19')"
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Ground Truth\n",
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[8010].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tomato, soup, meat, noodle, pork, fish, egg, busy, pot, tart, lunch, tofu, juice, avocado, apple, strawberry, pop, lettuce, miso, skewer]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[np.argsort(np.ravel(I_K[8010].todense()))[::-1][:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hit\n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[8010].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crown Princess Fine Dining (many reviews Chinese food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 235, 'name': 'Crown Princess Fine Dining', 'stars': 3.5}\n",
      "8517\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, 'ovlWOSKVjGecnaPuZLv_OQ')\n",
    "print get_itemindex_from_business_id(ItemIndex, 'ovlWOSKVjGecnaPuZLv_OQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wait', 'thai', 'tea', 'mall', 'fresh', 'soup', 'pot', 'noodle',\n",
       "       'bar', 'chicken', 'beef', 'friendly', 'dumpling', 'dinner',\n",
       "       'curry', 'spicy', 'egg', 'pork', 'busy', 'tart', 'lunch', 'quick',\n",
       "       'fried', 'clean', 'chinese', 'cake', 'fish', 'shrimp', 'sushi',\n",
       "       'rib', 'meat', 'dim sum', 'dessert', 'tuna', 'pancake', 'milk',\n",
       "       'fast', 'fair', 'crispy', 'stick', 'downtown', 'store',\n",
       "       'attentive', 'bun', 'steamed', 'tofu', 'salt', 'salad', 'parking',\n",
       "       'pop', 'coconut', 'seafood', 'wine', 'wing', 'bean', 'sour',\n",
       "       'window', 'asian', 'creamy', 'music', 'fry', 'wrap', 'beer',\n",
       "       'markham', 'comfortable', 'ice cream', 'reasonable', 'scallop',\n",
       "       'yummy', 'bread'], dtype='|S19')"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Predicted TF\n",
    "test_list = list(map(int, explanation[8517])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mall', 'wait', 'tea', 'fresh', 'pot', 'friendly', 'chicken',\n",
       "       'tempura', 'salad', 'pork', 'beef', 'noodle', 'meat', 'skewer',\n",
       "       'cone', 'egg', 'avocado', 'spicy', 'patty', 'tart', 'cocktail',\n",
       "       'pork bone soup', 'quick', 'busy', 'cheese', 'accept debit',\n",
       "       'soup', 'balsamic vinegar', 'fried', 'public transit', 'gong cha',\n",
       "       'dessert', 'lactose intolerant', 'dinner', 'lunch', 'pale ale',\n",
       "       'wild boar', 'bar', 'alcoholic beverage', 'fast', 'stick', 'thai',\n",
       "       'cake', 'clean', 'sour', 'burger', 'fish', 'bacon', 'cocktail',\n",
       "       'fry', 'waffle', 'miso', 'vietnamese', 'chip', 'kimchi', 'lamb',\n",
       "       'sushi', 'lobster', 'ramen', 'coffee', 'pancake', 'brunch',\n",
       "       'buffet', 'store', 'bread', 'bubble tea', 'pork belly', 'rib',\n",
       "       'wrap', 'topped'], dtype='|S19')"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Predicted TFIDF\n",
    "test_list = list(map(int, explanation[8517])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mall', 'wait', 'amazing service', 'tea', 'fresh', 'pot',\n",
       "       'friendly', 'stick', 'sour', 'chicken', 'sure everything',\n",
       "       'pleasant', 'la carnita', 'favourite place', 'balsamic vinegar',\n",
       "       'hair cut', 'accept debit', 'public transit', 'much fun',\n",
       "       'great coffee', 'escape room', 'egg', 'great custom service',\n",
       "       'taste menu', 'super nice', 'awesome place', 'many flavour',\n",
       "       'great staff', 'nail salon', 'farmer market', 'bang bang',\n",
       "       'pale ale', 'cheese', 'wild boar', 'gong cha', 'pork bone soup',\n",
       "       'quick', 'busy', 'cocktail', 'deer garden', 'tart', 'patty',\n",
       "       'avocado', 'skewer', 'cone', 'tempura', 'spicy', 'meat', 'noodle',\n",
       "       'beef', 'pork', 'salad', 'vietnamese coffee', 'lactose intolerant',\n",
       "       'favourit thing', 'alcoholic beverage', 'quick meal',\n",
       "       'averag price', 'fried', 'bar', 'second chance', 'dessert', 'fast',\n",
       "       'dry side', 'bit bland', 'soup', 'empty table', 'instant noodle',\n",
       "       'dinner', 'bad service'], dtype='|S20')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Predicted TFIDF Vader\n",
    "test_list = list(map(int, explanation[8517])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wait, thai, tea, mall, fresh, soup, pot, noodle, bar, chicken, beef, friendly, dumpling, dinner, curry, spicy, egg, pork, busy, tart]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mall, wait, tea, fresh, pot, friendly, chicken, tempura, salad, pork, beef, noodle, meat, skewer, cone, egg, avocado, spicy, patty, tart]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mall, wait, amazing service, tea, fresh, pot, friendly, stick, sour, chicken, sure everything, pleasant, la carnita, favourite place, balsamic vinegar, hair cut, accept debit, public transit, much fun, great coffee]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['dim sum', 'rice', 'tea', 'art', 'chinese', 'dumpling', 'downtown',\n",
       "       'pork', 'wait', 'shrimp', 'bun', 'pot', 'fried', 'dinner',\n",
       "       'chicken', 'congee', 'egg', 'cake', 'dessert', 'tart', 'bbq',\n",
       "       'mall', 'fancy', 'rib', 'lunch', 'scallop', 'fresh', 'stick',\n",
       "       'noodle', 'pricey', 'busy', 'markham', 'wrap', 'squid', 'clean',\n",
       "       'duck', 'salt', 'crispy', 'bean', 'octopus', 'yummy', 'fair',\n",
       "       'attentive', 'tax', 'french', 'brunch', 'soup', 'refill', 'quick',\n",
       "       'corn', 'steamed', 'greasy', 'fast', 'curry', 'cheaper', 'solid',\n",
       "       'quiet', 'sesame', 'meat', 'disappointing', 'asian', 'parking',\n",
       "       'bar', 'baked', 'reasonable', 'crunchy', 'traditional', 'lobster',\n",
       "       'milk', 'comfortable'], dtype='|S19')"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Ground Truth\n",
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[8517].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dim sum, tea, chinese, dumpling, shrimp, pork, downtown, wait, bun, pot, fried, dinner, chicken, congee, cake, egg, dessert, tart, bbq, mall]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[np.argsort(np.ravel(I_K[8517].todense()))[::-1][:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hit\n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[8517].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Queen And Beaver Public House (many reviews dessert place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 355, 'name': 'The Queen And Beaver Public House', 'stars': 4.0}\n",
      "8764\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, 'qaNt4vtVdge_S68DVjw5Jg')\n",
    "print get_itemindex_from_business_id(ItemIndex, 'qaNt4vtVdge_S68DVjw5Jg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mall', 'pot', 'wait', 'bar', 'friendly', 'beer', 'tea', 'fresh',\n",
       "       'chicken', 'pizza', 'beef', 'salad', 'sandwich', 'busy', 'quick',\n",
       "       'lunch', 'tart', 'meat', 'egg', 'store', 'dinner', 'cheese',\n",
       "       'fast', 'pop', 'soup', 'spicy', 'bun', 'pub', 'pork', 'ice cream',\n",
       "       'bread', 'wing', 'fry', 'dessert', 'tofu', 'cake', 'coffee',\n",
       "       'crust', 'fried', 'rib', 'stick', 'clean', 'burger', 'downtown',\n",
       "       'corn', 'roasted', 'cocktail', 'apple', 'tuna', 'bean', 'wine',\n",
       "       'noodle', 'potato', 'tomato', 'chocolate', 'crispy', 'movie',\n",
       "       'comfortable', 'bbq', 'shrimp', 'fair', 'helpful', 'thai', 'salt',\n",
       "       'cozy', 'topped', 'casual', 'chip', 'theatre', 'reasonable'],\n",
       "      dtype='|S19')"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TF\n",
    "test_list = list(map(int, explanation[8764])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mall', 'wait', 'tea', 'pot', 'friendly', 'fresh', 'chicken',\n",
       "       'salad', 'pork', 'beef', 'egg', 'meat', 'fried', 'tapioca', 'bar',\n",
       "       'uber eats', 'tart', 'cocktail', 'bubble tea', 'fast', 'quick',\n",
       "       'soup', 'lunch', 'busy', 'gong cha', 'general tao', 'wild boar',\n",
       "       'dinner', 'dessert', 'cheese', 'noodle', 'fry', 'vietnamese',\n",
       "       'cake', 'kimchi', 'clean', 'spicy', 'fried rice', 'sushi',\n",
       "       'japanese', 'gelato', 'pork bone soup', 'fish', 'matcha',\n",
       "       'pork belly', 'scallop', 'coffee', 'alcoholic beverage', 'bubble',\n",
       "       'ramen', 'octopus', 'dietary restriction', 'spring roll', 'congee',\n",
       "       'lobster', 'poutine', 'skewer', 'store', 'squid', 'tempura',\n",
       "       'mexican', 'juicy', 'accept debit', 'croissant', 'cone', 'crispy',\n",
       "       'seafood', 'calamari', 'milk tea', 'rib'], dtype='|S19')"
      ]
     },
     "execution_count": 847,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TFIDF\n",
    "test_list = list(map(int, explanation[8764])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mall, pot, wait, bar, friendly, beer, tea, fresh, chicken, pizza, beef, salad, sandwich, busy, quick, lunch, tart, meat, egg, store]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mall, wait, tea, pot, friendly, fresh, chicken, salad, pork, beef, egg, meat, fried, tapioca, bar, uber eats, tart, cocktail, bubble tea, fast]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pub', 'beer', 'dinner', 'pot', 'bar', 'chip', 'burger', 'mall',\n",
       "       'fish', 'egg', 'cozy', 'tea', 'wait', 'fry', 'fresh', 'beef',\n",
       "       'brunch', 'cocktail', 'salt', 'cheese', 'bacon', 'stick',\n",
       "       'comfortable', 'downtown', 'meat', 'friendly', 'chair', 'potato',\n",
       "       'rib', 'lunch', 'bun', 'bread', 'pork', 'greasy', 'pricey',\n",
       "       'patty', 'traditional', 'tomato', 'salad', 'wing', 'sausage',\n",
       "       'quick', 'attentive', 'tart', 'fast', 'toast', 'fancy', 'dark',\n",
       "       'solid', 'breakfast', 'dessert', 'seasoned', 'crispy', 'washroom',\n",
       "       'pleasant', 'lemon', 'vegetarian', 'clean', 'latte', 'lamb',\n",
       "       'gravy', 'refreshing', 'fried', 'reasonable', 'refill', 'french',\n",
       "       'crowded', 'soup', 'rare', 'bean'], dtype='|S19')"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[8764].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pub, beer, dinner, pot, bar, chip, burger, mall, fish, egg, cozy, tea, wait, fry, fresh, beef, brunch, cocktail, salt, cheese]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[np.argsort(np.ravel(I_K[8764].todense()))[::-1][:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hit\n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[8764].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red Lobster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 93, 'name': 'Red Lobster', 'stars': 3.0}\n",
      "7312\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, 'hTdJAjSZtHWwqqh5cCeAfA')\n",
    "print get_itemindex_from_business_id(ItemIndex, 'hTdJAjSZtHWwqqh5cCeAfA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tea', 'chicken', 'mall', 'wait', 'friendly', 'fresh', 'fast',\n",
       "       'pot', 'quick', 'milk', 'clean', 'bubble', 'bubble tea', 'busy',\n",
       "       'rib', 'dessert', 'fry', 'pop', 'cake', 'lunch', 'milk tea', 'egg',\n",
       "       'tart', 'salad', 'breakfast', 'beef', 'cheese', 'dinner', 'soup',\n",
       "       'store', 'noodle', 'fried', 'meat', 'tapioca', 'bar', 'spicy',\n",
       "       'burger', 'fish', 'curry', 'parking', 'steak', 'coffee', 'wing',\n",
       "       'fruit', 'chinese', 'corn', 'potato', 'bread', 'bacon',\n",
       "       'chocolate', 'fair', 'tax', 'mango', 'plaza', 'helpful',\n",
       "       'attentive', 'bean', 'crispy', 'toast', 'stick', 'bun',\n",
       "       'reasonable', 'lemon', 'sour', 'juicy', 'asian', 'salt',\n",
       "       'comfortable', 'markham', 'pleasant'], dtype='|S19')"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TF\n",
    "test_list = list(map(int, explanation[7312])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tea', 'chicken', 'mall', 'wait', 'friendly', 'fresh', 'fast',\n",
       "       'pot', 'quick', 'milk', 'clean', 'bubble', 'bubble tea', 'busy',\n",
       "       'rib', 'dessert', 'fry', 'pop', 'cake', 'lunch', 'milk tea', 'egg',\n",
       "       'tart', 'salad', 'breakfast', 'beef', 'cheese', 'dinner', 'soup',\n",
       "       'store', 'noodle', 'fried', 'meat', 'tapioca', 'bar', 'spicy',\n",
       "       'burger', 'fish', 'curry', 'parking', 'steak', 'coffee', 'wing',\n",
       "       'fruit', 'chinese', 'corn', 'potato', 'bread', 'bacon',\n",
       "       'chocolate', 'fair', 'tax', 'mango', 'plaza', 'helpful',\n",
       "       'attentive', 'bean', 'crispy', 'toast', 'stick', 'bun',\n",
       "       'reasonable', 'lemon', 'sour', 'juicy', 'asian', 'salt',\n",
       "       'comfortable', 'markham', 'pleasant'], dtype='|S20')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TF with Vader\n",
    "test_list = list(map(int, explanation[7312])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wait', 'mall', 'tea', 'fresh', 'pot', 'friendly', 'chicken',\n",
       "       'cheese', 'salad', 'pork', 'beef', 'meat', 'croissant', 'cocktail',\n",
       "       'tart', 'cake', 'english muffin', 'pork belly', 'pork bone soup',\n",
       "       'quick', 'clean', 'busy', 'egg', 'accept debit', 'soup',\n",
       "       'pale ale', 'public transit', 'financial district', 'wild boar',\n",
       "       'dinner', 'lunch', 'balsamic vinegar', 'dessert', 'mexico',\n",
       "       'fried', 'uber eats', 'general tao', 'bar', 'grand opening',\n",
       "       'fast', 'noodle', 'pleasant', 'spicy', 'cocktail', 'sashimi',\n",
       "       'fry', 'fluffy', 'dim sum', 'oyster', 'tempura', 'fish', 'duck',\n",
       "       'congee', 'italian', 'scallop', 'soggy', 'octopus', 'smoked',\n",
       "       'ramen', 'tofu', 'beer', 'sushi', 'taco', 'wine', 'classic',\n",
       "       'cookie', 'cone', 'pasta', 'rib', 'japanese'], dtype='|S19')"
      ]
     },
     "execution_count": 849,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TFIDF\n",
    "test_list = list(map(int, explanation[7312])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mall', 'wait', 'tea', 'fresh', 'ok nothing', 'pot', 'friendly',\n",
       "       'chicken', 'pleasant', 'little gem', 'escape room',\n",
       "       'super helpful', 'great coffee', 'pork bone soup',\n",
       "       'favourite place', 'great quality', 'mexico', 'taste menu',\n",
       "       'great custom service', 'la carnita', 'soup', 'amazing food',\n",
       "       'lunch', 'many flavour', 'croissant', 'farmer market', 'bang bang',\n",
       "       'sure everything', 'perfect place', 'sushi bar', 'quick',\n",
       "       'amazing experience', 'wonderful experience',\n",
       "       'favourite restaurant', 'cake', 'egg', 'cheese', 'deer garden',\n",
       "       'salad', 'pork', 'beef', 'meat', 'general tao', 'wild boar',\n",
       "       'financial district', 'pale ale', 'public transit',\n",
       "       'balsamic vinegar', 'uber eats', 'busy', 'grand opening', 'clean',\n",
       "       'english muffin', 'accept debit', 'dinner', 'favourit thing',\n",
       "       'pork belly', 'vietnamese coffee', 'second chance', 'tart', 'fast',\n",
       "       'instant noodle', 'cocktail', 'high side', 'averag price',\n",
       "       'little bland', 'high price', 'slow service', 'bar',\n",
       "       'terrible service'], dtype='|S20')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TFIDF with Vader\n",
    "test_list = list(map(int, explanation[7312])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tea, chicken, mall, wait, friendly, fresh, fast, pot, quick, milk, clean, bubble, bubble tea, busy, rib, dessert, fry, pop, cake, lunch]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wait, mall, tea, fresh, pot, friendly, chicken, cheese, salad, pork, beef, meat, croissant, cocktail, tart, cake, english muffin, pork belly, pork bone soup, quick]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mall, wait, tea, fresh, ok nothing, pot, friendly, chicken, pleasant, little gem, escape room, super helpful, great coffee, pork bone soup, favourite place, great quality, mexico, taste menu, great custom service, la carnita]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lobster', 'wait', 'seafood', 'shrimp', 'friendly', 'fresh',\n",
       "       'mall', 'fish', 'dinner', 'pot', 'potato', 'tea', 'salad',\n",
       "       'dessert', 'fried', 'lunch', 'crispy', 'refill', 'dip', 'bar',\n",
       "       'stuffed', 'quick', 'busy', 'four', 'pasta', 'bread', 'soup',\n",
       "       'coconut', 'tart', 'baked', 'fry', 'chocolate', 'chip', 'scallop',\n",
       "       'quiet', 'bun', 'cheese', 'chicken', 'parking', 'rib', 'tuna',\n",
       "       'creamy', 'greasy', 'salt', 'casual', 'classic', 'immediately',\n",
       "       'roasted', 'cheaper', 'pizza', 'crowded', 'beer', 'wing', 'cake',\n",
       "       'topped', 'cheesecake', 'latte', 'strawberry', 'steamed',\n",
       "       'markham', 'pop', 'meat', 'juicy', 'ice cream', 'tomato', 'cookie',\n",
       "       'deep fried', 'salmon', 'steak', 'dog'], dtype='|S19')"
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ground Truth\n",
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[7312].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lobster, wait, seafood, shrimp, friendly, fresh, mall, fish, dinner, pot, potato, tea, salad, dessert, fried, lunch, crispy, refill, dip, bar]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[np.argsort(np.ravel(I_K[7312].todense()))[::-1][:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hits \n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[7312].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 604, 'name': 'Miku', 'stars': 4.0}\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, '0a2O150ytxrDjDzXNfRWkA')\n",
    "print get_itemindex_from_business_id(ItemIndex, '0a2O150ytxrDjDzXNfRWkA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wait', 'mall', 'tea', 'fresh', 'cake', 'pot', 'egg', 'meat',\n",
       "       'pork', 'dessert', 'chicken', 'friendly', 'tart', 'dinner',\n",
       "       'cheese', 'soup', 'busy', 'ice cream', 'bar', 'fish', 'noodle',\n",
       "       'quick', 'fried', 'beef', 'lunch', 'spicy', 'salad', 'pop', 'salt',\n",
       "       'sushi', 'matcha', 'japanese', 'rib', 'shrimp', 'fast', 'salmon',\n",
       "       'ramen', 'tuna', 'corn', 'creamy', 'crispy', 'seafood', 'coffee',\n",
       "       'milk', 'latte', 'wing', 'bean', 'oyster', 'waffle', 'clean',\n",
       "       'bbq', 'lobster', 'brunch', 'fair', 'pricey', 'parking', 'belly',\n",
       "       'cheesecake', 'attentive', 'sashimi', 'green tea', 'chocolate',\n",
       "       'cocktail', 'fry', 'pancake', 'yummy', 'bread', 'pork belly',\n",
       "       'curry', 'potato'], dtype='|S19')"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TF\n",
    "test_list = list(map(int, explanation[273])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wait', 'mall', 'tea', 'fresh', 'cake', 'pot', 'egg', 'meat',\n",
       "       'pork', 'dessert', 'chicken', 'friendly', 'tart', 'dinner',\n",
       "       'cheese', 'soup', 'busy', 'ice cream', 'bar', 'fish', 'noodle',\n",
       "       'quick', 'fried', 'beef', 'lunch', 'spicy', 'salad', 'pop', 'salt',\n",
       "       'sushi', 'matcha', 'japanese', 'rib', 'shrimp', 'fast', 'salmon',\n",
       "       'ramen', 'tuna', 'corn', 'creamy', 'crispy', 'seafood', 'coffee',\n",
       "       'milk', 'latte', 'wing', 'bean', 'oyster', 'waffle', 'clean',\n",
       "       'bbq', 'lobster', 'brunch', 'fair', 'pricey', 'parking', 'belly',\n",
       "       'cheesecake', 'attentive', 'sashimi', 'green tea', 'chocolate',\n",
       "       'cocktail', 'fry', 'pancake', 'yummy', 'bread', 'pork belly',\n",
       "       'curry', 'potato'], dtype='|S20')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TF with vader\n",
    "test_list = list(map(int, explanation[273])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mall', 'wait', 'tea', 'fresh', 'friendly', 'pot', 'chicken',\n",
       "       'bar', 'tart', 'quick', 'meat', 'beef', 'salad', 'cheese', 'egg',\n",
       "       'busy', 'cocktail', 'gong cha', 'fast', 'fried', 'lunch', 'dinner',\n",
       "       'soup', 'dessert', 'pork', 'cake', 'noodle', 'spicy', 'pale ale',\n",
       "       'clean', 'fry', 'wild boar', 'alcoholic beverage', 'general tao',\n",
       "       'uber eats', 'fish', 'pork bone soup', 'mexico', 'english muffin',\n",
       "       'poutine', 'burger', 'mexican', 'dim sum', 'gravy', 'coffee',\n",
       "       'accept debit', 'store', 'bread', 'congee', 'buffet', 'cocktail',\n",
       "       'rib', 'sandwich', 'gelato', 'brunch', 'vietnamese', 'skewer',\n",
       "       'sashimi', 'balsamic vinegar', 'croissant', 'bakery', 'tempura',\n",
       "       'pizza', 'curry', 'indian', 'beer', 'sushi', 'spring roll',\n",
       "       'donut', 'olive'], dtype='|S19')"
      ]
     },
     "execution_count": 851,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TFIDF\n",
    "test_list = list(map(int, explanation[273])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mall', 'wait', 'tea', 'friendly', 'fresh', 'pot', 'chicken',\n",
       "       'bar', 'favourite place', 'averag price', 'meat',\n",
       "       'great custom service', 'beef', 'tart', 'cheese', 'egg',\n",
       "       'nail salon', 'favourit thing', 'salad', 'gong cha', 'fast',\n",
       "       'cocktail', 'busy', 'soup', 'lunch', 'dinner', 'fried', 'quick',\n",
       "       'pork', 'dessert', 'escape room', 'cake', 'terrible service',\n",
       "       'noodle', 'hair cut', 'deer garden', 'board game', 'spicy',\n",
       "       'pale ale', 'asian legend', 'clean', 'la carnita', 'farmer market',\n",
       "       'wild boar', 'fry', 'alcoholic beverage', 'general tao', 'fish',\n",
       "       'pork bone soup', 'uber eats', 'dry side', 'english muffin',\n",
       "       'dim sum place', 'mexico', 'poutine', 'burger', 'mexican', 'gravy',\n",
       "       'dim sum', 'swiss chalet', 'vietnamese coffee', 'bread', 'coffee',\n",
       "       'store', 'congee', 'buffet', 'accept debit', 'skewer', 'bad day',\n",
       "       'cocktail'], dtype='|S20')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TFIDF Vader\n",
    "test_list = list(map(int, explanation[273])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wait, mall, tea, fresh, cake, pot, egg, meat, pork, dessert, chicken, friendly, tart, dinner, cheese, soup, busy, ice cream, bar, fish]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mall, wait, tea, fresh, friendly, pot, chicken, bar, tart, quick, meat, beef, salad, cheese, egg, busy, cocktail, gong cha, fast, fried]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mall, wait, tea, friendly, fresh, pot, chicken, bar, favourite place, averag price, meat, great custom service, beef, tart, cheese, egg, nail salon, favourit thing, salad, gong cha]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sushi', 'dessert', 'salmon', 'fish', 'tea', 'fresh', 'sashimi',\n",
       "       'green tea', 'cake', 'dinner', 'japanese', 'matcha', 'tart',\n",
       "       'lunch', 'ice cream', 'tuna', 'chocolate', 'wait', 'scallop',\n",
       "       'mall', 'miso', 'salad', 'beef', 'latte', 'birthday', 'seafood',\n",
       "       'friendly', 'attentive', 'pot', 'rib', 'tofu', 'bar', 'shrimp',\n",
       "       'oyster', 'modern', 'bean', 'egg', 'pricey', 'cocktail', 'clean',\n",
       "       'nicely', 'fried', 'lobster', 'sesame', 'baked', 'quick', 'wine',\n",
       "       'salt', 'creamy', 'busy', 'spicy', 'traditional', 'calamari',\n",
       "       'dark', 'pop', 'meat', 'crunchy', 'seasoned', 'four', 'spacious',\n",
       "       'squid', 'fruit', 'stick', 'crispy', 'bacon', 'topped', 'bread',\n",
       "       'potato', 'tomato', 'pleasant'], dtype='|S19')"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ground Truth\n",
    "np.array(keyphrases)[np.argsort(np.ravel(I_K[273].todense()))[::-1][:70]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sushi, dessert, salmon, fish, tea, fresh, sashimi, green tea, cake, dinner, japanese, matcha, tart, lunch, ice cream, tuna, chocolate, wait, scallop, mall]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[np.argsort(np.ravel(I_K[273].todense()))[::-1][:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of Hits \n",
    "len(np.where([i in np.array(keyphrases)[np.argsort(np.ravel(I_K[273].todense()))[::-1][:70]] for i in np.array(keyphrases)[test_list]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nando's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 604, 'name': 'Miku', 'stars': 4.0}\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "print get_restaurant_info(business_df, '0a2O150ytxrDjDzXNfRWkA')\n",
    "print get_itemindex_from_business_id(ItemIndex, '0a2O150ytxrDjDzXNfRWkA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mall', 'wait', 'tea', 'friendly', 'fresh', 'pot', 'chicken',\n",
       "       'bar', 'favourite place', 'averag price', 'meat',\n",
       "       'great custom service', 'beef', 'tart', 'cheese', 'egg',\n",
       "       'nail salon', 'favourit thing', 'salad', 'gong cha', 'fast',\n",
       "       'cocktail', 'busy', 'soup', 'lunch', 'dinner', 'fried', 'quick',\n",
       "       'pork', 'dessert', 'escape room', 'cake', 'terrible service',\n",
       "       'noodle', 'hair cut', 'deer garden', 'board game', 'spicy',\n",
       "       'pale ale', 'asian legend', 'clean', 'la carnita', 'farmer market',\n",
       "       'wild boar', 'fry', 'alcoholic beverage', 'general tao', 'fish',\n",
       "       'pork bone soup', 'uber eats', 'dry side', 'english muffin',\n",
       "       'dim sum place', 'mexico', 'poutine', 'burger', 'mexican', 'gravy',\n",
       "       'dim sum', 'swiss chalet', 'vietnamese coffee', 'bread', 'coffee',\n",
       "       'store', 'congee', 'buffet', 'accept debit', 'skewer', 'bad day',\n",
       "       'cocktail'], dtype='|S20')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TF-IDF Vader\n",
    "test_list = list(map(int, explanation[273])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mall, wait, tea, friendly, fresh, pot, chicken, bar, favourite place, averag price, meat, great custom service, beef, tart, cheese, egg, nail salon, favourit thing, salad, gong cha]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wait, chicken, mall, friendly, fresh, pot, tea, la carnita, cheese, cake, egg, fish, burger, gelato, salad, lactose intolerant, beef, meat, chili, refreshing]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[np.argsort(np.ravel(I_K[273].todense()))[::-1][:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GYUBEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_count': 106, 'name': 'Gyubee Japanese BBQ - Downtown', 'stars': 4.0}\n",
      "8745\n"
     ]
    }
   ],
   "source": [
    "# 8745\n",
    "print get_restaurant_info(business_df, 'qR4EIktJfQKc4rnKgBzvtw')\n",
    "print get_itemindex_from_business_id(ItemIndex, 'qR4EIktJfQKc4rnKgBzvtw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mall', 'wait', 'fresh', 'tea', 'pot', 'chicken', 'friendly',\n",
       "       'la carnita', 'favourite place', 'public transit',\n",
       "       'balsamic vinegar', 'hair cut', 'escape room', 'wild boar',\n",
       "       'great coffee', 'great custom service', 'favourit thing', 'mexico',\n",
       "       'pale ale', 'cheese', 'general tao', 'egg', 'lunch', 'salad',\n",
       "       'beef', 'meat', 'congee', 'tart', 'cocktail', 'busy', 'quick',\n",
       "       'soup', 'english muffin', 'fast', 'bad taste', 'averag price',\n",
       "       'bar', 'dessert', 'dinner', 'plus side', 'fried', 'spicy', 'pork',\n",
       "       'noodle', 'cake', 'gelato', 'clean', 'fry', 'filet mignon',\n",
       "       'patty', 'bang bang', 'swiss chalet', 'terrible service',\n",
       "       'nail salon', 'second chance', 'buffet', 'foie gras',\n",
       "       'great recommend', 'real deal', 'thai', 'coffee', 'separate bill',\n",
       "       'pizza', 'dim sum', 'extra star', 'mediocre food', 'quick meal',\n",
       "       'croissant', 'bad day', 'sushi bar'], dtype='|S20')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted TF-IDF Vader\n",
    "test_list = list(map(int, explanation[8745])) \n",
    "np.array(keyphrases)[test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mall, wait, fresh, tea, pot, chicken, friendly, la carnita, favourite place, public transit, balsamic vinegar, hair cut, escape room, wild boar, great coffee, great custom service, favourit thing, mexico, pale ale, cheese]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[test_list][:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pot, fresh, mall, wait, tea, chicken, friendly, milk, fried chicken, milk tea, green tea, spring roll, bubble tea, dumpling, fried rice, pork bone soup, parking, shopping, pasta, la carnita]\n"
     ]
    }
   ],
   "source": [
    "print '[%s]' % ', '.join(map(str, np.array(keyphrases)[np.argsort(np.ravel(I_K[8745].todense()))[::-1][:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-based TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df(U_K):\n",
    "    \"\"\"\n",
    "    Get the df for keyphrase matrix\n",
    "    \"\"\"\n",
    "    DF = {}\n",
    "    for i in range(U_K.shape[0]):\n",
    "        vector_u = np.ravel(U_K[i].todense())\n",
    "        for keyword_idx in range(U_K.shape[1]):\n",
    "            keyword_frequency = vector_u[keyword_idx]\n",
    "            if keyword_idx in DF:\n",
    "                DF[keyword_idx] += keyword_frequency\n",
    "            else:\n",
    "                DF[keyword_idx] = keyword_frequency\n",
    "    return DF\n",
    "\n",
    "def keyword_popularity_matrix(U_K,df_uk):\n",
    "    \"\"\"\n",
    "    return the U_K normalized by keyphrase frequency\n",
    "    \"\"\"\n",
    "    df = normalize(np.array(df_uk.values()).reshape(1,-1))[0]\n",
    "    for i in tqdm(range(U_K.shape[0])):\n",
    "        U_K[i] = U_K[i]/df\n",
    "#         vector_u = np.ravel(U_K[i].todense())\n",
    "#         for entry in range(U_K.shape[1]):\n",
    "#             U_K[i,entry] = U_K[i,entry]/df[entry]\n",
    "    return normalize(U_K)\n",
    "    \n",
    "def tf_idf(U_K,df_uk):\n",
    "    \"\"\"\n",
    "    Change the U_K/I_K keyphrase matrix to TF-IDF.\n",
    "    where:\n",
    "    IDF = log(N/(DF+1))\n",
    "    TF_IDF = (1+log(TF))* IDF\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(U_K.shape[0])):\n",
    "        vector_u = np.ravel(U_K[i].todense())\n",
    "        words_count = np.sum(vector_u)\n",
    "        \n",
    "        for entry in range(U_K.shape[1]):\n",
    "            if U_K[i,entry] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                tf = U_K[i,entry]/words_count\n",
    "                df = df_uk[entry]\n",
    "                idf = np.log(U_K.shape[0]/(df+1))\n",
    "                tf_idf = (1+ np.log(tf))*idf\n",
    "                U_K[i,entry] = tf_idf\n",
    "    return normalize(U_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \n",
    "U_K = load_npz('../../data/yelp/U_K.npz')\n",
    "I_K = load_npz('../../data/yelp/I_K.npz')\n",
    "# U_K_test = load_npz('../../data/yelp/U_K_test.npz')\n",
    "# I_K_test = load_npz('../../data/yelp/I_K_test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_uk = df(U_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:56<00:00, 43.61it/s]\n"
     ]
    }
   ],
   "source": [
    "U_K_tfidf = tf_idf(U_K, df_uk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:01<00:00, 1637.75it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:00<00:00, 7168.12it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity = train(rtrain)\n",
    "explanation_scores = predict(U_K_tfidf, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 70, U_K_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:04<00:00, 500.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:05<00:00, 456.44it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:05<00:00, 458.73it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:05<00:00, 492.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:05<00:00, 478.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.8037531863150353, 0.0025584264470937825),\n",
       " 'MAP@20': (0.8887167514060423, 0.0006770233228284741),\n",
       " 'MAP@5': (0.6919010416666667, 0.006329136104857982),\n",
       " 'MAP@50': (0.8761517727900731, 0.00023810741276318617),\n",
       " 'NDCG': (0.862984800655276, 0.00015005118952633647),\n",
       " 'Precision@10': (0.6789496527777779, 0.0016965583911800802),\n",
       " 'Precision@20': (0.756814236111111, 0.0010347377179239256),\n",
       " 'Precision@5': (0.6246527777777777, 0.0041900668251206455),\n",
       " 'Precision@50': (0.7324131944444444, 0.0005940355445510568),\n",
       " 'R-Precision': (0.7324131944444444, 0.0005940355445510568),\n",
       " 'Recall@10': (0.6789496527777779, 0.0016965583911800802),\n",
       " 'Recall@20': (0.756814236111111, 0.0010347377179239256),\n",
       " 'Recall@5': (0.6246527777777777, 0.0041900668251206455),\n",
       " 'Recall@50': (0.7324131944444444, 0.0005940355445510568)}"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, U_K_tfidf, atK=[5,10,20,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:07<00:00, 351.43it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:07<00:00, 349.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:07<00:00, 347.72it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:07<00:00, 347.14it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 2473/2473 [00:07<00:00, 344.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.7252943466159611, 0.0034826018579463785),\n",
       " 'MAP@20': (0.8975108855170579, 0.0006411193458138319),\n",
       " 'MAP@5': (0.7052025462962963, 0.006176983458372947),\n",
       " 'MAP@50': (0.8379645827078837, 0.0002830322565324534),\n",
       " 'NDCG': (0.838787940381189, 0.00017489650280682394),\n",
       " 'Precision@10': (0.6920138888888889, 0.0019660558509479948),\n",
       " 'Precision@20': (0.7953993055555555, 0.000999342626346402),\n",
       " 'Precision@5': (0.6313368055555555, 0.004201135758998467),\n",
       " 'Precision@50': (0.6735590277777778, 0.0005995899582526865),\n",
       " 'R-Precision': (0.6735590277777778, 0.0005995899582526865),\n",
       " 'Recall@10': (0.6920138888888889, 0.0019660558509479948),\n",
       " 'Recall@20': (0.7953993055555555, 0.000999342626346402),\n",
       " 'Recall@5': (0.6313368055555555, 0.004201135758998467),\n",
       " 'Recall@50': (0.6735590277777778, 0.0005995899582526865)}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With Vader keyphrases\n",
    "explain_evaluate(explanation, U_K_tfidf, atK=[5,10,20,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-based TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ik = df(I_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10282/10282 [02:25<00:00, 70.47it/s]\n"
     ]
    }
   ],
   "source": [
    "I_K_tfidf = tf_idf(I_K,df_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10282/10282 [00:06<00:00, 1651.20it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 10282/10282 [00:01<00:00, 7472.38it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity = train(np.transpose(rtrain))\n",
    "explanation_scores = predict(I_K_tfidf, 100, similarity)\n",
    "# explanation =  predict_explanation(explanation_scores)\n",
    "explanation = explain_prediction(explanation_scores, 70, I_K_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:08<00:00, 150.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:08<00:00, 149.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:07<00:00, 152.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:07<00:00, 152.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:07<00:00, 151.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.35213217629153576, 0.0010968833246985527),\n",
       " 'MAP@20': (0.27081058935900426, 0.0010039121441790747),\n",
       " 'MAP@5': (0.4431528783835303, 0.002145923498719915),\n",
       " 'MAP@50': (0.27487874661333556, 0.0015240019863223988),\n",
       " 'NDCG': (0.31342782756537074, 0.001028350554491715),\n",
       " 'Precision@10': (0.20875587749396368, 0.000586581336588827),\n",
       " 'Precision@20': (0.16421400432075234, 0.0015456733985390193),\n",
       " 'Precision@5': (0.356157072054899, 0.0016005530998861061),\n",
       " 'Precision@50': (0.2102732240437159, 0.0013700342110130942),\n",
       " 'R-Precision': (0.2102732240437159, 0.0013700342110130942),\n",
       " 'Recall@10': (0.20875587749396368, 0.000586581336588827),\n",
       " 'Recall@20': (0.16421400432075234, 0.0015456733985390193),\n",
       " 'Recall@5': (0.356157072054899, 0.0016005530998861061),\n",
       " 'Recall@50': (0.2102732240437159, 0.0013700342110130942)}"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_evaluate(explanation, I_K_tfidf, atK=[5,10,20,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:38<00:00, 104.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:37<00:00, 105.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:38<00:00, 104.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:38<00:00, 104.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 10282/10282 [01:38<00:00, 104.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.3507409410095894, 0.0011045160159365245),\n",
       " 'MAP@20': (0.24377436981475908, 0.0005716380060796823),\n",
       " 'MAP@5': (0.4456696208764156, 0.002164483417861961),\n",
       " 'MAP@50': (0.23215810481762375, 0.0007119926667678546),\n",
       " 'NDCG': (0.22997626464891138, 0.0007149598007652743),\n",
       " 'Precision@10': (0.20089857213195475, 0.00019460249576673538),\n",
       " 'Precision@20': (0.10384662727720337, 0.0003505696725099028),\n",
       " 'Precision@5': (0.3505169867060562, 0.0016703656098363416),\n",
       " 'Precision@50': (0.1551649433776465, 0.0007062145480243385),\n",
       " 'R-Precision': (0.1551649433776465, 0.0007062145480243385),\n",
       " 'Recall@10': (0.20089857213195475, 0.00019460249576673538),\n",
       " 'Recall@20': (0.10384662727720337, 0.0003505696725099028),\n",
       " 'Recall@5': (0.3505169867060562, 0.0016703656098363416),\n",
       " 'Recall@50': (0.1551649433776465, 0.0007062145480243385)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With Vader\n",
    "explain_evaluate(explanation, I_K_tfidf, atK=[5,10,20,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top_items items associated with keyphrase_ids \n",
    "def item_associated_with_keyphrase(I_K, keyphrase_ids, top_items = 100):\n",
    "    \"\"\"\n",
    "    I_K: Item Keyphrase Matrix\n",
    "    Keyphrase_ids: top items described by keyphrase_ids\n",
    "    top_n: how many top items described by the keyphrases will be output \n",
    "    output item list (unique)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for keyphrase_id in keyphrase_ids:\n",
    "        res.append(np.argsort(np.ravel(I_K.todense()[:,keyphrase_id]))[::-1][:top_items])\n",
    "    return np.unique(res)\n",
    "\n",
    "# Modify U_U latent Space from U_I\n",
    "def modify_user_preference(U_I, items, user_id = 0):\n",
    "    \"\"\"\n",
    "    TODO: Fix the function s.t. it will not modify the initial matrix\n",
    "    \"\"\"\n",
    "    U_I[user_id,:] = 0\n",
    "    for i in items:\n",
    "        U_I[0,i] = 1\n",
    "    return U_I\n",
    "\n",
    "def clear_user_keyphrase(U_K, user_id = 0):\n",
    "    U_K[user_id,:] = 0\n",
    "\n",
    "def explain_synthetic_user(rtrain, U_K, I_K, keyphrases, top_keyphrase = 20, user_id = 0, k = 100, top_items = 100, **Not_used):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    items = item_associated_with_keyphrase(I_K, keyphrases, top_items = top_items) # 8 is coffee\n",
    "    U_I = modify_user_preference(rtrain, items, user_id=user_id)\n",
    "    modified_U_U = train(U_I)\n",
    "    U_K[user_id, :] = 0\n",
    "    synthetic_user_keyphrase = normalize(predict(U_K, k, modified_U_U))[user_id]\n",
    "    return np.argsort(synthetic_user_keyphrase)[::-1][:top_keyphrase]\n",
    "\n",
    "def modify_user_keyphrase(U_K, keyphrase_ids, normalization = True, keyval = 1, user_id = 0, **Not_Used):\n",
    "    \"\"\"\n",
    "    Change all keyphrase_ids to some fixed number, all others to 0\n",
    "    Return the U_K matrix with user_id row the synthetic user1\n",
    "    \"\"\"\n",
    "    U_K[user_id,:] = 0\n",
    "    for keyphrase_id in keyphrase_ids:\n",
    "        U_K[user_id,keyphrase_id] = keyval\n",
    "    if normalization == True:\n",
    "        return normalize(U_K)\n",
    "    return U_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:19: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "# Modify user preference matrix\n",
    "items = item_associated_with_keyphrase(I_K, [0], top_items = 200) # 'chinese'\n",
    "U_I = modify_user_preference(rtrain, items, user_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:25: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "# make synthetic user1's keyphrase preference all 0\n",
    "clear_user_keyphrase(U_K, user_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2343x233 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 298309 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7456x233 sparse matrix of type '<type 'numpy.int32'>'\n",
       "\twith 351924 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:01<00:00, 2151.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 97624.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get latent user similarity embedding\n",
    "modified_U_U = train(U_I)\n",
    "# predict\n",
    "explanation_scores1 = predict(U_K, 100, modified_U_U)\n",
    "explanation1 =  predict_pilot_explanation(explanation_scores1, top_keyphrase = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50,  60, 155, 205,  57, 160, 197,  61, 164, 226], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_in_prediction(rtrain, U_K, I_K, top_items = 200, keyphrase = 0,user_i = 0):\n",
    "    \"\"\"\n",
    "    Get the rank for user_i with keyphrase\n",
    "    TODO: modify so that no need to reload U_K,I_K\n",
    "    \"\"\"  \n",
    "    U_K = load_npz('../../data/yelp/U_K.npz')\n",
    "    U_K = normalize(U_K)\n",
    "    rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")\n",
    "    \n",
    "    # Modify user preference matrix\n",
    "    items = item_associated_with_keyphrase(I_K, keyphrase, top_items = top_items) # 'raspberry'\n",
    "    U_I = modify_user_preference(rtrain, items, user_id = 0)\n",
    "    \n",
    "    # make synthetic user1's keyphrase preference all 0\n",
    "    clear_user_keyphrase(U_K, user_id = 0)\n",
    "    \n",
    "    # Get latent user similarity embedding\n",
    "    modified_U_U = train(U_I)\n",
    "    # predict\n",
    "    explanation_scores1 = predict(U_K, 100, modified_U_U)\n",
    "    explanation1 =  predict_pilot_explanation(explanation_scores1, top_keyphrase = 230)\n",
    "    return list(explanation1[user_i]).index(keyphrase[0])\n",
    "    \n",
    "def evaluate_pilot_test(rtrain,U_K, I_K,keyphrase_list, top_items = 200, user_i = 0):\n",
    "    # Get the average rank for user_i with keyphrase  \n",
    "    res1 = 0\n",
    "    for i in range(75):\n",
    "        a = rank_in_prediction(rtrain, U_K, I_K, top_items = top_items, keyphrase = [i],user_i = user_i)\n",
    "        print \"keyphrase\", keyphrase_list[i], \"'s rank is \", a\n",
    "        res1+= a\n",
    "    return res1/75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_pilot_test(rtrain,U_K,I_K,keyphrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Synthetic item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top_users items associated with keyphrase_ids \n",
    "def users_with_keyphrase_preference(U_K, keyphrase_ids, top_users = 100, norm = True):\n",
    "    \"\"\"\n",
    "    U_K: User Keyphrase Matrix\n",
    "    Keyphrase_ids: top_users who like keyphrase_ids\n",
    "    output item list (unique)\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    if norm:\n",
    "        U_K = normalize(U_K)\n",
    "    try:\n",
    "        for keyphrase_id in keyphrase_ids:\n",
    "            res.append(np.argsort(np.ravel(U_K.todense()[:,keyphrase_id]))[::-1][:top_users])\n",
    "    except:\n",
    "        return np.argsort(np.ravel(U_K.todense()[:,keyphrase_ids]))[::-1][:top_users]\n",
    "    return np.unique(res)\n",
    "\n",
    "# Modify I_I latent Space from U_I\n",
    "def modify_item_history(U_I, users, item_id = 0):\n",
    "    \"\"\"\n",
    "    TODO: Fix the function s.t. it will not modify the initial matrix\n",
    "    \"\"\"\n",
    "    U_I[:,item_id] = 0\n",
    "    for i in users:\n",
    "        U_I[i, item_id] = 1\n",
    "    return normalize(U_I)\n",
    "\n",
    "def clear_item_keyphrase(I_K, item_id = 0):\n",
    "    I_K[item_id, :] = 0\n",
    "\n",
    "def explain_synthetic_item(rtrain, U_K, I_K, keyphrases, top_keyphrase = 20, user_id = 0, k = 100, top_users = 100, **Not_used):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    items = item_associated_with_keyphrase(I_K, keyphrases, top_items = top_items) # 8 is coffee\n",
    "    U_I = modify_user_preference(rtrain, items, user_id=user_id)\n",
    "    modified_U_U = train(U_I)\n",
    "    U_K[user_id, :] = 0\n",
    "    synthetic_user_keyphrase = normalize(predict(U_K, k, modified_U_U))[user_id]\n",
    "    return np.argsort(synthetic_user_keyphrase)[::-1][:top_keyphrase]\n",
    "\n",
    "def modify_user_keyphrase(U_K, keyphrase_ids, normalization = True, keyval = 1, user_id = 0, **Not_Used):\n",
    "    \"\"\"\n",
    "    Change all keyphrase_ids to some fixed number, all others to 0\n",
    "    Return the U_K matrix with user_id row the synthetic user1\n",
    "    \"\"\"\n",
    "    U_K[user_id,:] = 0\n",
    "    for keyphrase_id in keyphrase_ids:\n",
    "        U_K[user_id,keyphrase_id] = keyval\n",
    "    if normalization == True:\n",
    "        return normalize(U_K)\n",
    "    return U_K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U_K = load_npz('../../data/yelp/U_K.npz')\n",
    "I_K = load_npz('../../data/yelp/I_K.npz')\n",
    "U_K = normalize(U_K)\n",
    "I_K = normalize(I_K)\n",
    "rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:23: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n"
     ]
    }
   ],
   "source": [
    "# Modify user preference matrix\n",
    "users = users_with_keyphrase_preference(U_K, 70, top_users = 100, norm = True) # 'raspberry'\n",
    "U_I = modify_item_history(rtrain, users, item_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make synthetic item1's keyphrase to all 0\n",
    "clear_item_keyphrase(I_K, item_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:03<00:00, 2184.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 7456/7456 [00:00<00:00, 112969.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get latent item similarity embedding\n",
    "modified_I_I = train(np.transpose(U_I))\n",
    "# predict\n",
    "explanation_scores1 = predict(I_K, 100, modified_I_I)\n",
    "explanation1 =  predict_pilot_explanation(explanation_scores1, top_keyphrase = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_in_prediction_item(rtrain, U_K, I_K, top_users = 200, keyphrase = 70, item_i = 0):\n",
    "    \"\"\"\n",
    "    Get the rank for user_i with keyphrase\n",
    "    TODO: modify so that no need to reload U_K,I_K\n",
    "    \"\"\"  \n",
    "    U_K = load_npz('../../data/yelp/U_K.npz')\n",
    "    I_K = load_npz('../../data/yelp/I_K.npz')\n",
    "    U_K = normalize(U_K)\n",
    "    I_K = normalize(I_K)\n",
    "    rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")\n",
    "    \n",
    "    # Modify user preference matrix\n",
    "    users = users_with_keyphrase_preference(U_K, keyphrase, top_users = top_users, norm = True) # 'raspberry'\n",
    "    U_I = modify_item_history(rtrain, users, item_id = item_i)\n",
    "    \n",
    "    # make synthetic item1's keyphrase to all 0\n",
    "    clear_item_keyphrase(I_K, item_id = item_i)\n",
    "    \n",
    "    # Get latent item similarity embedding\n",
    "    modified_I_I = train(np.transpose(U_I))\n",
    "    # predict\n",
    "    explanation_scores1 = predict(I_K, 100, modified_I_I)\n",
    "    explanation1 =  predict_pilot_explanation(explanation_scores1, top_keyphrase = 230)\n",
    "    return list(explanation1[item_i]).index(keyphrase)\n",
    "\n",
    "    \n",
    "def evaluate_pilot_test_item(rtrain,U_K, I_K,keyphrase_list, top_users = 200, item_i = 0):\n",
    "    # Get the average rank for item_i with keyphrase  \n",
    "    res1 = 0\n",
    "    for i in range(75):\n",
    "        a = rank_in_prediction_item(rtrain, U_K, I_K, top_users = top_users, keyphrase = i, item_i = item_i)\n",
    "        print \"keyphrase\", keyphrase_list[i], \"'s rank is \", a\n",
    "        res1+= a\n",
    "    return res1/75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:23: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "D:\\Anaconda\\envs\\py27\\lib\\site-packages\\ipykernel_launcher.py:29: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 7456/7456 [00:03<00:00, 2163.04it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 7456/7456 [00:00<00:00, 112969.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_in_prediction_item(rtrain, U_K, I_K, top_users = 200, keyphrase = 10, item_i = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate_pilot_test_item(rtrain,U_K, I_K,keyphrases, top_users = 200, item_i = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract different words from single review and mutiple reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_reviews = len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>business_id</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>ghost</th>\n",
       "      <th>img_dsc</th>\n",
       "      <th>img_url</th>\n",
       "      <th>nr</th>\n",
       "      <th>...</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Binary</th>\n",
       "      <th>review</th>\n",
       "      <th>conca_review</th>\n",
       "      <th>keyVector</th>\n",
       "      <th>keyphrases_indices_length</th>\n",
       "      <th>UserIndex</th>\n",
       "      <th>ItemIndex</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>105</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada. ...</td>\n",
       "      <td>['https://s3-media4.fl.yelpcdn.com/bphoto/tu7j...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>['ordered', 'lemon', 'mango', 'slush', 'lemon'...</td>\n",
       "      <td>ordered lemon mango slush lemon taste strong ...</td>\n",
       "      <td>[53, 92, 99, 112, 130, 212]</td>\n",
       "      <td>6</td>\n",
       "      <td>2464</td>\n",
       "      <td>5546</td>\n",
       "      <td>2016-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>171</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada. ...</td>\n",
       "      <td>['https://s3-media3.fl.yelpcdn.com/bphoto/h110...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>['came', 'sunday', 'afternoon', 'nt', 'busy', ...</td>\n",
       "      <td>came sunday afternoon nt busy came sunday spe...</td>\n",
       "      <td>[53, 99, 126, 128, 130, 148, 151, 171]</td>\n",
       "      <td>8</td>\n",
       "      <td>1021</td>\n",
       "      <td>5546</td>\n",
       "      <td>2016-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>239</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada. ...</td>\n",
       "      <td>['https://s3-media4.fl.yelpcdn.com/bphoto/tS6Y...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>['grapefruit', 'yakult', 'green', 'tea', 'aloe...</td>\n",
       "      <td>grapefruit yakult green tea aloe jelly found ...</td>\n",
       "      <td>[53, 92, 103, 129, 192]</td>\n",
       "      <td>5</td>\n",
       "      <td>529</td>\n",
       "      <td>5546</td>\n",
       "      <td>2016-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada',...</td>\n",
       "      <td>['https://s3-media3.fl.yelpcdn.com/bphoto/rfB0...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>['saw', 'newly', 'opened', 'bubble', 'tea', 's...</td>\n",
       "      <td>saw newly opened bubble tea shop wanted give ...</td>\n",
       "      <td>[49, 53, 99, 126, 128, 130, 136, 161, 206, 212]</td>\n",
       "      <td>10</td>\n",
       "      <td>1616</td>\n",
       "      <td>5546</td>\n",
       "      <td>2016-09-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada',...</td>\n",
       "      <td>['https://s3-media1.fl.yelpcdn.com/bphoto/2jVn...</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>['happy', 'lemon', 'become', 'new', 'favourite...</td>\n",
       "      <td>happy lemon become new favourite place sweet ...</td>\n",
       "      <td>[53, 92, 99, 103, 126, 128, 148, 152, 165, 197]</td>\n",
       "      <td>10</td>\n",
       "      <td>590</td>\n",
       "      <td>5546</td>\n",
       "      <td>2018-06-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n",
       "0           0             0               6                 6   \n",
       "1           1             1               7                 7   \n",
       "2           2             2               8                 8   \n",
       "3           3             3               9                 9   \n",
       "4           4             4              22                22   \n",
       "\n",
       "              business_id  friend_count  ghost  \\\n",
       "0  Xo1LNzhnwE-ilqsM3ybs9Q           105  False   \n",
       "1  Xo1LNzhnwE-ilqsM3ybs9Q           171  False   \n",
       "2  Xo1LNzhnwE-ilqsM3ybs9Q           239  False   \n",
       "3  Xo1LNzhnwE-ilqsM3ybs9Q            10  False   \n",
       "4  Xo1LNzhnwE-ilqsM3ybs9Q            80  False   \n",
       "\n",
       "                                             img_dsc  \\\n",
       "0  ['Photo of Happy Lemon - Markham, ON, Canada. ...   \n",
       "1  ['Photo of Happy Lemon - Markham, ON, Canada. ...   \n",
       "2  ['Photo of Happy Lemon - Markham, ON, Canada. ...   \n",
       "3  ['Photo of Happy Lemon - Markham, ON, Canada',...   \n",
       "4  ['Photo of Happy Lemon - Markham, ON, Canada',...   \n",
       "\n",
       "                                             img_url     nr  ...  Month  Day  \\\n",
       "0  ['https://s3-media4.fl.yelpcdn.com/bphoto/tu7j...  False  ...     23    8   \n",
       "1  ['https://s3-media3.fl.yelpcdn.com/bphoto/h110...  False  ...      2   10   \n",
       "2  ['https://s3-media4.fl.yelpcdn.com/bphoto/tS6Y...  False  ...      6   11   \n",
       "3  ['https://s3-media3.fl.yelpcdn.com/bphoto/rfB0...  False  ...     25    9   \n",
       "4  ['https://s3-media1.fl.yelpcdn.com/bphoto/2jVn...  False  ...     30    6   \n",
       "\n",
       "   Binary                                             review  \\\n",
       "0       0  ['ordered', 'lemon', 'mango', 'slush', 'lemon'...   \n",
       "1       0  ['came', 'sunday', 'afternoon', 'nt', 'busy', ...   \n",
       "2       1  ['grapefruit', 'yakult', 'green', 'tea', 'aloe...   \n",
       "3       0  ['saw', 'newly', 'opened', 'bubble', 'tea', 's...   \n",
       "4       1  ['happy', 'lemon', 'become', 'new', 'favourite...   \n",
       "\n",
       "                                        conca_review  \\\n",
       "0   ordered lemon mango slush lemon taste strong ...   \n",
       "1   came sunday afternoon nt busy came sunday spe...   \n",
       "2   grapefruit yakult green tea aloe jelly found ...   \n",
       "3   saw newly opened bubble tea shop wanted give ...   \n",
       "4   happy lemon become new favourite place sweet ...   \n",
       "\n",
       "                                         keyVector keyphrases_indices_length  \\\n",
       "0                      [53, 92, 99, 112, 130, 212]                         6   \n",
       "1           [53, 99, 126, 128, 130, 148, 151, 171]                         8   \n",
       "2                          [53, 92, 103, 129, 192]                         5   \n",
       "3  [49, 53, 99, 126, 128, 130, 136, 161, 206, 212]                        10   \n",
       "4  [53, 92, 99, 103, 126, 128, 148, 152, 165, 197]                        10   \n",
       "\n",
       "  UserIndex ItemIndex   timestamp  \n",
       "0      2464      5546  2016-08-23  \n",
       "1      1021      5546  2016-10-02  \n",
       "2       529      5546  2016-11-06  \n",
       "3      1616      5546  2016-09-25  \n",
       "4       590      5546  2018-06-30  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 92, 99, 112, 130, 212]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_eval(df_train.keyVector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ordered',\n",
       " 'lemon',\n",
       " 'mango',\n",
       " 'slush',\n",
       " 'lemon',\n",
       " 'taste',\n",
       " 'strong',\n",
       " 'love',\n",
       " 'lemon',\n",
       " 'love',\n",
       " 'drink',\n",
       " 'informed',\n",
       " 'jelly',\n",
       " 'drink',\n",
       " 'added',\n",
       " 'bonus',\n",
       " 'since',\n",
       " 'love',\n",
       " 'drink',\n",
       " 'topping',\n",
       " 'jelly',\n",
       " 'different',\n",
       " 'compared',\n",
       " 'bubble',\n",
       " 'tea',\n",
       " 'place',\n",
       " 'soft',\n",
       " 'chewy',\n",
       " 'overall',\n",
       " 'enjoyed',\n",
       " 'drink']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# high covariance \n",
    "literal_eval(df_train['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102741"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
