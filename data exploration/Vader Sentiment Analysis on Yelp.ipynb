{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Python3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Litos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "from ast import literal_eval\n",
    "\n",
    "# For Python2 this have to be done\n",
    "# from __future__ import division\n",
    "\n",
    "\n",
    "import gzip\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "#Natural Language Processing Packages\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Counter\n",
    "from collections import Counter\n",
    "\n",
    "#Operation\n",
    "import operator\n",
    "\n",
    "## Download Resources\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.data import find\n",
    "\n",
    "## Machine Learning\n",
    "import sklearn\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_df = pd.read_csv('../../data/yelp/Data.csv', index_col=0,  encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe8 in position 50: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas\\_libs\\parsers.c:14858)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas\\_libs\\parsers.c:17119)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert (pandas\\_libs\\parsers.c:17347)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8 (pandas\\_libs\\parsers.c:23041)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe8 in position 50: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-730f7775d74c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'yelp'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcur_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../data/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'/Data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    980\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1717\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1719\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1720\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas\\_libs\\parsers.c:10862)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas\\_libs\\parsers.c:11138)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas\\_libs\\parsers.c:12175)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data (pandas\\_libs\\parsers.c:14136)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas\\_libs\\parsers.c:14972)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas\\_libs\\parsers.c:17119)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert (pandas\\_libs\\parsers.c:17347)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8 (pandas\\_libs\\parsers.c:23041)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe8 in position 50: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "dataset_name = 'yelp'\n",
    "cur_df = pd.read_csv('../../data/'+ dataset_name +'/Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "\n",
    "It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n",
    "\n",
    "positive sentiment: compound score >= 0.05\n",
    "neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "negative sentiment: compound score <= -0.05\n",
    "The pos, neu, and neg scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation). These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use vader to evaluated sentiment of reviews\n",
    "def evalSentences(sentences, to_df=False, columns=[]):\n",
    "    #Instantiate an instance to access SentimentIntensityAnalyzer class\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    pdlist = []\n",
    "\n",
    "    if to_df:\n",
    "        for sentence in tqdm(sentences):\n",
    "            ss = sid.polarity_scores(sentence)\n",
    "            pdlist.append([sentence]+[ss['compound']])\n",
    "        reviewDf = pd.DataFrame(pdlist)\n",
    "        reviewDf.columns = columns\n",
    "        return reviewDf\n",
    "    \n",
    "#     else:\n",
    "#         for sentence in tqdm(sentences):\n",
    "#             print(sentence)\n",
    "#             ss = sid.polarity_scores(sentence)\n",
    "#             for k in sorted(ss):\n",
    "#                 print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>business_id</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>ghost</th>\n",
       "      <th>img_dsc</th>\n",
       "      <th>img_url</th>\n",
       "      <th>nr</th>\n",
       "      <th>photo_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>...</th>\n",
       "      <th>review_language</th>\n",
       "      <th>review_text</th>\n",
       "      <th>ufc</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_loc</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>Updated</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>bSQGCheX1BwvL67Az1OJlA</td>\n",
       "      <td>90</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>231.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>I had a good experience at this store. I was g...</td>\n",
       "      <td>[10, 1, 10]</td>\n",
       "      <td>q41qqkChL9fRxF43cnkgbw</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>bSQGCheX1BwvL67Az1OJlA</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>I reaaaaaally wish I had read all the stories ...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>tNT5I9-QcljS-7w_doXCXQ</td>\n",
       "      <td>Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>bSQGCheX1BwvL67Az1OJlA</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>[\"Photo of Sleep Country - Toronto, ON, Canada...</td>\n",
       "      <td>['https://s3-media1.fl.yelpcdn.com/bphoto/E_gL...</td>\n",
       "      <td>False</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>Everything was fine until the delivery. I wait...</td>\n",
       "      <td>[3, 1, 2]</td>\n",
       "      <td>9q1Do6TVM27RNHUQAQxPRg</td>\n",
       "      <td>Toronto, Canada</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Z7JV11GXw_RsGr8Ik_Hsag</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>Thought I had bed bugs, but it turned out to b...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>cr-4gAf1_seYWLoO59Lp4w</td>\n",
       "      <td>Old Toronto, Toronto, Canada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2018</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Z7JV11GXw_RsGr8Ik_Hsag</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>When we called, Mr. Akram was willing to come ...</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>CQkTIyhNczRlRI4kIdAzSw</td>\n",
       "      <td>Toronto, Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1             business_id  friend_count  ghost  \\\n",
       "0           0             0  bSQGCheX1BwvL67Az1OJlA            90  False   \n",
       "1           1             1  bSQGCheX1BwvL67Az1OJlA             0  False   \n",
       "2           2             2  bSQGCheX1BwvL67Az1OJlA            52  False   \n",
       "3           3             3  Z7JV11GXw_RsGr8Ik_Hsag             0  False   \n",
       "4           4             4  Z7JV11GXw_RsGr8Ik_Hsag             0  False   \n",
       "\n",
       "                                             img_dsc  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [\"Photo of Sleep Country - Toronto, ON, Canada...   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                             img_url     nr  photo_count  \\\n",
       "0                                                 []  False        231.0   \n",
       "1                                                 []  False          9.0   \n",
       "2  ['https://s3-media1.fl.yelpcdn.com/bphoto/E_gL...  False         27.0   \n",
       "3                                                 []  False          NaN   \n",
       "4                                                 []  False          NaN   \n",
       "\n",
       "   rating ...   review_language  \\\n",
       "0     5.0 ...                en   \n",
       "1     1.0 ...                en   \n",
       "2     1.0 ...                en   \n",
       "3     5.0 ...                en   \n",
       "4     5.0 ...                en   \n",
       "\n",
       "                                         review_text          ufc  \\\n",
       "0  I had a good experience at this store. I was g...  [10, 1, 10]   \n",
       "1  I reaaaaaally wish I had read all the stories ...    [0, 0, 0]   \n",
       "2  Everything was fine until the delivery. I wait...    [3, 1, 2]   \n",
       "3  Thought I had bed bugs, but it turned out to b...    [0, 0, 0]   \n",
       "4  When we called, Mr. Akram was willing to come ...    [1, 0, 0]   \n",
       "\n",
       "                  user_id                      user_loc vote_count Updated  \\\n",
       "0  q41qqkChL9fRxF43cnkgbw                   Houston, TX       11.0   False   \n",
       "1  tNT5I9-QcljS-7w_doXCXQ               Toronto, Canada        0.0   False   \n",
       "2  9q1Do6TVM27RNHUQAQxPRg               Toronto, Canada        3.0   False   \n",
       "3  cr-4gAf1_seYWLoO59Lp4w  Old Toronto, Toronto, Canada        0.0   False   \n",
       "4  CQkTIyhNczRlRI4kIdAzSw               Toronto, Canada        1.0   False   \n",
       "\n",
       "   Year  Month  Day  \n",
       "0  2018      5    9  \n",
       "1  2018      6    3  \n",
       "2  2016     30    7  \n",
       "3  2018     16   12  \n",
       "4  2014      6    8  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df.drop(['Unnamed: 0','Unnamed: 0.1'],axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|███████████████████████████████████████▍                                 | 403536/747183 [08:34<07:15, 788.38it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a28c11d61083>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reviewDF = evalSentences(reviews, to_df=True, columns=['reviewCol','vader']) # Pre-processed Review\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreviewDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevalSentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reviewCol'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'vader'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Original Review\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mreviewDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-eec0b80fa940>\u001b[0m in \u001b[0;36mevalSentences\u001b[1;34m(sentences, to_df, columns)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mto_df\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mpdlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'compound'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mreviewDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36mpolarity_scores\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mvalence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \"\"\"\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[0msentitext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentiText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[1;31m#text, words_and_emoticons, is_cap_diff = self.preprocess(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords_and_emoticons\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_words_and_emoticons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'encode'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|███████████████████████████████████████▍                                 | 403536/747183 [08:50<07:15, 788.38it/s]"
     ]
    }
   ],
   "source": [
    "# reviewDF = evalSentences(reviews, to_df=True, columns=['reviewCol','vader']) # Pre-processed Review\n",
    "reviewDF = evalSentences(reviews, to_df=True, columns=['reviewCol','vader']) # Original Review\n",
    "reviewDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['vader'] = reviewDF['vader']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threshold = 0.05\n",
    "cur_df['Binary_vader_rating'] = (cur_df['vader'] > threshold)*1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(np.where(cur_df['Binary_vader_rating'] == df['Binary'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(np.where(cur_df['Binary_vader_rating'] == df['Binary'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(cur_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiter Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_dataset(df, threshold=3, popularity=True, filter_by_review_count=True, \n",
    "                   user_review_threshold=10, item_review_threshold=10, \n",
    "                   num_user=None, num_item=None, user_ratio=0.25, item_ratio=0.2, postive_negative = 1):\n",
    "    # Binarize rating\n",
    "    df[BINARY_RATING] = (df[RATING] > threshold)*1\n",
    "\n",
    "    # Filter dataset only based on positive ratings\n",
    "    df = df[df[BINARY_RATING] == postive_negative]\n",
    "    \n",
    "    print(\"The total number of users is {}\".format(df[USER_ID].nunique()))\n",
    "    print(\"The total number of items is {} \\n\".format(df[ITEM_ID].nunique()))\n",
    "    \n",
    "\n",
    "    values = df[ITEM_ID].value_counts().keys().tolist()\n",
    "    counts = df[ITEM_ID].value_counts().tolist()\n",
    "    item_df = pd.DataFrame.from_dict({ITEM_ID: values, \"count\": counts})\n",
    "    \n",
    "    values = df[USER_ID].value_counts().keys().tolist()\n",
    "    counts = df[USER_ID].value_counts().tolist()\n",
    "    user_df = pd.DataFrame.from_dict({USER_ID: values, \"count\": counts})\n",
    "    \n",
    "    if popularity:\n",
    "        print(\"Filter dataset by popularity. \\n\")\n",
    "        \n",
    "        if filter_by_review_count:\n",
    "            print(\"Filter dataset by review count. \\n\")\n",
    "            \n",
    "            filtered_item_df = item_df[item_df[\"count\"] >= item_review_threshold]\n",
    "            filtered_item_id = filtered_item_df[ITEM_ID].values\n",
    "            \n",
    "            filtered_user_df = user_df[user_df[\"count\"] >= user_review_threshold]\n",
    "            filtered_user_id = filtered_user_df[USER_ID].values\n",
    "            \n",
    "        else:\n",
    "            print(\"Filter dataset by user and item number. \\n\")\n",
    "            filtered_item_id = item_df[ITEM_ID].unique()[:num_item]\n",
    "            filtered_user_id = user_df[USER_ID].unique()[:num_user]\n",
    "            \n",
    "    else:\n",
    "        print(\"Filter dataset by sampling. \\n\")\n",
    "        np.random.seed(8292)\n",
    "        \n",
    "        filtered_item_id = np.take(item_df[ITEM_ID].unique(), \n",
    "                                   indices=np.random.choice(len(item_df), int(item_ratio*len(item_df))))\n",
    "        filtered_user_id = np.take(user_df[USER_ID].unique(), \n",
    "                                   indices=np.random.choice(len(user_df), int(user_ratio*len(user_df))))\n",
    "        \n",
    "    df = df.loc[(df[USER_ID].isin(filtered_user_id)) & (df[ITEM_ID].isin(filtered_item_id))]\n",
    "#     df = df.loc[(df[ITEM_ID].isin(filtered_item_id))]\n",
    "    print(\"Number of User: {}\".format(df[USER_ID].nunique()))\n",
    "    print(\"Number of Item: {}\".format(df[ITEM_ID].nunique()))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the dataset by popularity then by number of users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df = filter_dataset(df, threshold=3, popularity=True, filter_by_review_count=True, \n",
    "                             user_review_threshold=10, item_review_threshold=10, \n",
    "                             num_user=15000, num_item=1000, user_ratio=None, item_ratio=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df[['Binary','vader']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = filtered_df[USER_ID].value_counts().keys().tolist()\n",
    "counts = filtered_df[USER_ID].value_counts().tolist()\n",
    "user_df = pd.DataFrame.from_dict({USER_ID: values, \"count\": counts})\n",
    "\n",
    "values = filtered_df[ITEM_ID].value_counts().keys().tolist()\n",
    "counts = filtered_df[ITEM_ID].value_counts().tolist()\n",
    "item_df = pd.DataFrame.from_dict({ITEM_ID: values, \"count\": counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df[\"count\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_df[user_df[\"count\"] >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = user_df[user_df[\"count\"] >= 20][USER_ID].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_df = filtered_df.loc[filtered_df[USER_ID].isin(users)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items = pos_df[ITEM_ID].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (len(users))\n",
    "print (len(items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the final DF with reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_df = df.loc[(df[USER_ID].isin(users)) & (df[ITEM_ID].isin(items))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_df[USER_ID].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_df[ITEM_ID].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: You may want to use an NLTK tokenizer instead of a regular expression in the following\n",
    "def dataFrameTransformation(hotelDf, reviewDF, k=500):\n",
    "    reviews = reviewDF['reviewCol'].values\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # Top-k frequent terms\n",
    "    counter = Counter()\n",
    "    for review in reviews:\n",
    "            counter.update([word.lower() \n",
    "                            for word \n",
    "                            in re.findall(r'\\w+', review) \n",
    "                            if word.lower() not in stop and len(word) > 2])\n",
    "    topk = counter.most_common(k)        \n",
    "    \n",
    "    #Find out if a particular review has the word from topk list\n",
    "    freqReview = []\n",
    "    for i in range(len(reviews)):\n",
    "        tempCounter = Counter([word.lower() for word in re.findall(r'\\w+',reviews[i])])\n",
    "        topkinReview = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "        freqReview.append(topkinReview)\n",
    "        \n",
    "        \n",
    "    #Prepare freqReviewDf\n",
    "    freqReviewDf = pd.DataFrame(freqReview)\n",
    "    dfName = []\n",
    "    for c in topk:\n",
    "        dfName.append(c[0])\n",
    "    freqReviewDf.columns = dfName\n",
    "    finalreviewDf = reviewDF.join(freqReviewDf)\n",
    "    finaldf = hotelDf[['hotelName','ratingScore','groundTruth']].join(finalreviewDf)\n",
    "    return topk, finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "#     text = text.replace('.',' ').replace('/',' ').replace('quot;', ' ').replace('amp;', '').replace('-', ' ')\n",
    "    \n",
    "    text = text.replace('.',' ').replace('/t',' ').replace('\\t',' ').replace('/',' ').replace('-',' ')\n",
    "    \n",
    "    # Tokenize\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    text = [w.lower() for w in text]\n",
    "\n",
    "    # Remove Punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text]\n",
    "\n",
    "    # Remove tokens that are not alphabetic\n",
    "    text = [w for w in text if w.isalpha()]\n",
    "\n",
    "    # Remove Stopwords\n",
    "    # Get english stopwords\n",
    "    en_stopwords = set(stopwords.words('english'))\n",
    "    en_stopwords.remove('off')\n",
    "    text = [w for w in text if w not in en_stopwords]\n",
    "    \n",
    "    # Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "    text = \" \" + \" \".join(str(x) for x in text) + \" \"\n",
    "\n",
    "    text = text.replace('whitish', 'white')\n",
    "    text = text.replace('bisquity', ' biscuit ')\n",
    "    text = text.replace('carmel', ' caramel ')\n",
    "    text = text.replace('flower', ' floral ')\n",
    "    text = text.replace('piny', ' pine ')\n",
    "    text = text.replace('off white', 'offwhite')\n",
    "    text = text.replace('goden', 'gold')\n",
    "    text = text.replace('yello', 'yellow')\n",
    "    text = text.replace('reddish', ' red ') \n",
    "    \n",
    "\n",
    "    # Reset to token\n",
    "    text = nltk.word_tokenize(text)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text]\n",
    "    text = [w for w in text if w.isalpha()]\n",
    "#     en_stopwords = set(stopwords.words('english'))\n",
    "    text = [w for w in text if w not in en_stopwords]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(w) for w in text]\n",
    "    \n",
    "#     # Top-k frequent terms\n",
    "#     reviews = reviewDF['reviewCol'].values\n",
    "#     counter = Counter()\n",
    "#     for review in reviews:\n",
    "#         counter.update([word.lower() \n",
    "#                         for word \n",
    "#                         in re.findall(r'\\w+', review) \n",
    "#                         if word.lower() not in stop and len(word) > 2])\n",
    "#     topk = counter.most_common(k)   \n",
    "\n",
    "    return text\n",
    "    \n",
    "cur_df[\"review\"] = cur_df[REVIEW_TEXT].apply(preprocess)\n",
    "cur_df[\"conca_review\"] = cur_df[\"review\"].apply(lambda x: \" \" + \" \".join(str(x) for x in x) + \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index words since no further changes needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "token_list = cur_df[\"review\"].tolist()\n",
    "tokenizer.fit_on_texts(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word_index = pd.DataFrame(list(tokenizer.word_index.items()), columns=['word','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_flatten_list = [item for sublist in token_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_token = len(token_flatten_list)\n",
    "total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist1 = FreqDist(token_flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1.most_common(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_key = ['chinese', 'fast', 'thai', 'bar', 'fry', 'fried', 'dessert', 'dinner', 'lunch', 'soup', \n",
    "                'mexico', 'italian','mexican','vietnamese','buffet','takeout','casual','pub','bakery','indian','classic',\n",
    "               'modern','french','asian','birthday', 'vegetarian', 'downtown', 'bbq','japanese','breakfast','seafood',\n",
    "               'brunch'] \n",
    "\n",
    "food_key = ['taco', 'curry', 'potato', 'crispy', 'shrimp', 'bread', 'chocolate', 'ramen', 'pizza', 'beer', 'sandwich', 'cake',\n",
    "            'sushi', 'egg', 'fish', 'coffee', 'burger', 'cheese', 'salad', 'pork', 'beef', 'tea', 'noodle',\n",
    "           'meat', 'chicken', 'dim sum', 'cocktail', 'ice cream','squid','tempura','tapioca','donut','olive',\n",
    "            'espresso','octopus','croissant','banana','cookie','honey','cone','scallop','congee',\n",
    "           'skewer','miso','lettuce','pop','strawberry','apple','avocado','juice','booth','calamari','kimchi','patty',\n",
    "           'sesame','tart','four','crepe','tuna','wrap','lemon','vegan','coconut','corn','poutine','toast','belly','bubble',\n",
    "           'oyster','cocktail', 'cheesecake', 'fruit', 'sausage','latte','matcha','pancake','duck','tofu','sashimi',\n",
    "           'lamb','mango','bacon','tomato','lobster','wine','rib','waffle','bun','wing','dumpling','bean','steak','salmon',\n",
    "           'pasta','milk','fried chicken','milk tea','green tea','bubble tea','pork belly','spring roll','fried rice',\n",
    "            'pork bone soup']\n",
    "seasoning=['sugar','oil','soy','leaf','spice','butter','ginger','pepper','peanut','garlic']\n",
    "infrastruture_key = [ 'parking', 'store','shopping','nail','theatre','movie','washroom',\n",
    "                    'window','station','chair', 'markham','plaza','market', 'mall']\n",
    "# or we call this comment\n",
    "service_key = ['quick', 'clean', 'busy', 'fresh', 'friendly','convenient','refill','soggy','greeted','bright','crowded','overpriced',\n",
    "              'cheaper','immediately','dog','quiet','efficient','spacious','pleasant','fair','complaint','disappointing','fancy',\n",
    "             'comfortable', 'dark','cozy','helpful','tax','nicely','honestly', 'pricey','yummy','music','chip','attentive',\n",
    "              'reasonable','wait']\n",
    "\n",
    "taste_key = ['traditional', 'spicy','flavorful','fluffy','smooth','frozen','sweetness','mayo','gravy','healthy','rare',\n",
    "            'refreshing','crunchy','chili','crust','stick','steamed','greasy','dip','gelato','salt','stuffed','topped','smoked',\n",
    "            'roasted','seasoned','chewy','pot','solid','sour', 'baked', 'juicy','creamy','deep fried']\n",
    "\n",
    "bigram_key = ['ice cream', 'come back', 'go back', 'fried chicken', 'deep fried', 'milk tea', 'green tea', 'bubble tea',\n",
    "             'pork belly', 'pad thai', 'spring roll']\n",
    "from_pmi = ['lactose intolerant', 'dietary restriction', 'gong cha', 'general tao', 'wild boar', 'financial district', \n",
    "           'pale ale', 'public transit', 'balsamic vinegar', 'uber eats', 'alcoholic beverage', 'grand opening', 'north york',\n",
    "           'english muffin', 'accept debit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = category_key + food_key + infrastruture_key + service_key + taste_key + from_pmi\n",
    "len(category_key) + len(food_key) + len(infrastruture_key) + len(service_key) + len(taste_key) + len(from_pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ItemIndex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2392\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2393\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2394\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5239)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5085)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20405)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ItemIndex'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-12e861cc9273>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcur_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ItemIndex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2060\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2062\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2067\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2069\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1532\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1534\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1535\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1536\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2393\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2395\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5239)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5085)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20405)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ItemIndex'"
     ]
    }
   ],
   "source": [
    "cur_df['ItemIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Vader to find n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--DaPTJW3-tB1vP-PfdTEg</td>\n",
       "      <td>3.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0CCHBui57tZ_1y_14X-5Q</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0CTrPQNiSyClxhdO4HSDQ</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0DET7VdEQOJVJ_v6klEug</td>\n",
       "      <td>3.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0FA-Qdi3SPYIoJz9UQw-A</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0M3o2uWBnQZwd3hmfEwuw</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0NrB58jqKqJfuUCDupcsw</td>\n",
       "      <td>3.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-113IAvSQ4Nn_Jk7OrrPkg</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1LvG5O5lzfqUeti0Z3L5Q</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-25X5v1q3WU6s-craJSvTw</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2lSbNMhXKYu7URjEE59qA</td>\n",
       "      <td>2.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-2uYjztVuVZpkTNAC38zdg</td>\n",
       "      <td>3.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-3Yo-5oNDhJ3SFgCGfOyuw</td>\n",
       "      <td>2.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-3iRcdd9CPD5mqQj8q5jXQ</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-4B9nqmqqWUxzbQarf40uQ</td>\n",
       "      <td>3.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-4eA7Um2eiOKsGLmcXNx_w</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-4uT6QFVaRtc2Zmxvtd9Vg</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-5IeOTDklq3CUyEoNr9Ukg</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-5JxNFUpJIFEWTzp4OUcIw</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-5Np0EVLRVqjKlRwf_GShA</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-5TBEK1ddM41v_gqGm5oyw</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-5jeacQHetWxqLmOJCUsKA</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-66jkKvMTUu31lDvFfzLWA</td>\n",
       "      <td>3.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-6VXNsPHyYtHV71JBAdsIA</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-6nS5flLpZcJOB2F3JfbNA</td>\n",
       "      <td>3.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-6pKKkWhuxoQE3Oea_6_cA</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-6sE5v4OrVGmWxdIrbs-RQ</td>\n",
       "      <td>3.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-7BCZH437U5FjmNJ26llkg</td>\n",
       "      <td>4.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-7EwIdxcRC5McO35DVfeSQ</td>\n",
       "      <td>3.704918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-7F3BVqvfVAKeqUj_9WPBA</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10252</th>\n",
       "      <td>znu3EN7tDFIXmiTwKvX1Ow</td>\n",
       "      <td>3.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10253</th>\n",
       "      <td>zoIEe7uhsQT2EyiFLIfhrw</td>\n",
       "      <td>3.523077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10254</th>\n",
       "      <td>zo_z6lMRcwHqQF_Z1VZLpg</td>\n",
       "      <td>2.853659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10255</th>\n",
       "      <td>zoomKr4OD_xd-hTSeJzRkQ</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10256</th>\n",
       "      <td>zqRmOmO38SLmv3fmeoK3hA</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10257</th>\n",
       "      <td>zqyDcvVn-ygbXuB86i3y-Q</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10258</th>\n",
       "      <td>zrOnJaTxgUDTyURmmVj-9Q</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10259</th>\n",
       "      <td>zrQYQHawhaolF7muiU6Ckw</td>\n",
       "      <td>3.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10260</th>\n",
       "      <td>zrYpLdnGKA_EmOhgRCy_vg</td>\n",
       "      <td>3.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10261</th>\n",
       "      <td>zravs6cwUxUWsxf1z4P3_Q</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10262</th>\n",
       "      <td>zrcs31Gt6HuWX_RCWW-mCQ</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10263</th>\n",
       "      <td>zs-JJ2m_4gHiiqzYJbhY0A</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10264</th>\n",
       "      <td>zs710LuQ8y10YsQHIjxjyw</td>\n",
       "      <td>3.673469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10265</th>\n",
       "      <td>zsDR8jQrBFonhXPYaQucEQ</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10266</th>\n",
       "      <td>ztX_39BrHP76S8Vv4Y9LIw</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10267</th>\n",
       "      <td>ztzjD1Zs5tfMZTYKXkmL9Q</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10268</th>\n",
       "      <td>zu5dqKqPjJCycS5k-92Alg</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10269</th>\n",
       "      <td>zu8Wj0gkbdnia7Dt4grMkQ</td>\n",
       "      <td>3.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10270</th>\n",
       "      <td>zuCgnSkJa3aFB5s1-8ew4g</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10271</th>\n",
       "      <td>zvOy4znhODeXleiublTsRg</td>\n",
       "      <td>3.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10272</th>\n",
       "      <td>zvezLicNkf8tTqZyYhP83w</td>\n",
       "      <td>3.107143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10273</th>\n",
       "      <td>zvm5FZ2mzoztfacwJKrtFw</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10274</th>\n",
       "      <td>zvsTmyfFR0WAd1_6BzqiMw</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10275</th>\n",
       "      <td>zvxUINlAS0GF9tHC-E7Q7A</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10276</th>\n",
       "      <td>zw5Trj0Xoea-_XMoZVzevQ</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10277</th>\n",
       "      <td>zwOuVcnMnc9zfev4_1M4jQ</td>\n",
       "      <td>4.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10278</th>\n",
       "      <td>zwhgnF9ICofzzIAIuWtbkQ</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10279</th>\n",
       "      <td>zxNzSF2bGHmkFMrVzud3rQ</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10280</th>\n",
       "      <td>zy5UZvaG4iOLSbxuophtTQ</td>\n",
       "      <td>3.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10281</th>\n",
       "      <td>zzf3RkMI1Y2E1QaZqeU8yA</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10282 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  business_id    rating\n",
       "0      --DaPTJW3-tB1vP-PfdTEg  3.818182\n",
       "1      -0CCHBui57tZ_1y_14X-5Q  4.333333\n",
       "2      -0CTrPQNiSyClxhdO4HSDQ  2.777778\n",
       "3      -0DET7VdEQOJVJ_v6klEug  3.421053\n",
       "4      -0FA-Qdi3SPYIoJz9UQw-A  4.333333\n",
       "5      -0M3o2uWBnQZwd3hmfEwuw  5.000000\n",
       "6      -0NrB58jqKqJfuUCDupcsw  3.581395\n",
       "7      -113IAvSQ4Nn_Jk7OrrPkg  4.666667\n",
       "8      -1LvG5O5lzfqUeti0Z3L5Q  3.600000\n",
       "9      -25X5v1q3WU6s-craJSvTw  3.000000\n",
       "10     -2lSbNMhXKYu7URjEE59qA  2.875000\n",
       "11     -2uYjztVuVZpkTNAC38zdg  3.285714\n",
       "12     -3Yo-5oNDhJ3SFgCGfOyuw  2.800000\n",
       "13     -3iRcdd9CPD5mqQj8q5jXQ  5.000000\n",
       "14     -4B9nqmqqWUxzbQarf40uQ  3.454545\n",
       "15     -4eA7Um2eiOKsGLmcXNx_w  3.333333\n",
       "16     -4uT6QFVaRtc2Zmxvtd9Vg  4.666667\n",
       "17     -5IeOTDklq3CUyEoNr9Ukg  5.000000\n",
       "18     -5JxNFUpJIFEWTzp4OUcIw  5.000000\n",
       "19     -5Np0EVLRVqjKlRwf_GShA  5.000000\n",
       "20     -5TBEK1ddM41v_gqGm5oyw  5.000000\n",
       "21     -5jeacQHetWxqLmOJCUsKA  4.000000\n",
       "22     -66jkKvMTUu31lDvFfzLWA  3.714286\n",
       "23     -6VXNsPHyYtHV71JBAdsIA  4.000000\n",
       "24     -6nS5flLpZcJOB2F3JfbNA  3.250000\n",
       "25     -6pKKkWhuxoQE3Oea_6_cA  4.250000\n",
       "26     -6sE5v4OrVGmWxdIrbs-RQ  3.888889\n",
       "27     -7BCZH437U5FjmNJ26llkg  4.370370\n",
       "28     -7EwIdxcRC5McO35DVfeSQ  3.704918\n",
       "29     -7F3BVqvfVAKeqUj_9WPBA  5.000000\n",
       "...                       ...       ...\n",
       "10252  znu3EN7tDFIXmiTwKvX1Ow  3.727273\n",
       "10253  zoIEe7uhsQT2EyiFLIfhrw  3.523077\n",
       "10254  zo_z6lMRcwHqQF_Z1VZLpg  2.853659\n",
       "10255  zoomKr4OD_xd-hTSeJzRkQ  5.000000\n",
       "10256  zqRmOmO38SLmv3fmeoK3hA  3.000000\n",
       "10257  zqyDcvVn-ygbXuB86i3y-Q  3.666667\n",
       "10258  zrOnJaTxgUDTyURmmVj-9Q  3.000000\n",
       "10259  zrQYQHawhaolF7muiU6Ckw  3.384615\n",
       "10260  zrYpLdnGKA_EmOhgRCy_vg  3.933333\n",
       "10261  zravs6cwUxUWsxf1z4P3_Q  3.500000\n",
       "10262  zrcs31Gt6HuWX_RCWW-mCQ  3.600000\n",
       "10263  zs-JJ2m_4gHiiqzYJbhY0A  4.000000\n",
       "10264  zs710LuQ8y10YsQHIjxjyw  3.673469\n",
       "10265  zsDR8jQrBFonhXPYaQucEQ  4.333333\n",
       "10266  ztX_39BrHP76S8Vv4Y9LIw  4.142857\n",
       "10267  ztzjD1Zs5tfMZTYKXkmL9Q  4.000000\n",
       "10268  zu5dqKqPjJCycS5k-92Alg  3.500000\n",
       "10269  zu8Wj0gkbdnia7Dt4grMkQ  3.833333\n",
       "10270  zuCgnSkJa3aFB5s1-8ew4g  3.666667\n",
       "10271  zvOy4znhODeXleiublTsRg  3.250000\n",
       "10272  zvezLicNkf8tTqZyYhP83w  3.107143\n",
       "10273  zvm5FZ2mzoztfacwJKrtFw  3.750000\n",
       "10274  zvsTmyfFR0WAd1_6BzqiMw  4.000000\n",
       "10275  zvxUINlAS0GF9tHC-E7Q7A  5.000000\n",
       "10276  zw5Trj0Xoea-_XMoZVzevQ  4.000000\n",
       "10277  zwOuVcnMnc9zfev4_1M4jQ  4.285714\n",
       "10278  zwhgnF9ICofzzIAIuWtbkQ  5.000000\n",
       "10279  zxNzSF2bGHmkFMrVzud3rQ  5.000000\n",
       "10280  zy5UZvaG4iOLSbxuophtTQ  3.230769\n",
       "10281  zzf3RkMI1Y2E1QaZqeU8yA  4.666667\n",
       "\n",
       "[10282 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def average_vader_ground_truth(df):\n",
    "    return df.groupby('business_id', as_index=False).agg({'rating':'mean'})\n",
    "    \n",
    "average_vader_ground_truthDF = average_vader_ground_truth(cur_df)\n",
    "average_vader_ground_truthDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Top K words comparison between ground truth reviews and vader predicted reviews\n",
    "def getTopK(df, k, label_value, label_column='Binary', operation=operator.eq, value_column='conca_review'):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    #Add possible Stop Words for Hotel Reviews\n",
    "    stop.add('restaurant')\n",
    "    stop.add('restaurants')\n",
    "    stop.add('eat')\n",
    "#     stop.add('hotel')\n",
    "#     stop.add('room')\n",
    "#     stop.add('rooms')\n",
    "#     stop.add('stay')\n",
    "#     stop.add('staff')\n",
    "    counter = Counter()\n",
    "    for review in df.loc[operation(df[label_column],label_value)][value_column]:\n",
    "        counter.update([word.lower() \n",
    "                        for word \n",
    "                        in re.findall(r'\\w+', review) \n",
    "                        if word.lower() not in stop and len(word) > 2])\n",
    "    topk = counter.most_common(k)\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Analyze topK words between positive reviews and negative reviews\n",
    "def find_common_words(words_list1, words_list2):\n",
    "    list1 = [word[0] for word in words_list1]\n",
    "    list2 = [word[0] for word in words_list2]\n",
    "    return set(list1) & set(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = PerceptronTagger()\n",
    "pos_tag = tagger.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This grammar is described in the paper by S. N. Kim,\n",
    "# T. Baldwin, and M.-Y. Kan.\n",
    "# Evaluating n-gram based evaluation metrics for automatic\n",
    "# keyphrase extraction.\n",
    "# Technical report, University of Melbourne, Melbourne 2010.\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create phrase tree\n",
    "chunker = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Noun Phrase Extraction Support Functions\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "# generator, generate leaves one by one\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP' or t.label()=='JJ' or t.label()=='RB'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "# stemming, lematizing, lower case... \n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = stemmer.stem(word)\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "        \n",
    "# stop-words and length control\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords.words('english'))\n",
    "    return accepted\n",
    "        \n",
    "# generator, create item once a time\n",
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "        term = [normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        # Phrase only\n",
    "        if len(term)>1:\n",
    "            yield term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten phrase lists to get tokens for analysis\n",
    "def flatten(npTokenList):\n",
    "    finalList =[]\n",
    "    for phrase in npTokenList:\n",
    "        token = ''\n",
    "        for word in phrase:\n",
    "            token += word + ' '\n",
    "        finalList.append(token.rstrip())\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['ordered', 'lemon', 'mango', 'slush', 'lemon', 'taste', 'strong', 'love', 'lemon', 'love', 'drink', 'informed', 'jelly', 'drink', 'added', 'bonus', 'since', 'love', 'drink', 'topping', 'jelly', 'different', 'compared', 'bubble', 'tea', 'place', 'soft', 'chewy', 'overall', 'enjoyed', 'drink']\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_df[\"review\"].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_list = [literal_eval(review) for review in cur_df['review'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# token_list = literal_eval(cur_df[\"review\"]).tolist()\n",
    "tokenizer.fit_on_texts(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "tokens = itertools.chain.from_iterable(token_list)\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "bigramFinder.apply_freq_filter(100)\n",
    "\n",
    "bigram_freq = bigramFinder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), \n",
    "                               columns=['ngram','freq']).sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3537</th>\n",
       "      <td>(ca, nt)</td>\n",
       "      <td>16379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>(ice, cream)</td>\n",
       "      <td>14608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12034</th>\n",
       "      <td>(come, back)</td>\n",
       "      <td>13883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>(pretty, good)</td>\n",
       "      <td>9442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10735</th>\n",
       "      <td>(really, good)</td>\n",
       "      <td>8755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2507</th>\n",
       "      <td>(first, time)</td>\n",
       "      <td>8053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>(would, nt)</td>\n",
       "      <td>7714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9419</th>\n",
       "      <td>(go, back)</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6129</th>\n",
       "      <td>(next, time)</td>\n",
       "      <td>6864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12038</th>\n",
       "      <td>(could, nt)</td>\n",
       "      <td>6665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>(wo, nt)</td>\n",
       "      <td>6520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5776</th>\n",
       "      <td>(nt, know)</td>\n",
       "      <td>5842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>(feel, like)</td>\n",
       "      <td>5751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3791</th>\n",
       "      <td>(fried, chicken)</td>\n",
       "      <td>5713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7220</th>\n",
       "      <td>(nt, really)</td>\n",
       "      <td>5619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>(dim, sum)</td>\n",
       "      <td>5420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7649</th>\n",
       "      <td>(nt, get)</td>\n",
       "      <td>5236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10277</th>\n",
       "      <td>(great, place)</td>\n",
       "      <td>5100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3862</th>\n",
       "      <td>(even, though)</td>\n",
       "      <td>5098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11100</th>\n",
       "      <td>(would, definitely)</td>\n",
       "      <td>5026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>(nt, even)</td>\n",
       "      <td>4914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>(back, try)</td>\n",
       "      <td>4716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4791</th>\n",
       "      <td>(nt, think)</td>\n",
       "      <td>4707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3231</th>\n",
       "      <td>(food, good)</td>\n",
       "      <td>4638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397</th>\n",
       "      <td>(coming, back)</td>\n",
       "      <td>4487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8704</th>\n",
       "      <td>(highly, recommend)</td>\n",
       "      <td>4474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>(nt, like)</td>\n",
       "      <td>4381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>(make, sure)</td>\n",
       "      <td>4242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9511</th>\n",
       "      <td>(would, recommend)</td>\n",
       "      <td>4175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>(milk, tea)</td>\n",
       "      <td>4068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4860</th>\n",
       "      <td>(gra, prow)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>(roasted, marshmallow)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8664</th>\n",
       "      <td>(happy, experience)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>(first, canadian)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7083</th>\n",
       "      <td>(people, definitely)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>(wing, night)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>(come, always)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>(huge, menu)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7313</th>\n",
       "      <td>(back, never)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11945</th>\n",
       "      <td>(cake, tasted)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8432</th>\n",
       "      <td>(hot, like)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9019</th>\n",
       "      <td>(visiting, place)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443</th>\n",
       "      <td>(dessert, overall)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7290</th>\n",
       "      <td>(location, however)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>(order, pretty)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9636</th>\n",
       "      <td>(dining, option)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>(soft, bun)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>(best, dessert)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10892</th>\n",
       "      <td>(may, wait)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9657</th>\n",
       "      <td>(first, arrived)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3693</th>\n",
       "      <td>(specialty, drink)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7136</th>\n",
       "      <td>(andrew, walduck)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10685</th>\n",
       "      <td>(quick, fix)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4011</th>\n",
       "      <td>(even, people)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>(worth, service)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>(friend, brought)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>(finish, whole)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7122</th>\n",
       "      <td>(special, chicken)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7095</th>\n",
       "      <td>(chicken, club)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4121</th>\n",
       "      <td>(graham, cracker)</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12126 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ngram   freq\n",
       "3537                 (ca, nt)  16379\n",
       "1440             (ice, cream)  14608\n",
       "12034            (come, back)  13883\n",
       "1392           (pretty, good)   9442\n",
       "10735          (really, good)   8755\n",
       "2507            (first, time)   8053\n",
       "9129              (would, nt)   7714\n",
       "9419               (go, back)   7005\n",
       "6129             (next, time)   6864\n",
       "12038             (could, nt)   6665\n",
       "5933                 (wo, nt)   6520\n",
       "5776               (nt, know)   5842\n",
       "4364             (feel, like)   5751\n",
       "3791         (fried, chicken)   5713\n",
       "7220             (nt, really)   5619\n",
       "7566               (dim, sum)   5420\n",
       "7649                (nt, get)   5236\n",
       "10277          (great, place)   5100\n",
       "3862           (even, though)   5098\n",
       "11100     (would, definitely)   5026\n",
       "6833               (nt, even)   4914\n",
       "1736              (back, try)   4716\n",
       "4791              (nt, think)   4707\n",
       "3231             (food, good)   4638\n",
       "3397           (coming, back)   4487\n",
       "8704      (highly, recommend)   4474\n",
       "1771               (nt, like)   4381\n",
       "3851             (make, sure)   4242\n",
       "9511       (would, recommend)   4175\n",
       "1761              (milk, tea)   4068\n",
       "...                       ...    ...\n",
       "4860              (gra, prow)    100\n",
       "2177   (roasted, marshmallow)    100\n",
       "8664      (happy, experience)    100\n",
       "2191        (first, canadian)    100\n",
       "7083     (people, definitely)    100\n",
       "3663            (wing, night)    100\n",
       "7351           (come, always)    100\n",
       "1715             (huge, menu)    100\n",
       "7313            (back, never)    100\n",
       "11945          (cake, tasted)    100\n",
       "8432              (hot, like)    100\n",
       "9019        (visiting, place)    100\n",
       "8443       (dessert, overall)    100\n",
       "7290      (location, however)    100\n",
       "1565          (order, pretty)    100\n",
       "9636         (dining, option)    100\n",
       "1604              (soft, bun)    100\n",
       "1610          (best, dessert)    100\n",
       "10892             (may, wait)    100\n",
       "9657         (first, arrived)    100\n",
       "3693       (specialty, drink)    100\n",
       "7136        (andrew, walduck)    100\n",
       "10685            (quick, fix)    100\n",
       "4011           (even, people)    100\n",
       "498          (worth, service)    100\n",
       "9676        (friend, brought)    100\n",
       "8505          (finish, whole)    100\n",
       "7122       (special, chicken)    100\n",
       "7095          (chicken, club)    100\n",
       "4121        (graham, cracker)    100\n",
       "\n",
       "[12126 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramFreqTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Revise the previous dataframe transform function...\n",
    "def newDataFrameTransformation(hotelDf, reviewDF, k=50):\n",
    "    reviews = reviewDF['conca_review'].values\n",
    "    \n",
    "    # Top-k frequent terms\n",
    "    counter = Counter()\n",
    "    for review in reviews:\n",
    "            counter.update(flatten([word\n",
    "                            for word \n",
    "                            in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+', review)))) \n",
    "                            ]))\n",
    "    topk = counter.most_common(k)        \n",
    "    \n",
    "    #Find out if a particular review has the word from topk list\n",
    "    freqReview = []\n",
    "    for i in range(len(reviews)):\n",
    "        tempCounter = Counter(flatten([word \n",
    "                                       for word \n",
    "                                       in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+',reviews[i]))))]))\n",
    "        topkinReview = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "        freqReview.append(topkinReview)\n",
    "  \n",
    "    #Prepare freqReviewDf\n",
    "    freqReviewDf = pd.DataFrame(freqReview)\n",
    "    dfName = []\n",
    "    for c in topk:\n",
    "        dfName.append(c[0])\n",
    "    freqReviewDf.columns = dfName\n",
    "    finalreviewDf = reviewDF.join(freqReviewDf)\n",
    "    finaldf = hotelDf[['hotelName','ratingScore','groundTruth']].join(finalreviewDf)\n",
    "    return topk, finaldf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itemAnalysisDf = cur_df[['conca_review','Binary','vader']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topkGroundPos = getTopK(df=itemAnalysisDf, k=50, label_value=1)\n",
    "topkGroundNeg = getTopK(df=itemAnalysisDf, k=50, label_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('place', 83866),\n",
       " ('good', 77157),\n",
       " ('food', 66800),\n",
       " ('like', 53940),\n",
       " ('great', 52267),\n",
       " ('one', 50918),\n",
       " ('time', 50411),\n",
       " ('really', 45506),\n",
       " ('get', 45234),\n",
       " ('also', 44018),\n",
       " ('would', 40271),\n",
       " ('service', 37637),\n",
       " ('back', 33562),\n",
       " ('come', 32407),\n",
       " ('well', 30684),\n",
       " ('chicken', 30507),\n",
       " ('definitely', 29361),\n",
       " ('try', 28686),\n",
       " ('dish', 28437),\n",
       " ('nice', 28078),\n",
       " ('delicious', 27988),\n",
       " ('came', 26468),\n",
       " ('got', 25912),\n",
       " ('ordered', 25564),\n",
       " ('menu', 25349),\n",
       " ('love', 24983),\n",
       " ('order', 24239),\n",
       " ('pretty', 22580),\n",
       " ('sauce', 22446),\n",
       " ('make', 22321),\n",
       " ('bit', 22231),\n",
       " ('price', 22043),\n",
       " ('little', 21840),\n",
       " ('table', 21452),\n",
       " ('always', 21401),\n",
       " ('friend', 21312),\n",
       " ('best', 21257),\n",
       " ('lot', 20930),\n",
       " ('even', 19921),\n",
       " ('much', 19859),\n",
       " ('fresh', 19521),\n",
       " ('drink', 19031),\n",
       " ('friendly', 18885),\n",
       " ('people', 18803),\n",
       " ('flavour', 18777),\n",
       " ('small', 18229),\n",
       " ('first', 18046),\n",
       " ('made', 17851),\n",
       " ('rice', 17790),\n",
       " ('thing', 17627)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topkGroundPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 47995),\n",
       " ('place', 46997),\n",
       " ('good', 45685),\n",
       " ('like', 37767),\n",
       " ('would', 31714),\n",
       " ('one', 30177),\n",
       " ('time', 29731),\n",
       " ('really', 27328),\n",
       " ('service', 26404),\n",
       " ('get', 26165),\n",
       " ('came', 20525),\n",
       " ('also', 20205),\n",
       " ('ordered', 19882),\n",
       " ('back', 19742),\n",
       " ('chicken', 19201),\n",
       " ('come', 18837),\n",
       " ('order', 17570),\n",
       " ('dish', 17319),\n",
       " ('got', 17307),\n",
       " ('table', 17243),\n",
       " ('pretty', 16619),\n",
       " ('great', 15940),\n",
       " ('much', 15562),\n",
       " ('nice', 15231),\n",
       " ('price', 15172),\n",
       " ('menu', 15086),\n",
       " ('bit', 15049),\n",
       " ('even', 14570),\n",
       " ('could', 13868),\n",
       " ('better', 13689),\n",
       " ('try', 13553),\n",
       " ('friend', 13312),\n",
       " ('sauce', 13103),\n",
       " ('taste', 13051),\n",
       " ('well', 13030),\n",
       " ('drink', 13002),\n",
       " ('people', 12718),\n",
       " ('little', 12521),\n",
       " ('think', 12270),\n",
       " ('lot', 12046),\n",
       " ('quite', 11840),\n",
       " ('thing', 11723),\n",
       " ('rice', 11321),\n",
       " ('small', 11266),\n",
       " ('make', 11209),\n",
       " ('first', 11178),\n",
       " ('though', 10883),\n",
       " ('noodle', 10319),\n",
       " ('still', 10177),\n",
       " ('soup', 10143)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topkGroundNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topK_most_frequent_non_stopwords = find_common_words(topkGroundPos, topkGroundNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following words co-exist in top-50 most frequent non-stopwords in both positive and negative reviews: {'friend', 'price', 'order', 'got', 'time', 'much', 'really', 'would', 'good', 'chicken', 'like', 'make', 'well', 'also', 'thing', 'service', 'first', 'get', 'drink', 'food', 'sauce', 'small', 'dish', 'place', 'table', 'came', 'pretty', 'nice', 'try', 'great', 'come', 'even', 'bit', 'ordered', 'one', 'people', 'little', 'back', 'lot', 'rice', 'menu'}\n"
     ]
    }
   ],
   "source": [
    "print(\"The following words co-exist in top-50 most frequent non-stopwords in both positive and negative reviews: {}\".format(topK_most_frequent_non_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topK_noun_phrases(df, label_value):\n",
    "    noun_phrases_df = df.loc[df['Binary'] == label_value]\n",
    "    noun_phrases_df = noun_phrases_df[noun_phrases_df.columns[5:]].reset_index(drop=True)\n",
    "    noun_phrases_df = noun_phrases_df.append(noun_phrases_df.agg(['sum']))\n",
    "    noun_phrases_df = noun_phrases_df.T.sort_values('sum', ascending=False).T\n",
    "    return noun_phrases_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK_noun_phrases_positive = topK_noun_phrases(cur_df, label_value=1)\n",
    "topK_noun_phrases_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topK_noun_phrases_negative = topK_noun_phrases(cur_df, label_value=0)\n",
    "topK_noun_phrases_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The following words co-exist in top-50 most frequent noun phrases in both positive and negative reviews: {}'.format(set(topK_noun_phrases_positive) & set(topK_noun_phrases_negative)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Top K mutual information terms from the dataframe\n",
    "def getMI(topk, df, label_column='groundTruth'):\n",
    "    miScore = []\n",
    "    for word in topk:\n",
    "        miScore.append([word[0]]+[metrics.mutual_info_score(df[label_column], df[word[0]])])\n",
    "    miScoredf = pd.DataFrame(miScore).sort_values(1,ascending=0)\n",
    "    miScoredf.columns = ['Word','MI Score']\n",
    "    return miScoredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "miScoredf = getMI(topk, finaldf)\n",
    "miScoredf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "miScoredfNounPhrase = getMI(topk_phrase, finaldf_phrase)\n",
    "miScoredfNounPhrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Substitute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to filter for ADJ/NN bigrams\n",
    "def filter_type(ngram):\n",
    "    if '-pron-' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word.isspace():\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS')\n",
    "    ins = ('IN','TO')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    \n",
    "    if len(tags) == 2:\n",
    "        if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif len(tags) == 3:\n",
    "        if tags[0][1] in acceptable_types and tags[1][1] in ins and tags[2][1] in second_type:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if tags[0][1] in acceptable_types and tags[1][1] in ins and tags[2][1] in acceptable_types and tags[3][1] in second_type:\n",
    "            return True\n",
    "        else:\n",
    "            return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "token_list = np.array([literal_eval(cur_df[\"review\"][i]) for i in range(len(cur_df[\"review\"]))])\n",
    "# token_list = literal_eval(cur_df[\"review\"]).tolist()\n",
    "tokenizer.fit_on_texts(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_word_index = pd.DataFrame(list(tokenizer.word_index.items()), columns=['word','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_flatten_list = [item for sublist in token_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13248601"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_token = len(token_flatten_list)\n",
    "total_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist1 = FreqDist(token_flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "tokens = itertools.chain.from_iterable(token_list)\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "bigramFinder.apply_freq_filter(100)\n",
    "\n",
    "bigram_freq = bigramFinder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), \n",
    "                               columns=['ngram','freq']).sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigramFreqTable[bigramFreqTable[\"ngram\"].str.contains('dim', regex=False)][:250]['ngram'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norn Phrase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Happy Lemon has become my new favourite place for a sweet and refreshing drink!This location inside of Pacific Mall is great for grabbing a drink while browsing the many shops. If you prefer to sit, there are a couple of seats and two small tables.Even though there was only one person working when I went, she was efficient in taking our orders and making our drinks. Oreo Milk Tea - perfect combination of Oreo + Milk Tea taste. Neither is diluted and the two compliment each other perfectly. I thought it would taste weird but I actually enjoyed it. It reminds me of The Alley's Garden Milk Tea but this tastes much better!Milk Tea with Pudding, Bubble and Lychee Jelly - I'm a huge fan of original milk tea with pudding. It's my go to order regardless of which shop I go to. The lychee jelly intrigued me so I decided to try this drink instead. I personally wouldn't order this again. The lychee jelly is good in that it has quite a sweet and strong lychee taste. However, I think this diluted the taste of the milk tea so I wouldn't recommend it. The milk tea taste was stronger in the Oreo Milk Tea. The lychee jelly would taste excellent in one of the fruitier drinks.\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_df['review_text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' happy lemon become new favourite place sweet refreshing drink location inside pacific mall great grabbing drink browsing many shop prefer sit couple seat two small table even though one person working went efficient taking order making drink oreo milk tea perfect combination oreo milk tea taste neither diluted two compliment perfectly thought would taste weird actually enjoyed reminds alley garden milk tea taste much better milk tea pudding bubble lychee jelly huge fan original milk tea pudding go order regardless shop go lychee jelly intrigued decided try drink instead personally would nt order lychee jelly good quite sweet strong lychee taste however think diluted taste milk tea would nt recommend milk tea taste stronger oreo milk tea lychee jelly would taste excellent one fruitier drink '"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = cur_df['conca_review'][4]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"\"\"The Buddha, the Godhead, resides quite as comfortably in the circuits of a digital\n",
    "computer or the gears of a cycle transmission as he does at the top of a mountain\n",
    "or in the petals of a flower. To think otherwise is to demean the Buddha...which is\n",
    "to demean oneself.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' happy lemon become new favourite place sweet refreshing drink location inside pacific mall great grabbing drink browsing many shop prefer sit couple seat two small table even though one person working went efficient taking order making drink oreo milk tea perfect combination oreo milk tea taste neither diluted two compliment perfectly thought would taste weird actually enjoyed reminds alley garden milk tea taste much better milk tea pudding bubble lychee jelly huge fan original milk tea pudding go order regardless shop go lychee jelly intrigued decided try drink instead personally would nt order lychee jelly good quite sweet strong lychee taste however think diluted taste milk tea would nt recommend milk tea taste stronger oreo milk tea lychee jelly would taste excellent one fruitier drink '"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagger = PerceptronTagger()\n",
    "# Part of Speech Tagging\n",
    "# Google: https://en.wikipedia.org/wiki/Part-of-speech_tagging\n",
    "pos_tag = tagger.tag\n",
    "taggedToks = pos_tag(re.findall(r'\\w+', text))\n",
    "taggedToks\n",
    "# This grammar is described in the paper by S. N. Kim,\n",
    "# T. Baldwin, and M.-Y. Kan.\n",
    "# Evaluating n-gram based evaluation metrics for automatic\n",
    "# keyphrase extraction.\n",
    "# Technical report, University of Melbourne, Melbourne 2010.\n",
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create phrase tree\n",
    "chunker = nltk.RegexpParser(grammar)\n",
    "tree= chunker.parse(taggedToks)\n",
    "# Noun Phrase Extraction Support Functions\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "# generator, generate leaves one by one\n",
    "def leaves(tree):\n",
    "    \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP' or t.label()=='JJ' or t.label()=='RB'):\n",
    "        yield subtree.leaves()\n",
    "\n",
    "# stemming, lematizing, lower case... \n",
    "def normalise(word):\n",
    "    \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n",
    "    word = word.lower()\n",
    "    word = stemmer.stem(word)\n",
    "    word = lemmatizer.lemmatize(word)\n",
    "    return word\n",
    "\n",
    "# stop-words and length control\n",
    "def acceptable_word(word):\n",
    "    \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n",
    "    accepted = bool(2 <= len(word) <= 40\n",
    "        and word.lower() not in stopwords)\n",
    "    return accepted\n",
    "\n",
    "# generator, create item once a time\n",
    "def get_terms(tree):\n",
    "    for leaf in leaves(tree):\n",
    "        term = [normalise(w) for w,t in leaf if acceptable_word(w) ]\n",
    "        # Phrase only\n",
    "        if len(term)>1:\n",
    "            yield term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['new', 'favourit', 'place', 'sweet'],\n",
       " ['pacif', 'mall'],\n",
       " ['mani', 'shop'],\n",
       " ['sit', 'coupl', 'seat'],\n",
       " ['oreo', 'milk', 'tea', 'perfect', 'combin'],\n",
       " ['milk', 'tea', 'tast'],\n",
       " ['alley', 'garden', 'milk', 'tea', 'tast'],\n",
       " ['milk', 'tea'],\n",
       " ['bubbl', 'lyche'],\n",
       " ['huge', 'fan', 'origin', 'milk', 'tea'],\n",
       " ['tri', 'drink'],\n",
       " ['order', 'lyche'],\n",
       " ['sweet', 'strong', 'lyche', 'tast'],\n",
       " ['tast', 'milk', 'tea'],\n",
       " ['milk', 'tea', 'tast'],\n",
       " ['oreo', 'milk', 'tea', 'lyche']]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traverse tree and get noun phrases\n",
    "npTokenList = [word for word in get_terms(tree)]\n",
    "\n",
    "npTokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten phrase lists to get tokens for analysis\n",
    "def flatten(npTokenList):\n",
    "    finalList =[]\n",
    "    for phrase in npTokenList:\n",
    "        token = ''\n",
    "        for word in phrase:\n",
    "            token += word + ' '\n",
    "        finalList.append(token.rstrip())\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new favourit place sweet',\n",
       " 'pacif mall',\n",
       " 'mani shop',\n",
       " 'sit coupl seat',\n",
       " 'oreo milk tea perfect combin',\n",
       " 'milk tea tast',\n",
       " 'alley garden milk tea tast',\n",
       " 'milk tea',\n",
       " 'bubbl lyche',\n",
       " 'huge fan origin milk tea',\n",
       " 'tri drink',\n",
       " 'order lyche',\n",
       " 'sweet strong lyche tast',\n",
       " 'tast milk tea',\n",
       " 'milk tea tast',\n",
       " 'oreo milk tea lyche']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalList = flatten(npTokenList)\n",
    "finalList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chinese'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PMI for all terms and all possible labels\n",
    "def pmiForAllCal(df, label_column='groundTruth', topk=keyphrases):\n",
    "    #Try calculate all the pmi for top k and store them into one pmidf dataframe\n",
    "    pmilist = []\n",
    "    pmiposlist = []\n",
    "    pmineglist = []\n",
    "    for word in tqdm(topk):\n",
    "        pmilist.append([word[0]]+[pmiCal(df,word[0])])\n",
    "        pmiposlist.append([word[0]]+[pmiIndivCal(df,word[0],1,label_column)])\n",
    "        pmineglist.append([word[0]]+[pmiIndivCal(df,word[0],0,label_column)])\n",
    "    pmidf = pandas.DataFrame(pmilist)\n",
    "    pmiposlist = pandas.DataFrame(pmiposlist)\n",
    "    pmineglist = pandas.DataFrame(pmineglist)\n",
    "    pmiposlist.columns = ['word','pmi']\n",
    "    pmineglist.columns = ['word','pmi']\n",
    "    pmidf.columns = ['word','pmi']\n",
    "    return pmiposlist, pmineglist, pmidf\n",
    "\n",
    "def pmiIndivCal(df,x,gt, label_column='Binary'):\n",
    "    px = sum(df[label_column]==gt)/len(df)\n",
    "    py = sum(df[x]==1)/len(df)\n",
    "    pxy = len(df[(df[label_column]==gt) & (df[x]==1)])/len(df)\n",
    "    if pxy==0:#Log 0 cannot happen\n",
    "        pmi = math.log((pxy+0.0001)/(px*py))\n",
    "    else:\n",
    "        pmi = math.log(pxy/(px*py))\n",
    "    return pmi\n",
    "\n",
    "# Simple example of getting pairwise mutual information of a term\n",
    "def pmiCal(df, x):\n",
    "    pmilist=[]\n",
    "    for i in [1,0]:\n",
    "        for j in [0,1]:\n",
    "            px = sum(df['Binary']==i)/len(df)\n",
    "            py = sum(df[x]==j)/len(df)\n",
    "            pxy = len(df[(df['Binary']==i) & (df[x]==j)])/len(df)\n",
    "            if pxy==0:#Log 0 cannot happen\n",
    "                pmi = math.log((pxy+0.0001)/(px*py))\n",
    "            else:\n",
    "                pmi = math.log(pxy/(px*py))\n",
    "            pmilist.append([i]+[j]+[px]+[py]+[pxy]+[pmi])\n",
    "    pmidf = pandas.DataFrame(pmilist)\n",
    "    pmidf.columns = ['x','y','px','py','pxy','pmi']\n",
    "    return pmidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/235 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2392\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2393\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2394\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5239)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5085)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20405)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'c'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-67998970717e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpmiposlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpmineglist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpmidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpmiForAllCal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeyphrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-119-bcd1d791ae5b>\u001b[0m in \u001b[0;36mpmiForAllCal\u001b[1;34m(df, label_column, topk)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpmineglist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mpmilist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpmiCal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mpmiposlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpmiIndivCal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mpmineglist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpmiIndivCal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-119-bcd1d791ae5b>\u001b[0m in \u001b[0;36mpmiCal\u001b[1;34m(df, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mpx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Binary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mpxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Binary'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpxy\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#Log 0 cannot happen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2060\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2062\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2067\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2069\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2070\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2071\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1532\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1534\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1535\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1536\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\py35\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2393\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2395\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5239)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5085)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20405)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20359)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'c'"
     ]
    }
   ],
   "source": [
    "pmiposlist, pmineglist, pmidf = pmiForAllCal(cur_df, topk=keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmiposlist, pmineglist, pmidf = pmiForAllCal(finaldf_phrase, topk=topk_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmiposlist.sort_values('pmi',ascending=0).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmineglist.sort_values('pmi',ascending=0).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_df\", \"wb\") as output_file:\n",
    "    pkl.dump(test_df, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit our cur_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keyphrases = pd.read_csv('../../data/yelp/KeyPhrases.csv')['Phrases'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinese',\n",
       " 'fast',\n",
       " 'thai',\n",
       " 'bar',\n",
       " 'fry',\n",
       " 'fried',\n",
       " 'dessert',\n",
       " 'dinner',\n",
       " 'lunch',\n",
       " 'soup',\n",
       " 'mexico',\n",
       " 'italian',\n",
       " 'mexican',\n",
       " 'vietnamese',\n",
       " 'buffet',\n",
       " 'takeout',\n",
       " 'casual',\n",
       " 'pub',\n",
       " 'bakery',\n",
       " 'indian',\n",
       " 'classic',\n",
       " 'modern',\n",
       " 'french',\n",
       " 'asian',\n",
       " 'birthday',\n",
       " 'vegetarian',\n",
       " 'downtown',\n",
       " 'bbq',\n",
       " 'japanese',\n",
       " 'breakfast',\n",
       " 'seafood',\n",
       " 'brunch',\n",
       " 'taco',\n",
       " 'curry',\n",
       " 'potato',\n",
       " 'crispy',\n",
       " 'shrimp',\n",
       " 'bread',\n",
       " 'chocolate',\n",
       " 'ramen',\n",
       " 'pizza',\n",
       " 'beer',\n",
       " 'sandwich',\n",
       " 'cake',\n",
       " 'sushi',\n",
       " 'egg',\n",
       " 'fish',\n",
       " 'coffee',\n",
       " 'burger',\n",
       " 'cheese',\n",
       " 'salad',\n",
       " 'pork',\n",
       " 'beef',\n",
       " 'tea',\n",
       " 'noodle',\n",
       " 'meat',\n",
       " 'chicken',\n",
       " 'dim sum',\n",
       " 'cocktail',\n",
       " 'ice cream',\n",
       " 'squid',\n",
       " 'tempura',\n",
       " 'tapioca',\n",
       " 'donut',\n",
       " 'olive',\n",
       " 'espresso',\n",
       " 'octopus',\n",
       " 'croissant',\n",
       " 'banana',\n",
       " 'cookie',\n",
       " 'honey',\n",
       " 'cone',\n",
       " 'scallop',\n",
       " 'congee',\n",
       " 'skewer',\n",
       " 'miso',\n",
       " 'lettuce',\n",
       " 'pop',\n",
       " 'strawberry',\n",
       " 'apple',\n",
       " 'avocado',\n",
       " 'juice',\n",
       " 'booth',\n",
       " 'calamari',\n",
       " 'kimchi',\n",
       " 'patty',\n",
       " 'sesame',\n",
       " 'tart',\n",
       " 'four',\n",
       " 'crepe',\n",
       " 'tuna',\n",
       " 'wrap',\n",
       " 'lemon',\n",
       " 'vegan',\n",
       " 'coconut',\n",
       " 'corn',\n",
       " 'poutine',\n",
       " 'toast',\n",
       " 'belly',\n",
       " 'bubble',\n",
       " 'oyster',\n",
       " 'cocktail',\n",
       " 'cheesecake',\n",
       " 'fruit',\n",
       " 'sausage',\n",
       " 'latte',\n",
       " 'matcha',\n",
       " 'pancake',\n",
       " 'duck',\n",
       " 'tofu',\n",
       " 'sashimi',\n",
       " 'lamb',\n",
       " 'mango',\n",
       " 'bacon',\n",
       " 'tomato',\n",
       " 'lobster',\n",
       " 'wine',\n",
       " 'rib',\n",
       " 'waffle',\n",
       " 'bun',\n",
       " 'wing',\n",
       " 'dumpling',\n",
       " 'bean',\n",
       " 'steak',\n",
       " 'salmon',\n",
       " 'pasta',\n",
       " 'milk',\n",
       " 'fried chicken',\n",
       " 'milk tea',\n",
       " 'green tea',\n",
       " 'bubble tea',\n",
       " 'pork belly',\n",
       " 'spring roll',\n",
       " 'fried rice',\n",
       " 'pork bone soup',\n",
       " 'parking',\n",
       " 'store',\n",
       " 'shopping',\n",
       " 'nail',\n",
       " 'theatre',\n",
       " 'movie',\n",
       " 'washroom',\n",
       " 'window',\n",
       " 'station',\n",
       " 'chair',\n",
       " 'markham',\n",
       " 'plaza',\n",
       " 'market',\n",
       " 'mall',\n",
       " 'quick',\n",
       " 'clean',\n",
       " 'busy',\n",
       " 'fresh',\n",
       " 'friendly',\n",
       " 'convenient',\n",
       " 'refill',\n",
       " 'soggy',\n",
       " 'greeted',\n",
       " 'bright',\n",
       " 'crowded',\n",
       " 'overpriced',\n",
       " 'cheaper',\n",
       " 'immediately',\n",
       " 'dog',\n",
       " 'quiet',\n",
       " 'efficient',\n",
       " 'spacious',\n",
       " 'pleasant',\n",
       " 'fair',\n",
       " 'complaint',\n",
       " 'disappointing',\n",
       " 'fancy',\n",
       " 'comfortable',\n",
       " 'dark',\n",
       " 'cozy',\n",
       " 'helpful',\n",
       " 'tax',\n",
       " 'nicely',\n",
       " 'honestly',\n",
       " 'pricey',\n",
       " 'yummy',\n",
       " 'music',\n",
       " 'chip',\n",
       " 'attentive',\n",
       " 'reasonable',\n",
       " 'wait',\n",
       " 'traditional',\n",
       " 'spicy',\n",
       " 'flavorful',\n",
       " 'fluffy',\n",
       " 'smooth',\n",
       " 'frozen',\n",
       " 'sweetness',\n",
       " 'mayo',\n",
       " 'gravy',\n",
       " 'healthy',\n",
       " 'rare',\n",
       " 'refreshing',\n",
       " 'crunchy',\n",
       " 'chili',\n",
       " 'crust',\n",
       " 'stick',\n",
       " 'steamed',\n",
       " 'greasy',\n",
       " 'dip',\n",
       " 'gelato',\n",
       " 'salt',\n",
       " 'stuffed',\n",
       " 'topped',\n",
       " 'smoked',\n",
       " 'roasted',\n",
       " 'seasoned',\n",
       " 'chewy',\n",
       " 'pot',\n",
       " 'solid',\n",
       " 'sour',\n",
       " 'baked',\n",
       " 'juicy',\n",
       " 'creamy',\n",
       " 'deep fried',\n",
       " 'lactose intolerant',\n",
       " 'dietary restriction',\n",
       " 'gong cha',\n",
       " 'general tao',\n",
       " 'wild boar',\n",
       " 'financial district',\n",
       " 'pale ale',\n",
       " 'public transit',\n",
       " 'balsamic vinegar',\n",
       " 'uber eats',\n",
       " 'alcoholic beverage',\n",
       " 'grand opening',\n",
       " 'north york',\n",
       " 'english muffin',\n",
       " 'accept debit']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = Tokenizer()\n",
    "token_list = cur_df[\"review\"].tolist()\n",
    "tokenizer.fit_on_texts(token_list)\n",
    "\n",
    "\n",
    "fdist1 = FreqDist(token_flatten_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Revise the previous dataframe transform function...\n",
    "def newDataFrameTransformation(hotelDf, reviewDF, k=50):\n",
    "    reviews = reviewDF['conca_review'].values\n",
    "    \n",
    "    #Top-k frequent terms\n",
    "    counter = Counter()\n",
    "    for review in reviews:\n",
    "            counter.update(flatten([word\n",
    "                            for word \n",
    "                            in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+', review)))) \n",
    "                            ]))\n",
    "    topk = counter.most_common(k)   \n",
    "#     topk = keyphrases\n",
    "    \n",
    "    #Find out if a particular review has the word from topk list\n",
    "    freqReview = []\n",
    "    for i in range(len(reviews)):\n",
    "        tempCounter = Counter(flatten([word \n",
    "                                       for word \n",
    "                                       in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+',reviews[i]))))]))\n",
    "        topkinReview = [1 if tempCounter[word] > 0 else 0 for (word,wordCount) in topk]\n",
    "        freqReview.append(topkinReview)\n",
    "         \n",
    "    #Prepare freqReviewDf\n",
    "    freqReviewDf = pandas.DataFrame(freqReview)\n",
    "    dfName = []\n",
    "    for c in topk:\n",
    "        dfName.append(c[0])\n",
    "    freqReviewDf.columns = dfName\n",
    "    finalreviewDf = reviewDF.join(freqReviewDf)\n",
    "    finaldf = hotelDf[['hotelName','ratingScore','groundTruth']].join(finalreviewDf)\n",
    "    return topk, finaldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<unknown>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ordered lemon mango slush lemon taste strong love lemon love drink informed jelly drink added bonus since love drink topping jelly different compared bubble tea place soft chewy overall enjoyed drink\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# reviews = literal_eval([test_df['conca_review'].values[i] for i in range(len(test_df))])\n",
    "reviews = [literal_eval(review) for review in test_df['conca_review']]\n",
    "\n",
    "#Top-k frequent terms\n",
    "counter = Counter()\n",
    "for review in reviews:\n",
    "    counter.update(flatten([word for word in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+', review))))]))\n",
    "topk = counter.most_common(k)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ordered lemon mango slush lemon taste strong love lemon love drink informed jelly drink added bonus since love drink topping jelly different compared bubble tea place soft chewy overall enjoyed drink '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'WordListCorpusReader' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-05232a2f2277>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-81-05232a2f2277>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-75f11eefbfbe>\u001b[0m in \u001b[0;36mget_terms\u001b[1;34m(tree)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mleaf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mleaves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnormalise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mleaf\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0macceptable_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Phrase only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-75f11eefbfbe>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mleaf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mleaves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mterm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnormalise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mleaf\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0macceptable_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Phrase only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-75f11eefbfbe>\u001b[0m in \u001b[0;36macceptable_word\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;34m\"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     accepted = bool(2 <= len(word) <= 40\n\u001b[1;32m---> 28\u001b[1;33m         and word.lower() not in stopwords)\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maccepted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'WordListCorpusReader' is not iterable"
     ]
    }
   ],
   "source": [
    "[word for word in get_terms(chunker.parse(pos_tag(re.findall(r'\\w+', reviews[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = cur_df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>business_id</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>ghost</th>\n",
       "      <th>img_dsc</th>\n",
       "      <th>img_url</th>\n",
       "      <th>nr</th>\n",
       "      <th>photo_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>...</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Binary</th>\n",
       "      <th>review</th>\n",
       "      <th>conca_review</th>\n",
       "      <th>keyVector</th>\n",
       "      <th>keyphrases_indices_length</th>\n",
       "      <th>UserIndex</th>\n",
       "      <th>ItemIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>105</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada. ...</td>\n",
       "      <td>['https://s3-media4.fl.yelpcdn.com/bphoto/tu7j...</td>\n",
       "      <td>False</td>\n",
       "      <td>325.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>['ordered', 'lemon', 'mango', 'slush', 'lemon'...</td>\n",
       "      <td>ordered lemon mango slush lemon taste strong ...</td>\n",
       "      <td>[53, 92, 99, 112, 130, 212]</td>\n",
       "      <td>6</td>\n",
       "      <td>2464</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>171</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada. ...</td>\n",
       "      <td>['https://s3-media3.fl.yelpcdn.com/bphoto/h110...</td>\n",
       "      <td>False</td>\n",
       "      <td>307.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>['came', 'sunday', 'afternoon', 'nt', 'busy', ...</td>\n",
       "      <td>came sunday afternoon nt busy came sunday spe...</td>\n",
       "      <td>[53, 99, 126, 128, 130, 148, 151, 171]</td>\n",
       "      <td>8</td>\n",
       "      <td>1021</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>239</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada. ...</td>\n",
       "      <td>['https://s3-media4.fl.yelpcdn.com/bphoto/tS6Y...</td>\n",
       "      <td>False</td>\n",
       "      <td>78.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>['grapefruit', 'yakult', 'green', 'tea', 'aloe...</td>\n",
       "      <td>grapefruit yakult green tea aloe jelly found ...</td>\n",
       "      <td>[53, 92, 103, 129, 192]</td>\n",
       "      <td>5</td>\n",
       "      <td>529</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada',...</td>\n",
       "      <td>['https://s3-media3.fl.yelpcdn.com/bphoto/rfB0...</td>\n",
       "      <td>False</td>\n",
       "      <td>504.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2016</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>['saw', 'newly', 'opened', 'bubble', 'tea', 's...</td>\n",
       "      <td>saw newly opened bubble tea shop wanted give ...</td>\n",
       "      <td>[49, 53, 99, 126, 128, 130, 136, 161, 206, 212]</td>\n",
       "      <td>10</td>\n",
       "      <td>1616</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada',...</td>\n",
       "      <td>['https://s3-media1.fl.yelpcdn.com/bphoto/2jVn...</td>\n",
       "      <td>False</td>\n",
       "      <td>743.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2018</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>['happy', 'lemon', 'become', 'new', 'favourite...</td>\n",
       "      <td>happy lemon become new favourite place sweet ...</td>\n",
       "      <td>[53, 92, 99, 103, 126, 128, 148, 152, 165, 197]</td>\n",
       "      <td>10</td>\n",
       "      <td>590</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>115</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada. ...</td>\n",
       "      <td>['https://s3-media2.fl.yelpcdn.com/bphoto/hfu-...</td>\n",
       "      <td>False</td>\n",
       "      <td>514.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>['short', 'sweet', 'overall', 'logo', 'cute', ...</td>\n",
       "      <td>short sweet overall logo cute like thing pmal...</td>\n",
       "      <td>[49, 92, 95, 136, 148, 206, 222]</td>\n",
       "      <td>7</td>\n",
       "      <td>1365</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>126</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>56000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2018</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>['happy', 'lemon', 'one', 'numerous', 'bubble'...</td>\n",
       "      <td>happy lemon one numerous bubble tea shop insi...</td>\n",
       "      <td>[49, 53, 92, 99, 130, 148, 152, 197, 206]</td>\n",
       "      <td>9</td>\n",
       "      <td>556</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>122</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada. ...</td>\n",
       "      <td>['https://s3-media4.fl.yelpcdn.com/bphoto/xeay...</td>\n",
       "      <td>False</td>\n",
       "      <td>867.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>['honestly', 'satisfied', 'place', 'first', 't...</td>\n",
       "      <td>honestly satisfied place first time drink nt ...</td>\n",
       "      <td>[53, 99, 130, 178, 179]</td>\n",
       "      <td>5</td>\n",
       "      <td>246</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>112</td>\n",
       "      <td>False</td>\n",
       "      <td>['Photo of Happy Lemon - Markham, ON, Canada']</td>\n",
       "      <td>['https://s3-media1.fl.yelpcdn.com/bphoto/8hif...</td>\n",
       "      <td>False</td>\n",
       "      <td>54.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2018</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>['additional', 'note', 'tried', 'grapefruit', ...</td>\n",
       "      <td>additional note tried grapefruit lemon slushy...</td>\n",
       "      <td>[62, 92, 103, 105, 106, 112, 152, 178, 197]</td>\n",
       "      <td>9</td>\n",
       "      <td>2012</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>Xo1LNzhnwE-ilqsM3ybs9Q</td>\n",
       "      <td>247</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>1074.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>['always', 'wanted', 'try', 'happy', 'lemon', ...</td>\n",
       "      <td>always wanted try happy lemon pmall lemon per...</td>\n",
       "      <td>[49, 53, 92, 126, 128, 148, 206]</td>\n",
       "      <td>7</td>\n",
       "      <td>568</td>\n",
       "      <td>5546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0.1.1             business_id  friend_count  ghost  \\\n",
       "0             6               6  Xo1LNzhnwE-ilqsM3ybs9Q           105  False   \n",
       "1             7               7  Xo1LNzhnwE-ilqsM3ybs9Q           171  False   \n",
       "2             8               8  Xo1LNzhnwE-ilqsM3ybs9Q           239  False   \n",
       "3             9               9  Xo1LNzhnwE-ilqsM3ybs9Q            10  False   \n",
       "4            22              22  Xo1LNzhnwE-ilqsM3ybs9Q            80  False   \n",
       "5            24              24  Xo1LNzhnwE-ilqsM3ybs9Q           115  False   \n",
       "6            25              25  Xo1LNzhnwE-ilqsM3ybs9Q           126  False   \n",
       "7            27              27  Xo1LNzhnwE-ilqsM3ybs9Q           122  False   \n",
       "8            28              28  Xo1LNzhnwE-ilqsM3ybs9Q           112  False   \n",
       "9            29              29  Xo1LNzhnwE-ilqsM3ybs9Q           247  False   \n",
       "\n",
       "                                             img_dsc  \\\n",
       "0  ['Photo of Happy Lemon - Markham, ON, Canada. ...   \n",
       "1  ['Photo of Happy Lemon - Markham, ON, Canada. ...   \n",
       "2  ['Photo of Happy Lemon - Markham, ON, Canada. ...   \n",
       "3  ['Photo of Happy Lemon - Markham, ON, Canada',...   \n",
       "4  ['Photo of Happy Lemon - Markham, ON, Canada',...   \n",
       "5  ['Photo of Happy Lemon - Markham, ON, Canada. ...   \n",
       "6                                                 []   \n",
       "7  ['Photo of Happy Lemon - Markham, ON, Canada. ...   \n",
       "8     ['Photo of Happy Lemon - Markham, ON, Canada']   \n",
       "9                                                 []   \n",
       "\n",
       "                                             img_url     nr  photo_count  \\\n",
       "0  ['https://s3-media4.fl.yelpcdn.com/bphoto/tu7j...  False        325.0   \n",
       "1  ['https://s3-media3.fl.yelpcdn.com/bphoto/h110...  False        307.0   \n",
       "2  ['https://s3-media4.fl.yelpcdn.com/bphoto/tS6Y...  False         78.0   \n",
       "3  ['https://s3-media3.fl.yelpcdn.com/bphoto/rfB0...  False        504.0   \n",
       "4  ['https://s3-media1.fl.yelpcdn.com/bphoto/2jVn...  False        743.0   \n",
       "5  ['https://s3-media2.fl.yelpcdn.com/bphoto/hfu-...  False        514.0   \n",
       "6                                                 []  False      56000.0   \n",
       "7  ['https://s3-media4.fl.yelpcdn.com/bphoto/xeay...  False        867.0   \n",
       "8  ['https://s3-media1.fl.yelpcdn.com/bphoto/8hif...  False         54.0   \n",
       "9                                                 []  False       1074.0   \n",
       "\n",
       "   rating    ...      Year Month Day Binary  \\\n",
       "0     4.0    ...      2016    23   8      0   \n",
       "1     3.0    ...      2016     2  10      0   \n",
       "2     5.0    ...      2016     6  11      1   \n",
       "3     4.0    ...      2016    25   9      0   \n",
       "4     5.0    ...      2018    30   6      1   \n",
       "5     3.0    ...      2018     3   6      0   \n",
       "6     3.0    ...      2018    19   3      0   \n",
       "7     2.0    ...      2018     6   7      0   \n",
       "8     4.0    ...      2018    18   5      0   \n",
       "9     3.0    ...      2017    20   8      0   \n",
       "\n",
       "                                              review  \\\n",
       "0  ['ordered', 'lemon', 'mango', 'slush', 'lemon'...   \n",
       "1  ['came', 'sunday', 'afternoon', 'nt', 'busy', ...   \n",
       "2  ['grapefruit', 'yakult', 'green', 'tea', 'aloe...   \n",
       "3  ['saw', 'newly', 'opened', 'bubble', 'tea', 's...   \n",
       "4  ['happy', 'lemon', 'become', 'new', 'favourite...   \n",
       "5  ['short', 'sweet', 'overall', 'logo', 'cute', ...   \n",
       "6  ['happy', 'lemon', 'one', 'numerous', 'bubble'...   \n",
       "7  ['honestly', 'satisfied', 'place', 'first', 't...   \n",
       "8  ['additional', 'note', 'tried', 'grapefruit', ...   \n",
       "9  ['always', 'wanted', 'try', 'happy', 'lemon', ...   \n",
       "\n",
       "                                        conca_review  \\\n",
       "0   ordered lemon mango slush lemon taste strong ...   \n",
       "1   came sunday afternoon nt busy came sunday spe...   \n",
       "2   grapefruit yakult green tea aloe jelly found ...   \n",
       "3   saw newly opened bubble tea shop wanted give ...   \n",
       "4   happy lemon become new favourite place sweet ...   \n",
       "5   short sweet overall logo cute like thing pmal...   \n",
       "6   happy lemon one numerous bubble tea shop insi...   \n",
       "7   honestly satisfied place first time drink nt ...   \n",
       "8   additional note tried grapefruit lemon slushy...   \n",
       "9   always wanted try happy lemon pmall lemon per...   \n",
       "\n",
       "                                         keyVector keyphrases_indices_length  \\\n",
       "0                      [53, 92, 99, 112, 130, 212]                         6   \n",
       "1           [53, 99, 126, 128, 130, 148, 151, 171]                         8   \n",
       "2                          [53, 92, 103, 129, 192]                         5   \n",
       "3  [49, 53, 99, 126, 128, 130, 136, 161, 206, 212]                        10   \n",
       "4  [53, 92, 99, 103, 126, 128, 148, 152, 165, 197]                        10   \n",
       "5                 [49, 92, 95, 136, 148, 206, 222]                         7   \n",
       "6        [49, 53, 92, 99, 130, 148, 152, 197, 206]                         9   \n",
       "7                          [53, 99, 130, 178, 179]                         5   \n",
       "8      [62, 92, 103, 105, 106, 112, 152, 178, 197]                         9   \n",
       "9                 [49, 53, 92, 126, 128, 148, 206]                         7   \n",
       "\n",
       "   UserIndex  ItemIndex  \n",
       "0       2464       5546  \n",
       "1       1021       5546  \n",
       "2        529       5546  \n",
       "3       1616       5546  \n",
       "4        590       5546  \n",
       "5       1365       5546  \n",
       "6        556       5546  \n",
       "7        246       5546  \n",
       "8       2012       5546  \n",
       "9        568       5546  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
