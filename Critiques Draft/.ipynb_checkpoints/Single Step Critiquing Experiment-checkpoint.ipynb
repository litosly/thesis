{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "# sys.path.clear()\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu\\\\python36.zip')\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu\\\\DLLs')\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu\\\\lib')\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu')\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu\\\\lib\\\\site-packages')\n",
    "sys.path.insert(0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Original Data\n",
    "df_train = pd.read_csv('../../data/yelp/Train.csv',encoding='latin-1')\n",
    "df_valid = pd.read_csv('../../data/yelp/Valid.csv',encoding='latin-1')\n",
    "df_test = pd.read_csv('../../data/yelp/Test.csv',encoding='latin-1')\n",
    "keyphrases = pd.read_csv('../../data/yelp/KeyPhrases.csv')['Phrases'].tolist()\n",
    "keyphrase_popularity = np.loadtxt('../data/yelp/'+'keyphrase_popularity.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load U-I Data \n",
    "rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")\n",
    "rvalid = load_npz(\"../../data/yelp/Rvalid.npz\")\n",
    "rtest = load_npz(\"../../data/yelp/Rtest.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load user/item keyphrase data\n",
    "U_K = load_npz(\"../../data/yelp/U_K.npz\")\n",
    "I_K = load_npz(\"../../data/yelp/I_K.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, row_name = 'ItemIndex', shape = (3668,75)):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        key_vector = literal_eval(df['keyVector'][i])\n",
    "        rows.extend([df[row_name][i]]*len(key_vector)) ## Item index\n",
    "        cols.extend(key_vector) ## Keyword Index\n",
    "        vals.extend(np.array([1]*len(key_vector)))\n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        vector_u = prediction_score[user_index]\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "\n",
    "    vector_u = vector_u\n",
    "\n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    return vector_u[:topK]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    \"\"\"\n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    \"\"\"\n",
    "    prediction_scores = []\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores to all users\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "\n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def predict_vector(user_index, matrix_train, k, similarity):\n",
    "    \"\"\"\n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    get only user_index row\n",
    "    \"\"\"\n",
    "    vector_u = similarity[user_index]\n",
    "    \n",
    "    # Get closest K neighbors excluding user u self\n",
    "    similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "    # Get neighbors similarity weights and ratings\n",
    "    similar_users_weights = similarity[user_index][similar_users]\n",
    "    similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "    prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "    \n",
    "    return np.sum(prediction_scores_u, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evluation \n",
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        \"R-Precision\": r_precision,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = results[name]\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (np.average(results[name]),\n",
    "                                                              1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = results[name]\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (np.average(results[name]), 1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Initial Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Item KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:07<00:00, 311.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 2780.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4753.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4859.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4776.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4886.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4836.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4166.57it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity = normalize(train(rtrain))\n",
    "user_item_prediction_score = predict(rtrain, 100, similarity, item_similarity_en= False)\n",
    "user_item_predict = prediction(user_item_prediction_score, 50, rtrain)\n",
    "user_item_res = evaluate(user_item_predict, rtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.06333952750429245, 0.00455145183026277),\n",
       " 'MAP@15': (0.05872249612550844, 0.0038121823597348156),\n",
       " 'MAP@20': (0.055196280875748356, 0.003345258771111864),\n",
       " 'MAP@5': (0.06940666362391602, 0.0060646277121995185),\n",
       " 'MAP@50': (0.04436838784245958, 0.0022020570312340938),\n",
       " 'NDCG': (0.09071795198195, 0.003803970590016347),\n",
       " 'Precision@10': (0.05330899132816066, 0.0032544740534870857),\n",
       " 'Precision@15': (0.04698006998326487, 0.0025622684368576416),\n",
       " 'Precision@20': (0.043336376083979916, 0.002222006808632301),\n",
       " 'Precision@5': (0.06462802373345505, 0.004754606217931856),\n",
       " 'Precision@50': (0.032889091738931994, 0.0014317314500480152),\n",
       " 'R-Precision': (0.048464138894968055, 0.0027869069242192506),\n",
       " 'Recall@10': (0.04269615369598775, 0.0027831077562657665),\n",
       " 'Recall@15': (0.05562887733868642, 0.0031389991916236284),\n",
       " 'Recall@20': (0.0677664821917642, 0.003424484045821138),\n",
       " 'Recall@5': (0.026408137823500974, 0.002191309117794643),\n",
       " 'Recall@50': (0.12696642809611336, 0.0048241883971306436)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k = 100 \n",
    "user_item_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Similarity Matrix Learned with Linear Regression¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "X = normalize(U_K.todense())\n",
    "y = normalize(train(rtrain))\n",
    "clf = Ridge(alpha=0.1).fit(X, y) # Optimality at L2 regularization = 0.1\n",
    "lr_similarity = clf.predict(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediciting\n",
    "similarity = lr_similarity\n",
    "lr_prediction_score = predict(rtrain, 100, similarity, item_similarity_en= False)\n",
    "lr_predict = prediction(lr_prediction_score, 50, rtrain)\n",
    "lr_res = evaluate(lr_predict, rtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k = 100\n",
    "lr_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critiquing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding of critiquing\n",
    "def get_critiqued_UK(user_keyphrase_frequency,user_index,critiqued_keyphrase):\n",
    "    \"\"\"\n",
    "    user_keyphrase_frequency is the U_K matrix (csr sparse matrix)\n",
    "    return the one-hot encoding of the critique\n",
    "    \"\"\"\n",
    "    U_K_cp = user_keyphrase_frequency.copy()\n",
    "    U_K_cp[user_index] = 0\n",
    "    U_K_cp[user_index,critiqued_keyphrase] = 1\n",
    "    return U_K_cp\n",
    "\n",
    "def project_one_hot_encoding(reg, user_keyphrase_frequency,user_index = 0,critiqued_keyphrase = 0, normalize_en = True):\n",
    "    \"\"\"\n",
    "    Return the projection on user_sim space from one-hot encoding of critiqued keyphrase\n",
    "    The res[user_index] should be target embedding row\n",
    "    \"\"\"\n",
    "    critiqued_matrix = get_critiqued_UK(user_keyphrase_frequency, user_index, critiqued_keyphrase)\n",
    "    res = reg.predict(critiqued_matrix)\n",
    "    if normalize_en:\n",
    "        res = normalize((res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_initial_predictions(X = normalize(U_K.todense()), y = normalize(train(rtrain)),\n",
    "                            matrix_Train = rtrain, k = 100):\n",
    "    clf = Ridge(alpha=0.1).fit(X, y)\n",
    "    similarity = normalize(train(matrix_Train))\n",
    "    user_item_prediction_score = predict(matrix_Train, k, similarity, item_similarity_en= False)\n",
    "    return user_item_prediction_score, clf\n",
    "def get_valid_keyphrases(keyphrase_freq,top_recommendations,item = None,threshold=50,mutiple_keyphrases_en = False, top_items = None):\n",
    "    \"\"\"\n",
    "    Wrapper function to get either top 1 or top n keyphrases\n",
    "    \"\"\"\n",
    "    if mutiple_keyphrases_en:\n",
    "        top_keyphrases = []\n",
    "        for item in top_items:\n",
    "            top_keyphrases.extend(get_valid_keyphrases_for_one_item(keyphrase_freq,top_recommendations,item,threshold=threshold))\n",
    "        return np.ravel(list(set(top_keyphrases))) # remove duplicate and reformat to np array\n",
    "    else:\n",
    "        return get_valid_keyphrases_for_one_item(keyphrase_freq,top_recommendations,item,threshold=threshold)\n",
    "\n",
    "def get_valid_keyphrases_for_one_item(keyphrase_freq,top_recommendations, item,threshold=50):\n",
    "    \"\"\"\n",
    "    Get keyphrases of item that make sense\n",
    "    E.g. if the item has fewer than threshold=50 keyphrases, get all of them\n",
    "    otherwise get top 50 keyphrases\n",
    "    \"\"\"\n",
    "    keyphrase_length = len(keyphrase_freq[item].nonzero()[1])\n",
    "    if keyphrase_length<threshold:\n",
    "        return keyphrase_freq[item].nonzero()[1]\n",
    "    else:\n",
    "        keyphrases = np.ravel(keyphrase_freq[top_recommendations[0]].todense())\n",
    "        top_keyphrases = np.argsort(keyphrases)[::-1][:threshold]\n",
    "        return top_keyphrases\n",
    "    \n",
    "def predict_vector(user_index, matrix_train, k, similarity, with_keyphrase = False, \n",
    "                   keyphrase_freq = None, critiqued_keyphrase = None, alpha = 0):\n",
    "    \"\"\"\n",
    "    get only user_index row\n",
    "    if with_keyphrase = True, then penalize items without critiqued_keyphrase to alpha (default = 0)\n",
    "    \"\"\"\n",
    "    vector_u = similarity[user_index]\n",
    "    \n",
    "    # Get closest K neighbors excluding user u self\n",
    "    similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "    # Get neighbors similarity weights and ratings\n",
    "    similar_users_weights = similarity[user_index][similar_users]\n",
    "    similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "    \n",
    "    prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "    \n",
    "    if with_keyphrase == False:\n",
    "        return np.sum(prediction_scores_u, axis=0)\n",
    "    \n",
    "    # Only Predict items with critiqued_keyphrase \n",
    "    else:\n",
    "        prediction_scores = np.sum(prediction_scores_u, axis=0)\n",
    "#         print (prediction_scores)\n",
    "        #penalize items without critiqued keyphrase\n",
    "        items_with_keyphrase = np.ravel(keyphrase_freq.T[critiqued_keyphrase].nonzero()[1])\n",
    "#         print (items_with_keyphrase)\n",
    "        #Return the unique values in ar1 that are not in ar2.\n",
    "        items_without_keyphrase = np.setdiff1d(np.arange(matrix_train.shape[1]), items_with_keyphrase)\n",
    "        prediction_scores[items_without_keyphrase] = alpha # penalize\n",
    "        return prediction_scores\n",
    "#         print (prediction_scores)\n",
    "#         return prediction_scores/sum(prediction_scores)\n",
    "\n",
    "    \n",
    "def get_initial_prediction(user,X = normalize(U_K.todense()), y = normalize(train(rtrain)),\n",
    "                            matrix_Train = rtrain, k = 100):\n",
    "    \"\"\"\n",
    "    Get the initial knn predictions before critiquing pipelines\n",
    "    get the linear regression model for critiquing embedding (W_2)\n",
    "    get the initial user similarity matrix \n",
    "    k here is the parameter for KNN\n",
    "    \"\"\"\n",
    "    clf = Ridge(alpha=0.1).fit(X, y)\n",
    "    similarity = normalize(train(matrix_Train))\n",
    "    user_item_prediction_score = predict_vector(user, matrix_Train, k, similarity)\n",
    "    return user_item_prediction_score, clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For keyphrase selecting method # 3 \"diff\" \n",
    "def get_item_keyphrase_freq(keyphrase_freq,item):\n",
    "    \"\"\"\n",
    "    Get item's keyphrase frequency \n",
    "    \"\"\"\n",
    "    count = keyphrase_freq[item].todense()\n",
    "    return count/np.sum(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for getting restaurant info from ItemIndex\n",
    "def get_business_df(path = \"../../data/yelp/business.json\" ):\n",
    "    with open(path,encoding=\"utf8\") as json_file:\n",
    "        data = json_file.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_restaurant_info(business_df, business_id, name = True, review_count = True, stars = True ):\n",
    "    output_list = {}\n",
    "    row_idx = int(business_df.index[business_df['business_id'] == business_id].tolist()[0])\n",
    "    if name == True:\n",
    "        output_list['name'] = business_df['name'][row_idx].encode('utf-8').strip()\n",
    "    if review_count == True:\n",
    "        output_list['review_count'] = business_df['review_count'][row_idx]\n",
    "    if stars == True:\n",
    "        output_list['stars'] = business_df['stars'][row_idx] \n",
    "    return output_list\n",
    "\n",
    "# def get_businessid_from_Itemindex(ItemIndex_list, itemindex):\n",
    "#     return ItemIndex_list['business_id'].tolist()[itemindex]\n",
    "\n",
    "def get_restaurant_name(df_train, business_df, ItemIndex):\n",
    "    rows = np.where(df_train['ItemIndex'] == ItemIndex)\n",
    "    if len(rows)!= 0:\n",
    "        business_id = df_train.loc[rows[0][0]]['business_id']\n",
    "        item_info = get_restaurant_info(business_df, business_id)\n",
    "        return item_info['name']\n",
    "    return \"NOT_FOUND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keyphrase_popularity(df,keyphrases):\n",
    "    \"\"\"\n",
    "    Get keyphrase popularity (count) from dataframe\n",
    "    \"\"\"\n",
    "    keyphrase_popularity = np.zeros(len(keyphrases)) #initialize\n",
    "    for i in range(len(df)):\n",
    "        keyphrase_vector = literal_eval(df['keyVector'][i])\n",
    "        keyphrase_popularity[keyphrase_vector] += 1 # count\n",
    "    return keyphrase_popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keyphrase_popularity = get_keyphrase_popularity(df_train,keyphrases)\n",
    "\n",
    "# Save and load\n",
    "# np.savetxt('../data/yelp/'+'keyphrase_popularity.txt', keyphrase_popularity, fmt='%d')\n",
    "keyphrase_popularity = np.loadtxt('../data/yelp/'+'keyphrase_popularity.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_df = get_business_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize df for storing the experiment\n",
    "\n",
    "# post_ranki is post rank with different lambda ratio for combining pre-post User similarity matrix \n",
    "columns = ['user_id', 'target_item', 'item_name', 'iter', 'pre_rank', \n",
    "           'top_prediction_item_name','critiqued_keyphrase', 'keyphrase_name', \n",
    "           'post_rank0', \n",
    "           'post_rank1', \n",
    "           'post_rank2', \n",
    "           'post_rank3', \n",
    "           'post_rank4', \n",
    "           'post_rank5', \n",
    "           'post_rank6', \n",
    "           'post_rank7', \n",
    "           'post_rank8',\n",
    "           'post_rank9',\n",
    "           'post_rank10',\n",
    "           'num_existing_keyphrases'] \n",
    "df = pd.DataFrame(columns=columns)\n",
    "row = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_step_critiquing_experiment(user = 2, \n",
    "                           keyphrase_length_threshold = 150, \n",
    "                           max_iteration_threshold = 1,\n",
    "                           k = 50,\n",
    "                           df = df,\n",
    "                           row = row,\n",
    "                           business_df = business_df,\n",
    "                           keyphrases = keyphrases,\n",
    "                           keyphrase_popularity = keyphrase_popularity, \n",
    "                           keyphrase_selection_method = 'random',\n",
    "                           recommend_type = 'all',\n",
    "                           lams = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1] \n",
    "                          ):\n",
    "    \"\"\"\n",
    "    k: HR@k \n",
    "    keyphrase_length_threshold: limit the number of keyphrases in top recommended item\n",
    "    keyphrase_selection_method: 'random': randomly select keyphrase from wanted_keyphrases\n",
    "                                'pop': always select the most popular keyphrase in wanted_keyphrases\n",
    "                                'diff': select the keyphrase with largest frequency difference between top recommended \n",
    "                                        item and target item.\n",
    "    recommend_type: 'all': recommend all items\n",
    "                    'upper' (only_with_critiqued_keyphrase): recommend items with only critiqued_keyphrase\n",
    "    lam: modified_matrix = lam*origianl_matrix + (1-lam)*critiquing_embedding \n",
    "    \"\"\"\n",
    "    \n",
    "    row['user_id'] = user\n",
    "    print ('User ID ', user)\n",
    "    \n",
    "    # Set up (move to header line later)\n",
    "    matrix_Train = rtrain\n",
    "    matrix_Test = rtest\n",
    "    keyphrase_freq = I_K\n",
    "    num_items = rtrain.shape[1]\n",
    "    max_wanted_keyphrase = 10 # for keyphrase_selection_method == \"diff\"\n",
    "    initial_user_similarity_embedding = normalize(train(matrix_Train))\n",
    "    \n",
    "    # Get wanted items \n",
    "    candidate_items = matrix_Test[user].nonzero()[1]\n",
    "    train_items = matrix_Train[user].nonzero()[1]\n",
    "    wanted_items = np.setdiff1d(candidate_items, train_items)\n",
    "    print ('wanted_items length: ',len(wanted_items))\n",
    "    \n",
    "    # Get initial forward prediction \n",
    "    prediction_score,clf = get_initial_prediction(user, X = normalize(U_K.todense()), y = normalize(train(rtrain)),\n",
    "                            matrix_Train = rtrain, k = 100)\n",
    "    \n",
    "    # Get initial top recommended item(s)\n",
    "    top_recommendations = np.argsort(prediction_score)[::-1]\n",
    "    print (\"Initial top recommendation index\",top_recommendations[0])\n",
    "    try:\n",
    "        row['top_prediction_item_name'] = get_restaurant_name(df_train, business_df, top_recommendations[0])\n",
    "    # in case we cannot get the restaurant name\n",
    "    except: \n",
    "        row['top_prediction_item_name'] = 'CANNOT_FIND'\n",
    "        print ('Cannot get restaurant name for ItemIndex: ', top_recommendations[0])\n",
    "    \n",
    "    \n",
    "    # Get top recommended item's keyphrases\n",
    "    top_item = top_recommendations[0] \n",
    "    top_recommend_keyphrases = get_valid_keyphrases(keyphrase_freq,\n",
    "                                                    top_recommendations, \n",
    "                                                    item = top_item,\n",
    "                                                    threshold=keyphrase_length_threshold,\n",
    "                                                    mutiple_keyphrases_en = False, \n",
    "                                                    top_items = None)\n",
    "    print ('num_top_recommended_keyphrases ',len(top_recommend_keyphrases))\n",
    "\n",
    "    if keyphrase_selection_method == 'diff':\n",
    "        top_recommended_keyphrase_freq = get_item_keyphrase_freq(keyphrase_freq,item = top_item)\n",
    "    \n",
    "    \n",
    "    #####################################\n",
    "    # For each item, do the critiquing\n",
    "    \n",
    "    #limit the item to only 10\n",
    "    num_target_item = 0 # initialize item count\n",
    "    \n",
    "    for item in wanted_items:    \n",
    "        print ('target_item: ', item)\n",
    "        row['target_item'] = item\n",
    "        try:\n",
    "            row['item_name'] = get_restaurant_name(df_train, business_df, item)\n",
    "        except:\n",
    "            row['item_name'] = 'CANNOT_FIND'\n",
    "            print ('Cannot get restaurant name for ItemIndex: ', item)\n",
    "\n",
    "        # Get pre-critiquing rank\n",
    "        initial_rank = np.where(item == np.argsort(prediction_score)[::-1])[0][0]\n",
    "#         print ('target_item initial rank', int(initial_rank))\n",
    "        row['pre_rank'] = int(initial_rank)\n",
    "\n",
    "        # Get the target item's existing keyphrases\n",
    "        item_keyphrases = keyphrase_freq[item].nonzero()[1]\n",
    "#         print ('num_existing_keyphrases ',len(item_keyphrases))\n",
    "        \n",
    "        if keyphrase_selection_method == 'diff':\n",
    "            target_keyphrase_freq = get_item_keyphrase_freq(keyphrase_freq,item = item)\n",
    "            # indicate the keyphrase with large freq in target_item but small_keyphrase in top_recommended items\n",
    "            diff_keyphrase_freq = target_keyphrase_freq - top_recommended_keyphrase_freq\n",
    "            \n",
    "        # Get wanted keyphrases\n",
    "        if keyphrase_selection_method != 'diff':\n",
    "            # Get keyphrases that is not in the top recommended items but in the target item (we can select)\n",
    "            wanted_keyphrases = np.setdiff1d(item_keyphrases,top_recommend_keyphrases)\n",
    "\n",
    "            if len(wanted_keyphrases) == 0:\n",
    "                print (\"wanted_keyphrases is empty\")\n",
    "                break\n",
    "            row['num_existing_keyphrases'] = len(wanted_keyphrases)\n",
    "            \n",
    "        # For 'diff'\n",
    "        else:\n",
    "            wanted_keyphrases = np.argsort(np.ravel(diff_keyphrase_freq))[::-1][:max_wanted_keyphrase]\n",
    "            row['num_existing_keyphrases'] = len(wanted_keyphrases)\n",
    "\n",
    "        affected_items = np.array([])\n",
    "        modified_matrix = initial_user_similarity_embedding # initialize user similarity embedding\n",
    "        \n",
    "        #############################################\n",
    "        # Critiquing iteration\n",
    "        for iteration in range(max_iteration_threshold):\n",
    "            print ('cur_iter ', iteration)\n",
    "            row['iter'] = iteration\n",
    "            if keyphrase_selection_method == 'random':\n",
    "                # Randomly critique one keyphrase\n",
    "                critiqued_keyphrase = np.random.choice(wanted_keyphrases, size=1, replace=False)[0]\n",
    "            elif keyphrase_selection_method == 'pop':\n",
    "                # Always critique the most popular keyphrase\n",
    "                critiqued_keyphrase = wanted_keyphrases[np.argmax(keyphrase_popularity[wanted_keyphrases])]\n",
    "            elif keyphrase_selection_method == 'diff':\n",
    "                # critique the keyphrase with largest freq diff between top recommended_item and target_item\n",
    "                critiqued_keyphrase = wanted_keyphrases[0]\n",
    "#                 print (critiqued_keyphrase)\n",
    "            \n",
    "#             print ('critiqued_keyphrase ,',critiqued_keyphrase, keyphrases[critiqued_keyphrase])\n",
    "            row['critiqued_keyphrase'] = critiqued_keyphrase\n",
    "            row['keyphrase_name'] = keyphrases[critiqued_keyphrase]\n",
    "            \n",
    "            # Do not critique this keyphrase next time\n",
    "            wanted_keyphrases = np.delete(wanted_keyphrases, np.where(critiqued_keyphrase == wanted_keyphrases))\n",
    "            if len(wanted_keyphrases) == 0: \n",
    "                print ('no more keyphrase available')\n",
    "                break\n",
    "            \n",
    "            # Get affected items (items have critiqued keyphrase)\n",
    "            current_affected_items = keyphrase_freq[:, critiqued_keyphrase].nonzero()[0]\n",
    "            affected_items = np.unique(np.concatenate((affected_items, current_affected_items))).astype(int) \n",
    "            unaffected_items = np.setdiff1d(range(num_items), affected_items)\n",
    "\n",
    "            # Critiquing Embedding\n",
    "\n",
    "            # One hot encoding\n",
    "            critiqued_matrix_onehot = get_critiqued_UK(U_K,user,critiqued_keyphrase)\n",
    "            critiqued_matrix = clf.predict(critiqued_matrix_onehot)\n",
    "            critiqued_matrix = normalize(critiqued_matrix)\n",
    "            \n",
    "#             critiqued_matrix = project_one_hot_encoding(clf, \n",
    "#                                                         U_K,\n",
    "#                                                         user_index = user,\n",
    "#                                                         critiqued_keyphrase = critiqued_keyphrase, \n",
    "#                                                         normalize_en = True)\n",
    "\n",
    "#             modified_matrix = modified_matrix + critiqued_matrix # averaging user-item embedding and critiquing embeeding\n",
    "            \n",
    "            # Warning!!! The following is used only for testing single step critiquing, \n",
    "            # for full average critiquing, use the above commented line \n",
    "            post_ranks = []\n",
    "            for lam in lams:\n",
    "                modified_matrix = (1-lam)*normalize(train(matrix_Train)) + lam*critiqued_matrix \n",
    "                modified_matrix = normalize(modified_matrix)\n",
    "            \n",
    "                # Get new predictions from modified embedding\n",
    "                if recommend_type == 'all':\n",
    "                    prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix)\n",
    "                if recommend_type == 'upper':\n",
    "                    prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix, \n",
    "                                                         with_keyphrase = True, \n",
    "                                                         keyphrase_freq = keyphrase_freq, \n",
    "                                                         critiqued_keyphrase = critiqued_keyphrase, \n",
    "                                                         alpha = 0)\n",
    "                post_critique_rank = np.where(item == np.argsort(prediction_scores_u)[::-1])[0][0]\n",
    "                print ('target_item post-critique rank with lambda '+str(lam), int(post_critique_rank))\n",
    "                post_rank = int(post_critique_rank)\n",
    "                post_ranks.append(post_rank)\n",
    "            row['post_rank'] = post_ranks\n",
    "            row['post_rank0'] = post_ranks[0]\n",
    "            row['post_rank1'] = post_ranks[1]\n",
    "            row['post_rank2'] = post_ranks[2]\n",
    "            row['post_rank3'] = post_ranks[3]\n",
    "            row['post_rank4'] = post_ranks[4]\n",
    "            row['post_rank5'] = post_ranks[5]\n",
    "            row['post_rank6'] = post_ranks[6]\n",
    "            row['post_rank7'] = post_ranks[7]\n",
    "            row['post_rank8'] = post_ranks[8]\n",
    "            row['post_rank9'] = post_ranks[9]\n",
    "            row['post_rank10'] = post_ranks[10]\n",
    "            df = df.append(row, ignore_index=True)\n",
    "            \n",
    "\n",
    "        # break after got 10 target items \n",
    "        num_target_item += 1\n",
    "        if num_target_item >10: # only want max 10 items per user\n",
    "            break\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_step_with_avg_path = \"../tables/critiquing/single_step_lam_0105/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID  300\n",
      "wanted_items length:  14\n",
      "Initial top recommendation index 4443\n",
      "num_top_recommended_keyphrases  157\n",
      "target_item:  101\n",
      "cur_iter  0\n",
      "no more keyphrase available\n",
      "target_item:  886\n",
      "cur_iter  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 755\n",
      "target_item post-critique rank with lambda 0.1 765\n",
      "target_item post-critique rank with lambda 0.2 773\n",
      "target_item post-critique rank with lambda 0.3 371\n",
      "target_item post-critique rank with lambda 0.4 268\n",
      "target_item post-critique rank with lambda 0.5 246\n",
      "target_item post-critique rank with lambda 0.6 301\n",
      "target_item post-critique rank with lambda 0.7 249\n",
      "target_item post-critique rank with lambda 0.8 224\n",
      "target_item post-critique rank with lambda 0.9 196\n",
      "target_item post-critique rank with lambda 1 176\n",
      "cur_iter  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 755\n",
      "target_item post-critique rank with lambda 0.1 784\n",
      "target_item post-critique rank with lambda 0.2 783\n",
      "target_item post-critique rank with lambda 0.3 756\n",
      "target_item post-critique rank with lambda 0.4 885\n",
      "target_item post-critique rank with lambda 0.5 1874\n",
      "target_item post-critique rank with lambda 0.6 2967\n",
      "target_item post-critique rank with lambda 0.7 2934\n",
      "target_item post-critique rank with lambda 0.8 2901\n",
      "target_item post-critique rank with lambda 0.9 2919\n",
      "target_item post-critique rank with lambda 1 2903\n",
      "cur_iter  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 755\n",
      "target_item post-critique rank with lambda 0.1 740\n",
      "target_item post-critique rank with lambda 0.2 632\n",
      "target_item post-critique rank with lambda 0.3 600\n",
      "target_item post-critique rank with lambda 0.4 522\n",
      "target_item post-critique rank with lambda 0.5 662\n",
      "target_item post-critique rank with lambda 0.6 688\n",
      "target_item post-critique rank with lambda 0.7 769\n",
      "target_item post-critique rank with lambda 0.8 858\n",
      "target_item post-critique rank with lambda 0.9 929\n",
      "target_item post-critique rank with lambda 1 973\n",
      "cur_iter  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 755\n",
      "target_item post-critique rank with lambda 0.1 819\n",
      "target_item post-critique rank with lambda 0.2 906\n",
      "target_item post-critique rank with lambda 0.3 944\n",
      "target_item post-critique rank with lambda 0.4 1364\n",
      "target_item post-critique rank with lambda 0.5 2166\n",
      "target_item post-critique rank with lambda 0.6 2105\n",
      "target_item post-critique rank with lambda 0.7 1946\n",
      "target_item post-critique rank with lambda 0.8 1955\n",
      "target_item post-critique rank with lambda 0.9 1933\n",
      "target_item post-critique rank with lambda 1 1923\n",
      "cur_iter  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 755\n",
      "target_item post-critique rank with lambda 0.1 784\n",
      "target_item post-critique rank with lambda 0.2 875\n",
      "target_item post-critique rank with lambda 0.3 945\n",
      "target_item post-critique rank with lambda 0.4 2129\n",
      "target_item post-critique rank with lambda 0.5 2197\n",
      "target_item post-critique rank with lambda 0.6 2027\n",
      "target_item post-critique rank with lambda 0.7 3059\n",
      "target_item post-critique rank with lambda 0.8 4151\n",
      "target_item post-critique rank with lambda 0.9 4140\n",
      "target_item post-critique rank with lambda 1 2975\n",
      "target_item:  968\n",
      "cur_iter  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 574\n",
      "target_item post-critique rank with lambda 0.1 600\n",
      "target_item post-critique rank with lambda 0.2 614\n",
      "target_item post-critique rank with lambda 0.3 547\n",
      "target_item post-critique rank with lambda 0.4 443\n",
      "target_item post-critique rank with lambda 0.5 311\n",
      "target_item post-critique rank with lambda 0.6 694\n",
      "target_item post-critique rank with lambda 0.7 620\n",
      "target_item post-critique rank with lambda 0.8 581\n",
      "target_item post-critique rank with lambda 0.9 534\n",
      "target_item post-critique rank with lambda 1 498\n",
      "cur_iter  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 574\n",
      "target_item post-critique rank with lambda 0.1 586\n",
      "target_item post-critique rank with lambda 0.2 623\n",
      "target_item post-critique rank with lambda 0.3 686\n",
      "target_item post-critique rank with lambda 0.4 588\n",
      "target_item post-critique rank with lambda 0.5 607\n",
      "target_item post-critique rank with lambda 0.6 866\n",
      "target_item post-critique rank with lambda 0.7 818\n",
      "target_item post-critique rank with lambda 0.8 794\n",
      "target_item post-critique rank with lambda 0.9 797\n",
      "target_item post-critique rank with lambda 1 393\n",
      "cur_iter  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 574\n",
      "target_item post-critique rank with lambda 0.1 559\n",
      "target_item post-critique rank with lambda 0.2 548\n",
      "target_item post-critique rank with lambda 0.3 550\n",
      "target_item post-critique rank with lambda 0.4 492\n",
      "target_item post-critique rank with lambda 0.5 501\n",
      "target_item post-critique rank with lambda 0.6 533\n",
      "target_item post-critique rank with lambda 0.7 255\n",
      "target_item post-critique rank with lambda 0.8 254\n",
      "target_item post-critique rank with lambda 0.9 319\n",
      "target_item post-critique rank with lambda 1 320\n",
      "cur_iter  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 574\n",
      "target_item post-critique rank with lambda 0.1 407\n",
      "target_item post-critique rank with lambda 0.2 281\n",
      "target_item post-critique rank with lambda 0.3 136\n",
      "target_item post-critique rank with lambda 0.4 113\n",
      "target_item post-critique rank with lambda 0.5 154\n",
      "target_item post-critique rank with lambda 0.6 94\n",
      "target_item post-critique rank with lambda 0.7 105\n",
      "target_item post-critique rank with lambda 0.8 84\n",
      "target_item post-critique rank with lambda 0.9 75\n",
      "target_item post-critique rank with lambda 1 71\n",
      "cur_iter  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tensorflow_cpu\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_item post-critique rank with lambda 0 574\n",
      "target_item post-critique rank with lambda 0.1 625\n",
      "target_item post-critique rank with lambda 0.2 383\n",
      "target_item post-critique rank with lambda 0.3 436\n",
      "target_item post-critique rank with lambda 0.4 439\n",
      "target_item post-critique rank with lambda 0.5 420\n",
      "target_item post-critique rank with lambda 0.6 256\n",
      "target_item post-critique rank with lambda 0.7 263\n",
      "target_item post-critique rank with lambda 0.8 155\n",
      "target_item post-critique rank with lambda 0.9 133\n",
      "target_item post-critique rank with lambda 1 133\n",
      "target_item:  1620\n",
      "wanted_keyphrases is empty\n"
     ]
    }
   ],
   "source": [
    "# Initialize df for storing the experiment\n",
    "\n",
    "# post_ranki is post rank with different lambda ratio for combining pre-post User similarity matrix \n",
    "columns = ['user_id', 'target_item', 'item_name', 'iter', 'pre_rank', \n",
    "           'top_prediction_item_name','critiqued_keyphrase', 'keyphrase_name', \n",
    "           'post_rank0', \n",
    "           'post_rank1', \n",
    "           'post_rank2', \n",
    "           'post_rank3', \n",
    "           'post_rank4', \n",
    "           'post_rank5', \n",
    "           'post_rank6', \n",
    "           'post_rank7', \n",
    "           'post_rank8',\n",
    "           'post_rank9',\n",
    "           'post_rank10',\n",
    "           'num_existing_keyphrases'] \n",
    "df = pd.DataFrame(columns=columns)\n",
    "row = {}\n",
    "\n",
    "#only_with_critiqued_keyphrase\n",
    "for user in range(300,301):\n",
    "    df = single_step_critiquing_experiment(user = user, \n",
    "                           keyphrase_length_threshold = 230, \n",
    "                           max_iteration_threshold = 5,\n",
    "                           k = 50,\n",
    "                           df = df,\n",
    "                           row = row,\n",
    "                           business_df = business_df,\n",
    "                           keyphrases = keyphrases,\n",
    "                           keyphrase_popularity = keyphrase_popularity,\n",
    "                           keyphrase_selection_method = 'random',\n",
    "                           recommend_type = 'all'\n",
    "                           )\n",
    "df.to_csv(single_step_with_avg_path+\"random_all_50user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>target_item</th>\n",
       "      <th>item_name</th>\n",
       "      <th>iter</th>\n",
       "      <th>pre_rank</th>\n",
       "      <th>top_prediction_item_name</th>\n",
       "      <th>critiqued_keyphrase</th>\n",
       "      <th>keyphrase_name</th>\n",
       "      <th>post_rank0</th>\n",
       "      <th>post_rank1</th>\n",
       "      <th>...</th>\n",
       "      <th>post_rank3</th>\n",
       "      <th>post_rank4</th>\n",
       "      <th>post_rank5</th>\n",
       "      <th>post_rank6</th>\n",
       "      <th>post_rank7</th>\n",
       "      <th>post_rank8</th>\n",
       "      <th>post_rank9</th>\n",
       "      <th>post_rank10</th>\n",
       "      <th>num_existing_keyphrases</th>\n",
       "      <th>post_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>223</td>\n",
       "      <td>general tao</td>\n",
       "      <td>755</td>\n",
       "      <td>765</td>\n",
       "      <td>...</td>\n",
       "      <td>371</td>\n",
       "      <td>268</td>\n",
       "      <td>246</td>\n",
       "      <td>301</td>\n",
       "      <td>249</td>\n",
       "      <td>224</td>\n",
       "      <td>196</td>\n",
       "      <td>176</td>\n",
       "      <td>10</td>\n",
       "      <td>[755, 765, 773, 371, 268, 246, 301, 249, 224, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>1</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>60</td>\n",
       "      <td>squid</td>\n",
       "      <td>755</td>\n",
       "      <td>784</td>\n",
       "      <td>...</td>\n",
       "      <td>756</td>\n",
       "      <td>885</td>\n",
       "      <td>1874</td>\n",
       "      <td>2967</td>\n",
       "      <td>2934</td>\n",
       "      <td>2901</td>\n",
       "      <td>2919</td>\n",
       "      <td>2903</td>\n",
       "      <td>10</td>\n",
       "      <td>[755, 784, 783, 756, 885, 1874, 2967, 2934, 29...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>2</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>115</td>\n",
       "      <td>lobster</td>\n",
       "      <td>755</td>\n",
       "      <td>740</td>\n",
       "      <td>...</td>\n",
       "      <td>600</td>\n",
       "      <td>522</td>\n",
       "      <td>662</td>\n",
       "      <td>688</td>\n",
       "      <td>769</td>\n",
       "      <td>858</td>\n",
       "      <td>929</td>\n",
       "      <td>973</td>\n",
       "      <td>10</td>\n",
       "      <td>[755, 740, 632, 600, 522, 662, 688, 769, 858, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>3</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>72</td>\n",
       "      <td>scallop</td>\n",
       "      <td>755</td>\n",
       "      <td>819</td>\n",
       "      <td>...</td>\n",
       "      <td>944</td>\n",
       "      <td>1364</td>\n",
       "      <td>2166</td>\n",
       "      <td>2105</td>\n",
       "      <td>1946</td>\n",
       "      <td>1955</td>\n",
       "      <td>1933</td>\n",
       "      <td>1923</td>\n",
       "      <td>10</td>\n",
       "      <td>[755, 819, 906, 944, 1364, 2166, 2105, 1946, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>4</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>48</td>\n",
       "      <td>burger</td>\n",
       "      <td>755</td>\n",
       "      <td>784</td>\n",
       "      <td>...</td>\n",
       "      <td>945</td>\n",
       "      <td>2129</td>\n",
       "      <td>2197</td>\n",
       "      <td>2027</td>\n",
       "      <td>3059</td>\n",
       "      <td>4151</td>\n",
       "      <td>4140</td>\n",
       "      <td>2975</td>\n",
       "      <td>10</td>\n",
       "      <td>[755, 784, 875, 945, 2129, 2197, 2027, 3059, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>0</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>38</td>\n",
       "      <td>chocolate</td>\n",
       "      <td>574</td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>547</td>\n",
       "      <td>443</td>\n",
       "      <td>311</td>\n",
       "      <td>694</td>\n",
       "      <td>620</td>\n",
       "      <td>581</td>\n",
       "      <td>534</td>\n",
       "      <td>498</td>\n",
       "      <td>10</td>\n",
       "      <td>[574, 600, 614, 547, 443, 311, 694, 620, 581, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>1</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>42</td>\n",
       "      <td>sandwich</td>\n",
       "      <td>574</td>\n",
       "      <td>586</td>\n",
       "      <td>...</td>\n",
       "      <td>686</td>\n",
       "      <td>588</td>\n",
       "      <td>607</td>\n",
       "      <td>866</td>\n",
       "      <td>818</td>\n",
       "      <td>794</td>\n",
       "      <td>797</td>\n",
       "      <td>393</td>\n",
       "      <td>10</td>\n",
       "      <td>[574, 586, 623, 686, 588, 607, 866, 818, 794, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>2</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>105</td>\n",
       "      <td>latte</td>\n",
       "      <td>574</td>\n",
       "      <td>559</td>\n",
       "      <td>...</td>\n",
       "      <td>550</td>\n",
       "      <td>492</td>\n",
       "      <td>501</td>\n",
       "      <td>533</td>\n",
       "      <td>255</td>\n",
       "      <td>254</td>\n",
       "      <td>319</td>\n",
       "      <td>320</td>\n",
       "      <td>10</td>\n",
       "      <td>[574, 559, 548, 550, 492, 501, 533, 255, 254, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>3</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>65</td>\n",
       "      <td>espresso</td>\n",
       "      <td>574</td>\n",
       "      <td>407</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>113</td>\n",
       "      <td>154</td>\n",
       "      <td>94</td>\n",
       "      <td>105</td>\n",
       "      <td>84</td>\n",
       "      <td>75</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>[574, 407, 281, 136, 113, 154, 94, 105, 84, 75...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>4</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>230</td>\n",
       "      <td>alcoholic beverage</td>\n",
       "      <td>574</td>\n",
       "      <td>625</td>\n",
       "      <td>...</td>\n",
       "      <td>436</td>\n",
       "      <td>439</td>\n",
       "      <td>420</td>\n",
       "      <td>256</td>\n",
       "      <td>263</td>\n",
       "      <td>155</td>\n",
       "      <td>133</td>\n",
       "      <td>133</td>\n",
       "      <td>10</td>\n",
       "      <td>[574, 625, 383, 436, 439, 420, 256, 263, 155, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id target_item                   item_name iter pre_rank  \\\n",
       "0     300         886  b'Wok & Roast Chinese BBQ'    0      755   \n",
       "1     300         886  b'Wok & Roast Chinese BBQ'    1      755   \n",
       "2     300         886  b'Wok & Roast Chinese BBQ'    2      755   \n",
       "3     300         886  b'Wok & Roast Chinese BBQ'    3      755   \n",
       "4     300         886  b'Wok & Roast Chinese BBQ'    4      755   \n",
       "5     300         968          b'Porter Airlines'    0      574   \n",
       "6     300         968          b'Porter Airlines'    1      574   \n",
       "7     300         968          b'Porter Airlines'    2      574   \n",
       "8     300         968          b'Porter Airlines'    3      574   \n",
       "9     300         968          b'Porter Airlines'    4      574   \n",
       "\n",
       "  top_prediction_item_name critiqued_keyphrase      keyphrase_name post_rank0  \\\n",
       "0         b'Khao San Road'                 223         general tao        755   \n",
       "1         b'Khao San Road'                  60               squid        755   \n",
       "2         b'Khao San Road'                 115             lobster        755   \n",
       "3         b'Khao San Road'                  72             scallop        755   \n",
       "4         b'Khao San Road'                  48              burger        755   \n",
       "5         b'Khao San Road'                  38           chocolate        574   \n",
       "6         b'Khao San Road'                  42            sandwich        574   \n",
       "7         b'Khao San Road'                 105               latte        574   \n",
       "8         b'Khao San Road'                  65            espresso        574   \n",
       "9         b'Khao San Road'                 230  alcoholic beverage        574   \n",
       "\n",
       "  post_rank1                        ...                         post_rank3  \\\n",
       "0        765                        ...                                371   \n",
       "1        784                        ...                                756   \n",
       "2        740                        ...                                600   \n",
       "3        819                        ...                                944   \n",
       "4        784                        ...                                945   \n",
       "5        600                        ...                                547   \n",
       "6        586                        ...                                686   \n",
       "7        559                        ...                                550   \n",
       "8        407                        ...                                136   \n",
       "9        625                        ...                                436   \n",
       "\n",
       "  post_rank4 post_rank5 post_rank6 post_rank7 post_rank8 post_rank9  \\\n",
       "0        268        246        301        249        224        196   \n",
       "1        885       1874       2967       2934       2901       2919   \n",
       "2        522        662        688        769        858        929   \n",
       "3       1364       2166       2105       1946       1955       1933   \n",
       "4       2129       2197       2027       3059       4151       4140   \n",
       "5        443        311        694        620        581        534   \n",
       "6        588        607        866        818        794        797   \n",
       "7        492        501        533        255        254        319   \n",
       "8        113        154         94        105         84         75   \n",
       "9        439        420        256        263        155        133   \n",
       "\n",
       "  post_rank10 num_existing_keyphrases  \\\n",
       "0         176                      10   \n",
       "1        2903                      10   \n",
       "2         973                      10   \n",
       "3        1923                      10   \n",
       "4        2975                      10   \n",
       "5         498                      10   \n",
       "6         393                      10   \n",
       "7         320                      10   \n",
       "8          71                      10   \n",
       "9         133                      10   \n",
       "\n",
       "                                           post_rank  \n",
       "0  [755, 765, 773, 371, 268, 246, 301, 249, 224, ...  \n",
       "1  [755, 784, 783, 756, 885, 1874, 2967, 2934, 29...  \n",
       "2  [755, 740, 632, 600, 522, 662, 688, 769, 858, ...  \n",
       "3  [755, 819, 906, 944, 1364, 2166, 2105, 1946, 1...  \n",
       "4  [755, 784, 875, 945, 2129, 2197, 2027, 3059, 4...  \n",
       "5  [574, 600, 614, 547, 443, 311, 694, 620, 581, ...  \n",
       "6  [574, 586, 623, 686, 588, 607, 866, 818, 794, ...  \n",
       "7  [574, 559, 548, 550, 492, 501, 533, 255, 254, ...  \n",
       "8  [574, 407, 281, 136, 113, 154, 94, 105, 84, 75...  \n",
       "9  [574, 625, 383, 436, 439, 420, 256, 263, 155, ...  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>target_item</th>\n",
       "      <th>item_name</th>\n",
       "      <th>iter</th>\n",
       "      <th>pre_rank</th>\n",
       "      <th>top_prediction_item_name</th>\n",
       "      <th>critiqued_keyphrase</th>\n",
       "      <th>keyphrase_name</th>\n",
       "      <th>post_rank0</th>\n",
       "      <th>post_rank1</th>\n",
       "      <th>...</th>\n",
       "      <th>post_rank3</th>\n",
       "      <th>post_rank4</th>\n",
       "      <th>post_rank5</th>\n",
       "      <th>post_rank6</th>\n",
       "      <th>post_rank7</th>\n",
       "      <th>post_rank8</th>\n",
       "      <th>post_rank9</th>\n",
       "      <th>post_rank10</th>\n",
       "      <th>num_existing_keyphrases</th>\n",
       "      <th>post_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>145</td>\n",
       "      <td>markham</td>\n",
       "      <td>139</td>\n",
       "      <td>179</td>\n",
       "      <td>...</td>\n",
       "      <td>323</td>\n",
       "      <td>510</td>\n",
       "      <td>2321</td>\n",
       "      <td>2350</td>\n",
       "      <td>2297</td>\n",
       "      <td>2298</td>\n",
       "      <td>2298</td>\n",
       "      <td>2295</td>\n",
       "      <td>10</td>\n",
       "      <td>[139, 179, 264, 323, 510, 2321, 2350, 2297, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>1</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>72</td>\n",
       "      <td>scallop</td>\n",
       "      <td>141</td>\n",
       "      <td>163</td>\n",
       "      <td>...</td>\n",
       "      <td>206</td>\n",
       "      <td>255</td>\n",
       "      <td>320</td>\n",
       "      <td>316</td>\n",
       "      <td>320</td>\n",
       "      <td>328</td>\n",
       "      <td>328</td>\n",
       "      <td>332</td>\n",
       "      <td>10</td>\n",
       "      <td>[141, 163, 179, 206, 255, 320, 316, 320, 328, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>2</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>59</td>\n",
       "      <td>ice cream</td>\n",
       "      <td>285</td>\n",
       "      <td>311</td>\n",
       "      <td>...</td>\n",
       "      <td>389</td>\n",
       "      <td>409</td>\n",
       "      <td>471</td>\n",
       "      <td>477</td>\n",
       "      <td>473</td>\n",
       "      <td>505</td>\n",
       "      <td>523</td>\n",
       "      <td>2321</td>\n",
       "      <td>10</td>\n",
       "      <td>[285, 311, 342, 389, 409, 471, 477, 473, 505, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>3</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>48</td>\n",
       "      <td>burger</td>\n",
       "      <td>241</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>325</td>\n",
       "      <td>569</td>\n",
       "      <td>588</td>\n",
       "      <td>585</td>\n",
       "      <td>2340</td>\n",
       "      <td>2329</td>\n",
       "      <td>2331</td>\n",
       "      <td>2330</td>\n",
       "      <td>10</td>\n",
       "      <td>[241, 256, 301, 325, 569, 588, 585, 2340, 2329...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>4</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>60</td>\n",
       "      <td>squid</td>\n",
       "      <td>139</td>\n",
       "      <td>160</td>\n",
       "      <td>...</td>\n",
       "      <td>157</td>\n",
       "      <td>188</td>\n",
       "      <td>293</td>\n",
       "      <td>2205</td>\n",
       "      <td>2264</td>\n",
       "      <td>2235</td>\n",
       "      <td>2237</td>\n",
       "      <td>2235</td>\n",
       "      <td>10</td>\n",
       "      <td>[139, 160, 158, 157, 188, 293, 2205, 2264, 223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>0</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>103</td>\n",
       "      <td>fruit</td>\n",
       "      <td>255</td>\n",
       "      <td>185</td>\n",
       "      <td>...</td>\n",
       "      <td>170</td>\n",
       "      <td>148</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>[255, 185, 199, 170, 148, 36, 24, 20, 16, 24, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>1</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>230</td>\n",
       "      <td>alcoholic beverage</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>[25, 30, 23, 27, 33, 33, 26, 23, 19, 16, 17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>2</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>105</td>\n",
       "      <td>latte</td>\n",
       "      <td>335</td>\n",
       "      <td>333</td>\n",
       "      <td>...</td>\n",
       "      <td>333</td>\n",
       "      <td>305</td>\n",
       "      <td>307</td>\n",
       "      <td>330</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>220</td>\n",
       "      <td>219</td>\n",
       "      <td>10</td>\n",
       "      <td>[335, 333, 331, 333, 305, 307, 330, 182, 183, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>3</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>65</td>\n",
       "      <td>espresso</td>\n",
       "      <td>117</td>\n",
       "      <td>84</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>41</td>\n",
       "      <td>67</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>[117, 84, 66, 38, 41, 67, 48, 49, 43, 39, 36]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>300</td>\n",
       "      <td>968</td>\n",
       "      <td>b'Porter Airlines'</td>\n",
       "      <td>4</td>\n",
       "      <td>574</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>42</td>\n",
       "      <td>sandwich</td>\n",
       "      <td>267</td>\n",
       "      <td>279</td>\n",
       "      <td>...</td>\n",
       "      <td>318</td>\n",
       "      <td>292</td>\n",
       "      <td>308</td>\n",
       "      <td>406</td>\n",
       "      <td>380</td>\n",
       "      <td>371</td>\n",
       "      <td>368</td>\n",
       "      <td>214</td>\n",
       "      <td>10</td>\n",
       "      <td>[267, 279, 296, 318, 292, 308, 406, 380, 371, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id target_item                   item_name iter pre_rank  \\\n",
       "0     300         886  b'Wok & Roast Chinese BBQ'    0      755   \n",
       "1     300         886  b'Wok & Roast Chinese BBQ'    1      755   \n",
       "2     300         886  b'Wok & Roast Chinese BBQ'    2      755   \n",
       "3     300         886  b'Wok & Roast Chinese BBQ'    3      755   \n",
       "4     300         886  b'Wok & Roast Chinese BBQ'    4      755   \n",
       "5     300         968          b'Porter Airlines'    0      574   \n",
       "6     300         968          b'Porter Airlines'    1      574   \n",
       "7     300         968          b'Porter Airlines'    2      574   \n",
       "8     300         968          b'Porter Airlines'    3      574   \n",
       "9     300         968          b'Porter Airlines'    4      574   \n",
       "\n",
       "  top_prediction_item_name critiqued_keyphrase      keyphrase_name post_rank0  \\\n",
       "0         b'Khao San Road'                 145             markham        139   \n",
       "1         b'Khao San Road'                  72             scallop        141   \n",
       "2         b'Khao San Road'                  59           ice cream        285   \n",
       "3         b'Khao San Road'                  48              burger        241   \n",
       "4         b'Khao San Road'                  60               squid        139   \n",
       "5         b'Khao San Road'                 103               fruit        255   \n",
       "6         b'Khao San Road'                 230  alcoholic beverage         25   \n",
       "7         b'Khao San Road'                 105               latte        335   \n",
       "8         b'Khao San Road'                  65            espresso        117   \n",
       "9         b'Khao San Road'                  42            sandwich        267   \n",
       "\n",
       "  post_rank1                        ...                         post_rank3  \\\n",
       "0        179                        ...                                323   \n",
       "1        163                        ...                                206   \n",
       "2        311                        ...                                389   \n",
       "3        256                        ...                                325   \n",
       "4        160                        ...                                157   \n",
       "5        185                        ...                                170   \n",
       "6         30                        ...                                 27   \n",
       "7        333                        ...                                333   \n",
       "8         84                        ...                                 38   \n",
       "9        279                        ...                                318   \n",
       "\n",
       "  post_rank4 post_rank5 post_rank6 post_rank7 post_rank8 post_rank9  \\\n",
       "0        510       2321       2350       2297       2298       2298   \n",
       "1        255        320        316        320        328        328   \n",
       "2        409        471        477        473        505        523   \n",
       "3        569        588        585       2340       2329       2331   \n",
       "4        188        293       2205       2264       2235       2237   \n",
       "5        148         36         24         20         16         24   \n",
       "6         33         33         26         23         19         16   \n",
       "7        305        307        330        182        183        220   \n",
       "8         41         67         48         49         43         39   \n",
       "9        292        308        406        380        371        368   \n",
       "\n",
       "  post_rank10 num_existing_keyphrases  \\\n",
       "0        2295                      10   \n",
       "1         332                      10   \n",
       "2        2321                      10   \n",
       "3        2330                      10   \n",
       "4        2235                      10   \n",
       "5          23                      10   \n",
       "6          17                      10   \n",
       "7         219                      10   \n",
       "8          36                      10   \n",
       "9         214                      10   \n",
       "\n",
       "                                           post_rank  \n",
       "0  [139, 179, 264, 323, 510, 2321, 2350, 2297, 22...  \n",
       "1  [141, 163, 179, 206, 255, 320, 316, 320, 328, ...  \n",
       "2  [285, 311, 342, 389, 409, 471, 477, 473, 505, ...  \n",
       "3  [241, 256, 301, 325, 569, 588, 585, 2340, 2329...  \n",
       "4  [139, 160, 158, 157, 188, 293, 2205, 2264, 223...  \n",
       "5  [255, 185, 199, 170, 148, 36, 24, 20, 16, 24, 23]  \n",
       "6       [25, 30, 23, 27, 33, 33, 26, 23, 19, 16, 17]  \n",
       "7  [335, 333, 331, 333, 305, 307, 330, 182, 183, ...  \n",
       "8      [117, 84, 66, 38, 41, 67, 48, 49, 43, 39, 36]  \n",
       "9  [267, 279, 296, 318, 292, 308, 406, 380, 371, ...  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot HR vs. Performance for 3 keyphrase selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utlilty func\n",
    "def hr_at_k(df, k):\n",
    "    \"\"\"\n",
    "    Given the above dataframe, calculate the avg pre and post hit rate at k \n",
    "    \"\"\"\n",
    "    pre_hit = np.where(df['pre_rank']<k)[0]\n",
    "    post_hit = np.where(df['post_rank']<k)[0]\n",
    "    pre_hr = len(pre_hit)/len(df)\n",
    "    post_hr = len(post_hit)/len(df)\n",
    "    return pre_hr, post_hr\n",
    "\n",
    "def get_hr(l=5, rang = 200):\n",
    "    \"\"\"\n",
    "    Get the hit rate at different rang, with different lambda value\n",
    "    Output in the form of list\n",
    "    \"\"\"\n",
    "    pre_hr_list = []\n",
    "    post_hr_list = []\n",
    "    for k in range(1,rang):\n",
    "        pre_hr,post_hr = hr_at_k(df,4, k)\n",
    "        pre_hr_list.append(pre_hr)\n",
    "        post_hr_list.append(post_hr)\n",
    "    return pre_hr_list,post_hr_list\n",
    "\n",
    "def get_hr_of_all_lambda(methods):\n",
    "    for method in methods:\n",
    "        post_rates = []\n",
    "        for i in range(9):\n",
    "            _,a = get_hr(l=i)\n",
    "            diff.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_hr_list,b = get_hr(l=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hr_performance():\n",
    "    plt.figure(figsize=(12,12))\n",
    "    for \n",
    "    plt.plot(np.arange(len(pre_hr_list)), pre_hr_list)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a1)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a2)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a3)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a4)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a5)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a6)\n",
    "\n",
    "    # plt.xticks(np.arange(len(k_list)), k_list)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('HR@K')\n",
    "    plt.legend(['Pre-Critiquing','Random', 'Random_Upper','Pop','Pop_Upper','Diff','Diff_Upper'])\n",
    "    plt.show()\n",
    "    plt.savefig('../figs/three_keyphrase_selection_methods_with_upper_bound_0104')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
