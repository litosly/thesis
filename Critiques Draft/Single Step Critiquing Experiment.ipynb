{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "# sys.path.clear()\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu\\\\python36.zip')\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu\\\\DLLs')\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu\\\\lib')\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu')\n",
    "sys.path.insert(0, 'D:\\\\Anaconda\\\\envs\\\\tensorflow_cpu\\\\lib\\\\site-packages')\n",
    "sys.path.insert(0, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, load_npz, save_npz\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import yaml\n",
    "import scipy.sparse as sparse\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Original Data\n",
    "df_train = pd.read_csv('../../data/yelp/Train.csv',encoding='latin-1')\n",
    "df_valid = pd.read_csv('../../data/yelp/Valid.csv',encoding='latin-1')\n",
    "df_test = pd.read_csv('../../data/yelp/Test.csv',encoding='latin-1')\n",
    "keyphrases = pd.read_csv('../../data/yelp/KeyPhrases.csv')['Phrases'].tolist()\n",
    "keyphrase_popularity = np.loadtxt('../data/yelp/'+'keyphrase_popularity.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load U-I Data \n",
    "rtrain = load_npz(\"../../data/yelp/Rtrain.npz\")\n",
    "rvalid = load_npz(\"../../data/yelp/Rvalid.npz\")\n",
    "rtest = load_npz(\"../../data/yelp/Rtest.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load user/item keyphrase data\n",
    "U_K = load_npz(\"../../data/yelp/U_K.npz\")\n",
    "I_K = load_npz(\"../../data/yelp/I_K.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def train(matrix_train):\n",
    "    similarity = cosine_similarity(X=matrix_train, Y=None, dense_output=True)\n",
    "    return similarity\n",
    "\n",
    "def get_I_K(df, row_name = 'ItemIndex', shape = (3668,75)):\n",
    "    rows = []\n",
    "    cols = []\n",
    "    vals = []\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        key_vector = literal_eval(df['keyVector'][i])\n",
    "        rows.extend([df[row_name][i]]*len(key_vector)) ## Item index\n",
    "        cols.extend(key_vector) ## Keyword Index\n",
    "        vals.extend(np.array([1]*len(key_vector)))\n",
    "    return csr_matrix((vals, (rows, cols)), shape=shape)\n",
    "\n",
    "\n",
    "\n",
    "def prediction(prediction_score, topK, matrix_Train):\n",
    "\n",
    "    prediction = []\n",
    "\n",
    "    for user_index in tqdm(range(matrix_Train.shape[0])):\n",
    "        vector_u = prediction_score[user_index]\n",
    "        vector_train = matrix_Train[user_index]\n",
    "        if len(vector_train.nonzero()[0]) > 0:\n",
    "            vector_predict = sub_routine(vector_u, vector_train, topK=topK)\n",
    "        else:\n",
    "            vector_predict = np.zeros(topK, dtype=np.float32)\n",
    "\n",
    "        prediction.append(vector_predict)\n",
    "\n",
    "    return np.vstack(prediction)\n",
    "\n",
    "\n",
    "def sub_routine(vector_u, vector_train, topK=500):\n",
    "\n",
    "    train_index = vector_train.nonzero()[1]\n",
    "\n",
    "    vector_u = vector_u\n",
    "\n",
    "    candidate_index = np.argpartition(-vector_u, topK+len(train_index))[:topK+len(train_index)]\n",
    "    vector_u = candidate_index[vector_u[candidate_index].argsort()[::-1]]\n",
    "    vector_u = np.delete(vector_u, np.isin(vector_u, train_index).nonzero()[0])\n",
    "\n",
    "    return vector_u[:topK]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(matrix_train, k, similarity, item_similarity_en = False):\n",
    "    \"\"\"\n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    \"\"\"\n",
    "    prediction_scores = []\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        matrix_train = matrix_train.transpose()\n",
    "        \n",
    "    for user_index in tqdm(range(matrix_train.shape[0])):\n",
    "        # Get user u's prediction scores to all users\n",
    "        vector_u = similarity[user_index]\n",
    "\n",
    "        # Get closest K neighbors excluding user u self\n",
    "        similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "        # Get neighbors similarity weights and ratings\n",
    "        similar_users_weights = similarity[user_index][similar_users]\n",
    "        similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "\n",
    "        prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "\n",
    "        prediction_scores.append(np.sum(prediction_scores_u, axis=0))\n",
    "    res = np.array(prediction_scores)\n",
    "    \n",
    "    if item_similarity_en:\n",
    "        res = res.transpose()\n",
    "    \n",
    "    return res\n",
    "\n",
    "def predict_vector(user_index, matrix_train, k, similarity):\n",
    "    \"\"\"\n",
    "    res = similarity * matrix_train    if item_similarity_en = False\n",
    "    res = similarity * matrix_train.T  if item_similarity_en = True\n",
    "    get only user_index row\n",
    "    \"\"\"\n",
    "    vector_u = similarity[user_index]\n",
    "    \n",
    "    # Get closest K neighbors excluding user u self\n",
    "    similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "    # Get neighbors similarity weights and ratings\n",
    "    similar_users_weights = similarity[user_index][similar_users]\n",
    "    similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "    prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "    \n",
    "    return np.sum(prediction_scores_u, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evluation \n",
    "def recallk(vector_true_dense, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "def precisionk(vector_predict, hits, **unused):\n",
    "    hits = len(hits.nonzero()[0])\n",
    "    return float(hits)/len(vector_predict)\n",
    "\n",
    "\n",
    "def average_precisionk(vector_predict, hits, **unused):\n",
    "    precisions = np.cumsum(hits, dtype=np.float32)/range(1, len(vector_predict)+1)\n",
    "    return np.mean(precisions)\n",
    "\n",
    "\n",
    "def r_precision(vector_true_dense, vector_predict, **unused):\n",
    "    vector_predict_short = vector_predict[:len(vector_true_dense)]\n",
    "    hits = len(np.isin(vector_predict_short, vector_true_dense).nonzero()[0])\n",
    "    return float(hits)/len(vector_true_dense)\n",
    "\n",
    "\n",
    "def _dcg_support(size):\n",
    "    arr = np.arange(1, size+1)+1\n",
    "    return 1./np.log2(arr)\n",
    "\n",
    "\n",
    "def ndcg(vector_true_dense, vector_predict, hits):\n",
    "    idcg = np.sum(_dcg_support(len(vector_true_dense)))\n",
    "    dcg_base = _dcg_support(len(vector_predict))\n",
    "    dcg_base[np.logical_not(hits)] = 0\n",
    "    dcg = np.sum(dcg_base)\n",
    "    return dcg/idcg\n",
    "\n",
    "\n",
    "def click(hits, **unused):\n",
    "    first_hit = next((i for i, x in enumerate(hits) if x), None)\n",
    "    if first_hit is None:\n",
    "        return 5\n",
    "    else:\n",
    "        return first_hit/10\n",
    "\n",
    "\n",
    "def evaluate(matrix_Predict, matrix_Test, metric_names =['R-Precision', 'NDCG', 'Precision', 'Recall', 'MAP'], atK = [5, 10, 15, 20, 50], analytical=False):\n",
    "    \"\"\"\n",
    "    :param matrix_U: Latent representations of users, for LRecs it is RQ, for ALSs it is U\n",
    "    :param matrix_V: Latent representations of items, for LRecs it is Q, for ALSs it is V\n",
    "    :param matrix_Train: Rating matrix for training, features.\n",
    "    :param matrix_Test: Rating matrix for evaluation, true labels.\n",
    "    :param k: Top K retrieval\n",
    "    :param metric_names: Evaluation metrics\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global_metrics = {\n",
    "        \"R-Precision\": r_precision,\n",
    "        \"NDCG\": ndcg,\n",
    "        \"Clicks\": click\n",
    "    }\n",
    "\n",
    "    local_metrics = {\n",
    "        \"Precision\": precisionk,\n",
    "        \"Recall\": recallk,\n",
    "        \"MAP\": average_precisionk\n",
    "    }\n",
    "\n",
    "    output = dict()\n",
    "\n",
    "    num_users = matrix_Predict.shape[0]\n",
    "\n",
    "    for k in atK:\n",
    "\n",
    "        local_metric_names = list(set(metric_names).intersection(local_metrics.keys()))\n",
    "        results = {name: [] for name in local_metric_names}\n",
    "        topK_Predict = matrix_Predict[:, :k]\n",
    "\n",
    "        for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "            vector_predict = topK_Predict[user_index]\n",
    "            if len(vector_predict.nonzero()[0]) > 0:\n",
    "                vector_true = matrix_Test[user_index]\n",
    "                vector_true_dense = vector_true.nonzero()[1]\n",
    "                hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "                if vector_true_dense.size > 0:\n",
    "                    for name in local_metric_names:\n",
    "                        results[name].append(local_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                                 vector_predict=vector_predict,\n",
    "                                                                 hits=hits))\n",
    "\n",
    "        results_summary = dict()\n",
    "        if analytical:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = results[name]\n",
    "        else:\n",
    "            for name in local_metric_names:\n",
    "                results_summary['{0}@{1}'.format(name, k)] = (np.average(results[name]),\n",
    "                                                              1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "        output.update(results_summary)\n",
    "\n",
    "    global_metric_names = list(set(metric_names).intersection(global_metrics.keys()))\n",
    "    results = {name: [] for name in global_metric_names}\n",
    "\n",
    "    topK_Predict = matrix_Predict[:]\n",
    "\n",
    "    for user_index in tqdm(range(topK_Predict.shape[0])):\n",
    "        vector_predict = topK_Predict[user_index]\n",
    "\n",
    "        if len(vector_predict.nonzero()[0]) > 0:\n",
    "            vector_true = matrix_Test[user_index]\n",
    "            vector_true_dense = vector_true.nonzero()[1]\n",
    "            hits = np.isin(vector_predict, vector_true_dense)\n",
    "\n",
    "            # if user_index == 1:\n",
    "            #     import ipdb;\n",
    "            #     ipdb.set_trace()\n",
    "\n",
    "            if vector_true_dense.size > 0:\n",
    "                for name in global_metric_names:\n",
    "                    results[name].append(global_metrics[name](vector_true_dense=vector_true_dense,\n",
    "                                                              vector_predict=vector_predict,\n",
    "                                                              hits=hits))\n",
    "\n",
    "    results_summary = dict()\n",
    "    if analytical:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = results[name]\n",
    "    else:\n",
    "        for name in global_metric_names:\n",
    "            results_summary[name] = (np.average(results[name]), 1.96*np.std(results[name])/np.sqrt(num_users))\n",
    "    output.update(results_summary)\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Initial Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Item KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:07<00:00, 311.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 2780.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4753.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4859.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4776.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4886.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4836.14it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2343/2343 [00:00<00:00, 4166.57it/s]\n"
     ]
    }
   ],
   "source": [
    "similarity = normalize(train(rtrain))\n",
    "user_item_prediction_score = predict(rtrain, 100, similarity, item_similarity_en= False)\n",
    "user_item_predict = prediction(user_item_prediction_score, 50, rtrain)\n",
    "user_item_res = evaluate(user_item_predict, rtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MAP@10': (0.06333952750429245, 0.00455145183026277),\n",
       " 'MAP@15': (0.05872249612550844, 0.0038121823597348156),\n",
       " 'MAP@20': (0.055196280875748356, 0.003345258771111864),\n",
       " 'MAP@5': (0.06940666362391602, 0.0060646277121995185),\n",
       " 'MAP@50': (0.04436838784245958, 0.0022020570312340938),\n",
       " 'NDCG': (0.09071795198195, 0.003803970590016347),\n",
       " 'Precision@10': (0.05330899132816066, 0.0032544740534870857),\n",
       " 'Precision@15': (0.04698006998326487, 0.0025622684368576416),\n",
       " 'Precision@20': (0.043336376083979916, 0.002222006808632301),\n",
       " 'Precision@5': (0.06462802373345505, 0.004754606217931856),\n",
       " 'Precision@50': (0.032889091738931994, 0.0014317314500480152),\n",
       " 'R-Precision': (0.048464138894968055, 0.0027869069242192506),\n",
       " 'Recall@10': (0.04269615369598775, 0.0027831077562657665),\n",
       " 'Recall@15': (0.05562887733868642, 0.0031389991916236284),\n",
       " 'Recall@20': (0.0677664821917642, 0.003424484045821138),\n",
       " 'Recall@5': (0.026408137823500974, 0.002191309117794643),\n",
       " 'Recall@50': (0.12696642809611336, 0.0048241883971306436)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k = 100 \n",
    "user_item_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Similarity Matrix Learned with Linear Regression¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "X = normalize(U_K.todense())\n",
    "y = normalize(train(rtrain))\n",
    "clf = Ridge(alpha=0.1).fit(X, y) # Optimality at L2 regularization = 0.1\n",
    "lr_similarity = clf.predict(np.array(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediciting\n",
    "similarity = lr_similarity\n",
    "lr_prediction_score = predict(rtrain, 100, similarity, item_similarity_en= False)\n",
    "lr_predict = prediction(lr_prediction_score, 50, rtrain)\n",
    "lr_res = evaluate(lr_predict, rtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# k = 100\n",
    "lr_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critiquing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encoding of critiquing\n",
    "def get_critiqued_UK(user_keyphrase_frequency,user_index,critiqued_keyphrase):\n",
    "    \"\"\"\n",
    "    user_keyphrase_frequency is the U_K matrix (csr sparse matrix)\n",
    "    return the one-hot encoding of the critique\n",
    "    \"\"\"\n",
    "    U_K_cp = user_keyphrase_frequency.copy()\n",
    "    U_K_cp[user_index] = 0\n",
    "    U_K_cp[user_index,critiqued_keyphrase] = 1\n",
    "    return U_K_cp\n",
    "\n",
    "def project_one_hot_encoding(reg, user_keyphrase_frequency,user_index = 0,critiqued_keyphrase = 0, normalize_en = True):\n",
    "    \"\"\"\n",
    "    Return the projection on user_sim space from one-hot encoding of critiqued keyphrase\n",
    "    The res[user_index] should be target embedding row\n",
    "    \"\"\"\n",
    "    critiqued_matrix = get_critiqued_UK(user_keyphrase_frequency, user_index, critiqued_keyphrase)\n",
    "    res = reg.predict(critiqued_matrix)\n",
    "    if normalize_en:\n",
    "        res = normalize((res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_initial_predictions(X = normalize(U_K.todense()), y = normalize(train(rtrain)),\n",
    "                            matrix_Train = rtrain, k = 100):\n",
    "    clf = Ridge(alpha=0.1).fit(X, y)\n",
    "    similarity = normalize(train(matrix_Train))\n",
    "    user_item_prediction_score = predict(matrix_Train, k, similarity, item_similarity_en= False)\n",
    "    return user_item_prediction_score, clf\n",
    "def get_valid_keyphrases(keyphrase_freq,top_recommendations,item = None,threshold=50,mutiple_keyphrases_en = False, top_items = None):\n",
    "    \"\"\"\n",
    "    Wrapper function to get either top 1 or top n keyphrases\n",
    "    \"\"\"\n",
    "    if mutiple_keyphrases_en:\n",
    "        top_keyphrases = []\n",
    "        for item in top_items:\n",
    "            top_keyphrases.extend(get_valid_keyphrases_for_one_item(keyphrase_freq,top_recommendations,item,threshold=threshold))\n",
    "        return np.ravel(list(set(top_keyphrases))) # remove duplicate and reformat to np array\n",
    "    else:\n",
    "        return get_valid_keyphrases_for_one_item(keyphrase_freq,top_recommendations,item,threshold=threshold)\n",
    "\n",
    "def get_valid_keyphrases_for_one_item(keyphrase_freq,top_recommendations, item,threshold=50):\n",
    "    \"\"\"\n",
    "    Get keyphrases of item that make sense\n",
    "    E.g. if the item has fewer than threshold=50 keyphrases, get all of them\n",
    "    otherwise get top 50 keyphrases\n",
    "    \"\"\"\n",
    "    keyphrase_length = len(keyphrase_freq[item].nonzero()[1])\n",
    "    if keyphrase_length<threshold:\n",
    "        return keyphrase_freq[item].nonzero()[1]\n",
    "    else:\n",
    "        keyphrases = np.ravel(keyphrase_freq[top_recommendations[0]].todense())\n",
    "        top_keyphrases = np.argsort(keyphrases)[::-1][:threshold]\n",
    "        return top_keyphrases\n",
    "    \n",
    "def predict_vector(user_index, matrix_train, k, similarity, with_keyphrase = False, \n",
    "                   keyphrase_freq = None, critiqued_keyphrase = None, alpha = 0):\n",
    "    \"\"\"\n",
    "    get only user_index row\n",
    "    if with_keyphrase = True, then penalize items without critiqued_keyphrase to alpha (default = 0)\n",
    "    \"\"\"\n",
    "    vector_u = similarity[user_index]\n",
    "    \n",
    "    # Get closest K neighbors excluding user u self\n",
    "    similar_users = vector_u.argsort()[::-1][1:k+1]\n",
    "    # Get neighbors similarity weights and ratings\n",
    "    similar_users_weights = similarity[user_index][similar_users]\n",
    "    similar_users_ratings = matrix_train[similar_users].toarray()\n",
    "    \n",
    "    prediction_scores_u = similar_users_ratings * similar_users_weights[:, np.newaxis]\n",
    "    \n",
    "    if with_keyphrase == False:\n",
    "        return np.sum(prediction_scores_u, axis=0)\n",
    "    \n",
    "    # Only Predict items with critiqued_keyphrase \n",
    "    else:\n",
    "        prediction_scores = np.sum(prediction_scores_u, axis=0)\n",
    "#         print (prediction_scores)\n",
    "        #penalize items without critiqued keyphrase\n",
    "        items_with_keyphrase = np.ravel(keyphrase_freq.T[critiqued_keyphrase].nonzero()[1])\n",
    "#         print (items_with_keyphrase)\n",
    "        #Return the unique values in ar1 that are not in ar2.\n",
    "        items_without_keyphrase = np.setdiff1d(np.arange(matrix_train.shape[1]), items_with_keyphrase)\n",
    "        prediction_scores[items_without_keyphrase] = alpha # penalize\n",
    "        return prediction_scores\n",
    "#         print (prediction_scores)\n",
    "#         return prediction_scores/sum(prediction_scores)\n",
    "\n",
    "    \n",
    "def get_initial_prediction(user,X = normalize(U_K.todense()), y = normalize(train(rtrain)),\n",
    "                            matrix_Train = rtrain, k = 100):\n",
    "    \"\"\"\n",
    "    Get the initial knn predictions before critiquing pipelines\n",
    "    get the linear regression model for critiquing embedding (W_2)\n",
    "    get the initial user similarity matrix \n",
    "    k here is the parameter for KNN\n",
    "    \"\"\"\n",
    "    clf = Ridge(alpha=0.1).fit(X, y)\n",
    "    similarity = normalize(train(matrix_Train))\n",
    "    user_item_prediction_score = predict_vector(user, matrix_Train, k, similarity)\n",
    "    return user_item_prediction_score, clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For keyphrase selecting method # 3 \"diff\" \n",
    "def get_item_keyphrase_freq(keyphrase_freq,item):\n",
    "    \"\"\"\n",
    "    Get item's keyphrase frequency \n",
    "    \"\"\"\n",
    "    count = keyphrase_freq[item].todense()\n",
    "    return count/np.sum(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for getting restaurant info from ItemIndex\n",
    "def get_business_df(path = \"../../data/yelp/business.json\" ):\n",
    "    with open(path,encoding=\"utf8\") as json_file:\n",
    "        data = json_file.readlines()\n",
    "        data = list(map(json.loads, data))\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_restaurant_info(business_df, business_id, name = True, review_count = True, stars = True ):\n",
    "    output_list = {}\n",
    "    row_idx = int(business_df.index[business_df['business_id'] == business_id].tolist()[0])\n",
    "    if name == True:\n",
    "        output_list['name'] = business_df['name'][row_idx].encode('utf-8').strip()\n",
    "    if review_count == True:\n",
    "        output_list['review_count'] = business_df['review_count'][row_idx]\n",
    "    if stars == True:\n",
    "        output_list['stars'] = business_df['stars'][row_idx] \n",
    "    return output_list\n",
    "\n",
    "# def get_businessid_from_Itemindex(ItemIndex_list, itemindex):\n",
    "#     return ItemIndex_list['business_id'].tolist()[itemindex]\n",
    "\n",
    "def get_restaurant_name(df_train, business_df, ItemIndex):\n",
    "    rows = np.where(df_train['ItemIndex'] == ItemIndex)\n",
    "    if len(rows)!= 0:\n",
    "        business_id = df_train.loc[rows[0][0]]['business_id']\n",
    "        item_info = get_restaurant_info(business_df, business_id)\n",
    "        return item_info['name']\n",
    "    return \"NOT_FOUND\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_keyphrase_popularity(df,keyphrases):\n",
    "    \"\"\"\n",
    "    Get keyphrase popularity (count) from dataframe\n",
    "    \"\"\"\n",
    "    keyphrase_popularity = np.zeros(len(keyphrases)) #initialize\n",
    "    for i in range(len(df)):\n",
    "        keyphrase_vector = literal_eval(df['keyVector'][i])\n",
    "        keyphrase_popularity[keyphrase_vector] += 1 # count\n",
    "    return keyphrase_popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# keyphrase_popularity = get_keyphrase_popularity(df_train,keyphrases)\n",
    "\n",
    "# Save and load\n",
    "# np.savetxt('../data/yelp/'+'keyphrase_popularity.txt', keyphrase_popularity, fmt='%d')\n",
    "keyphrase_popularity = np.loadtxt('../data/yelp/'+'keyphrase_popularity.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_df = get_business_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_df.to_csv('../../data/yelp/business_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize df for storing the experiment\n",
    "\n",
    "# post_ranki is post rank with different lambda ratio for combining pre-post User similarity matrix \n",
    "columns = ['user_id', 'target_item', 'item_name', 'iter', 'pre_rank', \n",
    "           'top_prediction_item_name','critiqued_keyphrase', 'keyphrase_name', \n",
    "           'post_rank0', \n",
    "           'post_rank1', \n",
    "           'post_rank2', \n",
    "           'post_rank3', \n",
    "           'post_rank4', \n",
    "           'post_rank5', \n",
    "           'post_rank6', \n",
    "           'post_rank7', \n",
    "           'post_rank8',\n",
    "           'post_rank9',\n",
    "           'post_rank10',\n",
    "           'num_existing_keyphrases'] \n",
    "df = pd.DataFrame(columns=columns)\n",
    "row = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_step_critiquing_experiment_copy(user = 2, \n",
    "                           keyphrase_length_threshold = 150, \n",
    "                           max_iteration_threshold = 5,\n",
    "                           k = 50,\n",
    "                           df = df,\n",
    "                           row = row,\n",
    "                           business_df = business_df,\n",
    "                           keyphrases = keyphrases,\n",
    "                           keyphrase_popularity = keyphrase_popularity, \n",
    "                           keyphrase_selection_method = 'random',\n",
    "                           recommend_type = 'all',\n",
    "                           lams = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1] \n",
    "                          ):\n",
    "    \"\"\"\n",
    "    k: HR@k \n",
    "    keyphrase_length_threshold: limit the number of keyphrases in top recommended item\n",
    "    keyphrase_selection_method: 'random': randomly select keyphrase from wanted_keyphrases\n",
    "                                'pop': always select the most popular keyphrase in wanted_keyphrases\n",
    "                                'diff': select the keyphrase with largest frequency difference between top recommended \n",
    "                                        item and target item.\n",
    "    recommend_type: 'all': recommend all items\n",
    "                    'upper' (only_with_critiqued_keyphrase): recommend items with only critiqued_keyphrase\n",
    "    lam: modified_matrix = lam*origianl_matrix + (1-lam)*critiquing_embedding \n",
    "    \"\"\"\n",
    "    \n",
    "    row['user_id'] = user\n",
    "    print ('User ID ', user)\n",
    "    \n",
    "    # Set up (move to header line later)\n",
    "    matrix_Train = rtrain\n",
    "    matrix_Test = rtest\n",
    "    keyphrase_freq = I_K\n",
    "    num_items = rtrain.shape[1]\n",
    "    max_wanted_keyphrase = 10 # for keyphrase_selection_method == \"diff\"\n",
    "    initial_user_similarity_embedding = normalize(train(matrix_Train))\n",
    "    \n",
    "    # Get wanted items \n",
    "    candidate_items = matrix_Test[user].nonzero()[1]\n",
    "    train_items = matrix_Train[user].nonzero()[1]\n",
    "    wanted_items = np.setdiff1d(candidate_items, train_items)\n",
    "    print ('wanted_items length: ',len(wanted_items))\n",
    "    \n",
    "    # Get initial forward prediction \n",
    "    prediction_score,clf = get_initial_prediction(user, X = normalize(U_K.todense()), y = normalize(train(rtrain)),\n",
    "                            matrix_Train = rtrain, k = 100)\n",
    "    \n",
    "    # Get initial top recommended item(s)\n",
    "    top_recommendations = np.argsort(prediction_score)[::-1]\n",
    "    print (\"Initial top recommendation index\",top_recommendations[0])\n",
    "    try:\n",
    "        row['top_prediction_item_name'] = get_restaurant_name(df_train, business_df, top_recommendations[0])\n",
    "    # in case we cannot get the restaurant name\n",
    "    except: \n",
    "        row['top_prediction_item_name'] = 'CANNOT_FIND'\n",
    "        print ('Cannot get restaurant name for ItemIndex: ', top_recommendations[0])\n",
    "    \n",
    "    \n",
    "    # Get top recommended item's keyphrases\n",
    "    top_item = top_recommendations[0] \n",
    "    top_recommend_keyphrases = get_valid_keyphrases(keyphrase_freq,\n",
    "                                                    top_recommendations, \n",
    "                                                    item = top_item,\n",
    "                                                    threshold=keyphrase_length_threshold,\n",
    "                                                    mutiple_keyphrases_en = False, \n",
    "                                                    top_items = None)\n",
    "    print ('num_top_recommended_keyphrases ',len(top_recommend_keyphrases))\n",
    "\n",
    "    if keyphrase_selection_method == 'diff':\n",
    "        top_recommended_keyphrase_freq = get_item_keyphrase_freq(keyphrase_freq,item = top_item)\n",
    "    \n",
    "    \n",
    "    #####################################\n",
    "    # For each item, do the critiquing\n",
    "    \n",
    "    #limit the item to only 10\n",
    "    num_target_item = 0 # initialize item count\n",
    "    \n",
    "    for item in wanted_items:    \n",
    "        print ('target_item: ', item)\n",
    "        row['target_item'] = item\n",
    "        try:\n",
    "            row['item_name'] = get_restaurant_name(df_train, business_df, item)\n",
    "        except:\n",
    "            row['item_name'] = 'CANNOT_FIND'\n",
    "            print ('Cannot get restaurant name for ItemIndex: ', item)\n",
    "\n",
    "        # Get pre-critiquing rank\n",
    "        initial_rank = np.where(item == np.argsort(prediction_score)[::-1])[0][0]\n",
    "#         print ('target_item initial rank', int(initial_rank))\n",
    "        row['pre_rank'] = int(initial_rank)\n",
    "\n",
    "        # Get the target item's existing keyphrases\n",
    "        item_keyphrases = keyphrase_freq[item].nonzero()[1]\n",
    "#         print ('num_existing_keyphrases ',len(item_keyphrases))\n",
    "        \n",
    "        if keyphrase_selection_method == 'diff':\n",
    "            target_keyphrase_freq = get_item_keyphrase_freq(keyphrase_freq,item = item)\n",
    "            # indicate the keyphrase with large freq in target_item but small_keyphrase in top_recommended items\n",
    "            diff_keyphrase_freq = target_keyphrase_freq - top_recommended_keyphrase_freq\n",
    "            \n",
    "        # Get wanted keyphrases\n",
    "        if keyphrase_selection_method != 'diff':\n",
    "            # Get keyphrases that is not in the top recommended items but in the target item (we can select)\n",
    "            wanted_keyphrases = np.setdiff1d(item_keyphrases,top_recommend_keyphrases)\n",
    "\n",
    "            if len(wanted_keyphrases) == 0:\n",
    "                print (\"wanted_keyphrases is empty\")\n",
    "                break\n",
    "            row['num_existing_keyphrases'] = len(wanted_keyphrases)\n",
    "            \n",
    "        # For 'diff'\n",
    "        else:\n",
    "            wanted_keyphrases = np.argsort(np.ravel(diff_keyphrase_freq))[::-1][:max_wanted_keyphrase]\n",
    "            row['num_existing_keyphrases'] = len(wanted_keyphrases)\n",
    "\n",
    "        affected_items = np.array([])\n",
    "        modified_matrix = initial_user_similarity_embedding # initialize user similarity embedding\n",
    "        \n",
    "        #############################################\n",
    "        # Critiquing iteration\n",
    "        for iteration in range(max_iteration_threshold):\n",
    "            print ('cur_iter ', iteration)\n",
    "            row['iter'] = iteration\n",
    "            if keyphrase_selection_method == 'random':\n",
    "                # Randomly critique one keyphrase\n",
    "                critiqued_keyphrase = np.random.choice(wanted_keyphrases, size=1, replace=False)[0]\n",
    "            elif keyphrase_selection_method == 'pop':\n",
    "                # Always critique the most popular keyphrase\n",
    "                critiqued_keyphrase = wanted_keyphrases[np.argmax(keyphrase_popularity[wanted_keyphrases])]\n",
    "            elif keyphrase_selection_method == 'diff':\n",
    "                # critique the keyphrase with largest freq diff between top recommended_item and target_item\n",
    "                critiqued_keyphrase = wanted_keyphrases[0]\n",
    "#                 print (critiqued_keyphrase)\n",
    "            \n",
    "#             print ('critiqued_keyphrase ,',critiqued_keyphrase, keyphrases[critiqued_keyphrase])\n",
    "            row['critiqued_keyphrase'] = critiqued_keyphrase\n",
    "            row['keyphrase_name'] = keyphrases[critiqued_keyphrase]\n",
    "            \n",
    "            # Do not critique this keyphrase next time\n",
    "            wanted_keyphrases = np.delete(wanted_keyphrases, np.where(critiqued_keyphrase == wanted_keyphrases))\n",
    "            if len(wanted_keyphrases) == 0: \n",
    "                print ('no more keyphrase available')\n",
    "                break\n",
    "            \n",
    "            # Get affected items (items have critiqued keyphrase)\n",
    "            current_affected_items = keyphrase_freq[:, critiqued_keyphrase].nonzero()[0]\n",
    "            affected_items = np.unique(np.concatenate((affected_items, current_affected_items))).astype(int) \n",
    "            unaffected_items = np.setdiff1d(range(num_items), affected_items)\n",
    "\n",
    "            # Critiquing Embedding\n",
    "\n",
    "            # One hot encoding\n",
    "            critiqued_matrix_onehot = get_critiqued_UK(U_K,user,critiqued_keyphrase)\n",
    "            critiqued_matrix = clf.predict(critiqued_matrix_onehot)\n",
    "            critiqued_matrix = normalize(critiqued_matrix)\n",
    "            \n",
    "#             critiqued_matrix = project_one_hot_encoding(clf, \n",
    "#                                                         U_K,\n",
    "#                                                         user_index = user,\n",
    "#                                                         critiqued_keyphrase = critiqued_keyphrase, \n",
    "#                                                         normalize_en = True)\n",
    "\n",
    "#             modified_matrix = modified_matrix + critiqued_matrix # averaging user-item embedding and critiquing embeeding\n",
    "            \n",
    "            # Warning!!! The following is used only for testing single step critiquing, \n",
    "            # for full average critiquing, use the above commented line \n",
    "            post_ranks = []\n",
    "            for lam in lams:\n",
    "                modified_matrix = (1-lam)*normalize(train(matrix_Train)) + lam*critiqued_matrix \n",
    "#                 modified_matrix = normalize(modified_matrix)\n",
    "                if lam == 0:\n",
    "                    print (modified_matrix == initial_user_similarity_embedding)\n",
    "            \n",
    "                # Get new predictions from modified embedding\n",
    "                if recommend_type == 'all':\n",
    "                    prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix)\n",
    "                if recommend_type == 'upper':\n",
    "                    prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix, \n",
    "                                                         with_keyphrase = True, \n",
    "                                                         keyphrase_freq = keyphrase_freq, \n",
    "                                                         critiqued_keyphrase = critiqued_keyphrase, \n",
    "                                                         alpha = 0)\n",
    "                post_critique_rank = np.where(item == np.argsort(prediction_scores_u)[::-1])[0][0]\n",
    "                print ('target_item post-critique rank with lambda '+str(lam), int(post_critique_rank))\n",
    "                post_rank = int(post_critique_rank)\n",
    "                post_ranks.append(post_rank)\n",
    "            row['post_rank'] = post_ranks\n",
    "            row['post_rank0'] = post_ranks[0]\n",
    "            row['post_rank1'] = post_ranks[1]\n",
    "            row['post_rank2'] = post_ranks[2]\n",
    "            row['post_rank3'] = post_ranks[3]\n",
    "            row['post_rank4'] = post_ranks[4]\n",
    "            row['post_rank5'] = post_ranks[5]\n",
    "            row['post_rank6'] = post_ranks[6]\n",
    "            row['post_rank7'] = post_ranks[7]\n",
    "            row['post_rank8'] = post_ranks[8]\n",
    "            row['post_rank9'] = post_ranks[9]\n",
    "            row['post_rank10'] = post_ranks[10]\n",
    "            df = df.append(row, ignore_index=True)\n",
    "            \n",
    "\n",
    "        # break after got 10 target items \n",
    "        num_target_item += 1\n",
    "        if num_target_item >10: # only want max 10 items per user\n",
    "            break\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_step_critiquing_experiment(user = 2, \n",
    "                           keyphrase_length_threshold = 150, \n",
    "                           max_iteration_threshold = 5,\n",
    "                           k = 50,\n",
    "                           df = df,\n",
    "                           row = row,\n",
    "                           business_df = business_df,\n",
    "                           keyphrases = keyphrases,\n",
    "                           keyphrase_popularity = keyphrase_popularity, \n",
    "                           keyphrase_selection_method = 'random',\n",
    "                           recommend_type = 'all',\n",
    "                           lams = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1] \n",
    "                          ):\n",
    "    \"\"\"\n",
    "    k: HR@k \n",
    "    keyphrase_length_threshold: limit the number of keyphrases in top recommended item\n",
    "    keyphrase_selection_method: 'random': randomly select keyphrase from wanted_keyphrases\n",
    "                                'pop': always select the most popular keyphrase in wanted_keyphrases\n",
    "                                'diff': select the keyphrase with largest frequency difference between top recommended \n",
    "                                        item and target item.\n",
    "    recommend_type: 'all': recommend all items\n",
    "                    'upper' (only_with_critiqued_keyphrase): recommend items with only critiqued_keyphrase\n",
    "    lam: modified_matrix = lam*origianl_matrix + (1-lam)*critiquing_embedding \n",
    "    \"\"\"\n",
    "    \n",
    "    row['user_id'] = user\n",
    "    print ('User ID ', user)\n",
    "    \n",
    "    # Set up (move to header line later)\n",
    "    matrix_Train = rtrain\n",
    "    matrix_Test = rtest\n",
    "    keyphrase_freq = I_K\n",
    "    num_items = rtrain.shape[1]\n",
    "    max_wanted_keyphrase = 10 # for keyphrase_selection_method == \"diff\"\n",
    "    initial_user_similarity_embedding = normalize(train(matrix_Train))\n",
    "    \n",
    "    # Get wanted items \n",
    "    candidate_items = matrix_Test[user].nonzero()[1]\n",
    "    train_items = matrix_Train[user].nonzero()[1]\n",
    "    wanted_items = np.setdiff1d(candidate_items, train_items)\n",
    "    print ('wanted_items length: ',len(wanted_items))\n",
    "    \n",
    "    # Get initial forward prediction \n",
    "    prediction_score,clf = get_initial_prediction(user, X = normalize(U_K.todense()), y = normalize(train(rtrain)),\n",
    "                            matrix_Train = rtrain, k = 100)\n",
    "    \n",
    "    # Get initial top recommended item(s)\n",
    "    top_recommendations = np.argsort(prediction_score)[::-1]\n",
    "    print (\"Initial top recommendation index\",top_recommendations[0])\n",
    "    try:\n",
    "        row['top_prediction_item_name'] = get_restaurant_name(df_train, business_df, top_recommendations[0])\n",
    "    # in case we cannot get the restaurant name\n",
    "    except: \n",
    "        row['top_prediction_item_name'] = 'CANNOT_FIND'\n",
    "        print ('Cannot get restaurant name for ItemIndex: ', top_recommendations[0])\n",
    "    \n",
    "    \n",
    "    # Get top recommended item's keyphrases\n",
    "    top_item = top_recommendations[0] \n",
    "    top_recommend_keyphrases = get_valid_keyphrases(keyphrase_freq,\n",
    "                                                    top_recommendations, \n",
    "                                                    item = top_item,\n",
    "                                                    threshold=keyphrase_length_threshold,\n",
    "                                                    mutiple_keyphrases_en = False, \n",
    "                                                    top_items = None)\n",
    "    top_recommended_keyphrase_freq = get_item_keyphrase_freq(keyphrase_freq,item = top_item)\n",
    "    \n",
    "    \n",
    "    #####################################\n",
    "    # For each item, do the critiquing\n",
    "    \n",
    "    #limit the item to only 10\n",
    "    num_target_item = 0 # initialize item count\n",
    "    \n",
    "    for item in wanted_items:    \n",
    "        print ('target_item: ', item)\n",
    "        row['target_item'] = item\n",
    "        try:\n",
    "            row['item_name'] = get_restaurant_name(df_train, business_df, item)\n",
    "        except:\n",
    "            row['item_name'] = 'CANNOT_FIND'\n",
    "            print ('Cannot get restaurant name for ItemIndex: ', item)\n",
    "\n",
    "        # Get pre-critiquing rank\n",
    "        initial_rank = np.where(item == np.argsort(prediction_score)[::-1])[0][0]\n",
    "#         print ('target_item initial rank', int(initial_rank))\n",
    "        row['pre_rank'] = int(initial_rank)\n",
    "\n",
    "        # Get the target item's existing keyphrases\n",
    "        item_keyphrases = keyphrase_freq[item].nonzero()[1]\n",
    "#         print ('num_existing_keyphrases ',len(item_keyphrases))\n",
    "        \n",
    "#         if keyphrase_selection_method == 'diff':\n",
    "#             target_keyphrase_freq = get_item_keyphrase_freq(keyphrase_freq,item = item)\n",
    "#             # indicate the keyphrase with large freq in target_item but small_keyphrase in top_recommended items\n",
    "#             diff_keyphrase_freq = target_keyphrase_freq - top_recommended_keyphrase_freq\n",
    "            \n",
    "#         # Get wanted keyphrases\n",
    "#         if keyphrase_selection_method != 'diff':\n",
    "#             # Get keyphrases that is not in the top recommended items but in the target item (we can select)\n",
    "#             wanted_keyphrases = np.setdiff1d(item_keyphrases,top_recommend_keyphrases)\n",
    "\n",
    "#             if len(wanted_keyphrases) == 0:\n",
    "#                 print (\"wanted_keyphrases is empty\")\n",
    "#                 break\n",
    "#             row['num_existing_keyphrases'] = len(wanted_keyphrases)\n",
    "            \n",
    "#         # For 'diff'\n",
    "#         else:\n",
    "#             wanted_keyphrases = np.argsort(np.ravel(diff_keyphrase_freq))[::-1][:max_wanted_keyphrase]\n",
    "#             row['num_existing_keyphrases'] = len(wanted_keyphrases)\n",
    "        \n",
    "        # For diff \n",
    "        target_keyphrase_freq = get_item_keyphrase_freq(keyphrase_freq,item = item)\n",
    "        diff_keyphrase_freq = target_keyphrase_freq - top_recommended_keyphrase_freq\n",
    "        \n",
    "        wanted_keyphrases_random = np.setdiff1d(item_keyphrases,top_recommend_keyphrases)\n",
    "        wanted_keyphrases_pop = np.setdiff1d(item_keyphrases,top_recommend_keyphrases)\n",
    "        wanted_keyphrases_diff = np.argsort(np.ravel(diff_keyphrase_freq))[::-1][:max_wanted_keyphrase]\n",
    "        \n",
    "            \n",
    "        affected_items = np.array([])\n",
    "        modified_matrix = initial_user_similarity_embedding # initialize user similarity embedding\n",
    "        \n",
    "        #############################################\n",
    "        # Critiquing iteration\n",
    "        for iteration in range(max_iteration_threshold):\n",
    "            print ('cur_iter ', iteration)\n",
    "            row['iter'] = iteration\n",
    "            \n",
    "#             if keyphrase_selection_method == 'random':\n",
    "#                 # Randomly critique one keyphrase\n",
    "#                 critiqued_keyphrase = np.random.choice(wanted_keyphrases, size=1, replace=False)[0]\n",
    "#             elif keyphrase_selection_method == 'pop':\n",
    "#                 # Always critique the most popular keyphrase\n",
    "#                 critiqued_keyphrase = wanted_keyphrases[np.argmax(keyphrase_popularity[wanted_keyphrases])]\n",
    "#             elif keyphrase_selection_method == 'diff':\n",
    "#                 # critique the keyphrase with largest freq diff between top recommended_item and target_item\n",
    "#                 critiqued_keyphrase = wanted_keyphrases[0]\n",
    "            print (wanted_keyphrases_random)\n",
    "            if len(wanted_keyphrases_random) == 0 or len(wanted_keyphrases_diff) == 0: \n",
    "                print ('no more keyphrase available')\n",
    "                break\n",
    "            critiqued_keyphrase_random = np.random.choice(wanted_keyphrases_random, size=1, replace=False)[0]\n",
    "            critiqued_keyphrase_pop = wanted_keyphrases_pop[np.argmin(keyphrase_popularity[wanted_keyphrases_pop])] # Select the least popular\n",
    "            critiqued_keyphrase_diff = wanted_keyphrases_diff[0]\n",
    "            \n",
    "            row['critiqued_keyphrase_random'] = critiqued_keyphrase_random\n",
    "            row['keyphrase_name_random'] = keyphrases[critiqued_keyphrase_random]\n",
    "            row['critiqued_keyphrase_pop'] = critiqued_keyphrase_pop\n",
    "            row['keyphrase_name_pop'] = keyphrases[critiqued_keyphrase_pop]\n",
    "            row['critiqued_keyphrase_diff'] = critiqued_keyphrase_diff\n",
    "            row['keyphrase_name_diff'] = keyphrases[critiqued_keyphrase_diff]\n",
    "            \n",
    "            # Do not critique this keyphrase next time\n",
    "            wanted_keyphrases_random = np.delete(wanted_keyphrases_random, np.where(critiqued_keyphrase_random == wanted_keyphrases_random))\n",
    "            wanted_keyphrases_pop = np.delete(wanted_keyphrases_pop, np.where(critiqued_keyphrase_pop == wanted_keyphrases_pop))\n",
    "            wanted_keyphrases_diff = np.delete(wanted_keyphrases_diff, np.where(critiqued_keyphrase_diff == wanted_keyphrases_diff))\n",
    "            \n",
    "            \n",
    "            # Get affected items (items have critiqued keyphrase)\n",
    "#             current_affected_items = keyphrase_freq[:, critiqued_keyphrase].nonzero()[0]\n",
    "#             affected_items = np.unique(np.concatenate((affected_items, current_affected_items))).astype(int) \n",
    "#             unaffected_items = np.setdiff1d(range(num_items), affected_items)\n",
    "\n",
    "            # Critiquing Embedding\n",
    "\n",
    "            # One hot encoding\n",
    "            critiqued_matrix_onehot = get_critiqued_UK(U_K,user,critiqued_keyphrase_random)\n",
    "            critiqued_matrix = clf.predict(critiqued_matrix_onehot)\n",
    "            critiqued_matrix_random = normalize(critiqued_matrix)\n",
    "\n",
    "            critiqued_matrix_onehot = get_critiqued_UK(U_K,user,critiqued_keyphrase_pop)\n",
    "            critiqued_matrix = clf.predict(critiqued_matrix_onehot)\n",
    "            critiqued_matrix_pop = normalize(critiqued_matrix)\n",
    "\n",
    "            critiqued_matrix_onehot = get_critiqued_UK(U_K,user,critiqued_keyphrase_diff)\n",
    "            critiqued_matrix = clf.predict(critiqued_matrix_onehot)\n",
    "            critiqued_matrix_diff = normalize(critiqued_matrix)\n",
    "\n",
    "\n",
    "#             critiqued_matrix = project_one_hot_encoding(clf, \n",
    "#                                                         U_K,\n",
    "#                                                         user_index = user,\n",
    "#                                                         critiqued_keyphrase = critiqued_keyphrase, \n",
    "#                                                         normalize_en = True)\n",
    "\n",
    "#             modified_matrix = modified_matrix + critiqued_matrix # averaging user-item embedding and critiquing embeeding\n",
    "            \n",
    "            # Warning!!! The following is used only for testing single step critiquing, \n",
    "            # for full average critiquing, use the above commented line \n",
    "            post_ranks_random_all = []\n",
    "            post_ranks_random_upper = []\n",
    "            post_ranks_pop_all = []\n",
    "            post_ranks_pop_upper = []\n",
    "            post_ranks_diff_all = []\n",
    "            post_ranks_diff_upper = []\n",
    "            \n",
    "            for lam in lams:\n",
    "                modified_matrix_random = (1-lam)*normalize(train(matrix_Train)) + lam*critiqued_matrix_random \n",
    "                modified_matrix_pop = (1-lam)*normalize(train(matrix_Train)) + lam*critiqued_matrix_pop \n",
    "                modified_matrix_diff = (1-lam)*normalize(train(matrix_Train)) + lam*critiqued_matrix_diff \n",
    "#                 modified_matrix = normalize(modified_matrix)\n",
    "\n",
    "            \n",
    "                # Get new predictions from modified embedding\n",
    "#                 if recommend_type == 'all':\n",
    "#                     prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix)\n",
    "#                 if recommend_type == 'upper':\n",
    "#                     prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix, \n",
    "#                                                          with_keyphrase = True, \n",
    "#                                                          keyphrase_freq = keyphrase_freq, \n",
    "#                                                          critiqued_keyphrase = critiqued_keyphrase, \n",
    "#                                                          alpha = 0)\n",
    "#                 post_critique_rank = np.where(item == np.argsort(prediction_scores_u)[::-1])[0][0]\n",
    "#                 print ('target_item post-critique rank with lambda '+str(lam), int(post_critique_rank))\n",
    "#                 post_rank = int(post_critique_rank)\n",
    "#                 post_ranks.append(post_rank)\n",
    "                \n",
    "                \n",
    "                # Random \n",
    "                prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix_random)\n",
    "                post_critique_rank = np.where(item == np.argsort(prediction_scores_u)[::-1])[0][0]\n",
    "                post_ranks_random_all.append(post_critique_rank)\n",
    "            \n",
    "                prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix_random, \n",
    "                                                         with_keyphrase = True, \n",
    "                                                         keyphrase_freq = keyphrase_freq, \n",
    "                                                         critiqued_keyphrase = critiqued_keyphrase_random, \n",
    "                                                         alpha = 0)\n",
    "                post_critique_rank = np.where(item == np.argsort(prediction_scores_u)[::-1])[0][0]\n",
    "                post_ranks_random_upper.append(post_critique_rank)\n",
    "                \n",
    "                #Pop\n",
    "                prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix_pop)\n",
    "                post_critique_rank = np.where(item == np.argsort(prediction_scores_u)[::-1])[0][0]\n",
    "                post_ranks_pop_all.append(post_critique_rank)\n",
    "            \n",
    "                prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix_pop, \n",
    "                                                         with_keyphrase = True, \n",
    "                                                         keyphrase_freq = keyphrase_freq, \n",
    "                                                         critiqued_keyphrase = critiqued_keyphrase_pop, \n",
    "                                                         alpha = 0)\n",
    "                post_critique_rank = np.where(item == np.argsort(prediction_scores_u)[::-1])[0][0]\n",
    "                post_ranks_pop_upper.append(post_critique_rank)\n",
    "                \n",
    "                #Diff\n",
    "                prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix_diff)\n",
    "                post_critique_rank = np.where(item == np.argsort(prediction_scores_u)[::-1])[0][0]\n",
    "                post_ranks_diff_all.append(post_critique_rank)\n",
    "            \n",
    "                prediction_scores_u = predict_vector(user, matrix_Train, 100, modified_matrix_diff, \n",
    "                                                         with_keyphrase = True, \n",
    "                                                         keyphrase_freq = keyphrase_freq, \n",
    "                                                         critiqued_keyphrase = critiqued_keyphrase_diff, \n",
    "                                                         alpha = 0)\n",
    "                post_critique_rank = np.where(item == np.argsort(prediction_scores_u)[::-1])[0][0]\n",
    "                post_ranks_diff_upper.append(post_critique_rank)\n",
    "            \n",
    "            row['post_rank_random_all'] = post_ranks_random_all\n",
    "            row['post_rank_random_upper'] = post_ranks_random_upper\n",
    "            row['post_rank_pop_all'] = post_ranks_pop_all\n",
    "            row['post_rank_pop_upper'] = post_ranks_pop_upper\n",
    "            row['post_rank_diff_all'] = post_ranks_diff_all\n",
    "            row['post_rank_diff_upper'] = post_ranks_diff_upper\n",
    "#             row['post_rank0'] = post_ranks[0]\n",
    "#             row['post_rank1'] = post_ranks[1]\n",
    "#             row['post_rank2'] = post_ranks[2]\n",
    "#             row['post_rank3'] = post_ranks[3]\n",
    "#             row['post_rank4'] = post_ranks[4]\n",
    "#             row['post_rank5'] = post_ranks[5]\n",
    "#             row['post_rank6'] = post_ranks[6]\n",
    "#             row['post_rank7'] = post_ranks[7]\n",
    "#             row['post_rank8'] = post_ranks[8]\n",
    "#             row['post_rank9'] = post_ranks[9]\n",
    "#             row['post_rank10'] = post_ranks[10]\n",
    "            df = df.append(row, ignore_index=True)\n",
    "            \n",
    "\n",
    "        # break after got 10 target items \n",
    "        num_target_item += 1\n",
    "        if num_target_item >10: # only want max 10 items per user\n",
    "            break\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_with_avg_path = \"../tables/critiquing/single_step_lam_all/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize df for storing the experiment\n",
    "\n",
    "# post_ranki is post rank with different lambda ratio for combining pre-post User similarity matrix \n",
    "columns = ['user_id', 'target_item', 'item_name', 'iter', 'pre_rank', \n",
    "           'top_prediction_item_name',\n",
    "           'post_rank_random_all',\n",
    "           'post_rank_random_upper',\n",
    "           'post_rank_pop_all',\n",
    "           'post_rank_pop_upper',\n",
    "           'post_rank_diff_all',\n",
    "           'post_rank_diff_upper',\n",
    "           'critiqued_keyphrase_random',\n",
    "           'keyphrase_name_random',\n",
    "           'critiqued_keyphrase_pop',\n",
    "           'keyphrase_name_pop',\n",
    "           'critiqued_keyphrase_diff',\n",
    "           'keyphrase_name_diff',\n",
    "           'num_existing_keyphrases'] \n",
    "df = pd.DataFrame(columns=columns)\n",
    "row = {}\n",
    "\n",
    "#only_with_critiqued_keyphrase\n",
    "for user in range(300,350):\n",
    "    df = single_step_critiquing_experiment(user = user, \n",
    "                           keyphrase_length_threshold = 230, \n",
    "                           max_iteration_threshold = 10,\n",
    "                           k = 50,\n",
    "                           df = df,\n",
    "                           row = row,\n",
    "                           business_df = business_df,\n",
    "                           keyphrases = keyphrases,\n",
    "                           keyphrase_popularity = keyphrase_popularity,\n",
    "                           keyphrase_selection_method = 'random',\n",
    "                           recommend_type = 'all'\n",
    "                           )\n",
    "df.to_csv(single_step_with_avg_path+\"50user.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load experiment res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(single_step_with_avg_path+\"50user.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>target_item</th>\n",
       "      <th>item_name</th>\n",
       "      <th>iter</th>\n",
       "      <th>pre_rank</th>\n",
       "      <th>top_prediction_item_name</th>\n",
       "      <th>post_rank_random_all</th>\n",
       "      <th>post_rank_random_upper</th>\n",
       "      <th>post_rank_pop_all</th>\n",
       "      <th>post_rank_pop_upper</th>\n",
       "      <th>post_rank_diff_all</th>\n",
       "      <th>post_rank_diff_upper</th>\n",
       "      <th>critiqued_keyphrase_random</th>\n",
       "      <th>keyphrase_name_random</th>\n",
       "      <th>critiqued_keyphrase_pop</th>\n",
       "      <th>keyphrase_name_pop</th>\n",
       "      <th>critiqued_keyphrase_diff</th>\n",
       "      <th>keyphrase_name_diff</th>\n",
       "      <th>num_existing_keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>101</td>\n",
       "      <td>b'Cabin Fever'</td>\n",
       "      <td>0</td>\n",
       "      <td>3311</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>[3311, 3536, 2960, 3115, 3546, 3465, 3482, 345...</td>\n",
       "      <td>[2908, 2907, 2903, 2907, 2822, 2812, 2822, 282...</td>\n",
       "      <td>[3311, 3536, 2960, 3115, 3546, 3465, 3482, 345...</td>\n",
       "      <td>[2908, 2907, 2903, 2907, 2822, 2812, 2822, 282...</td>\n",
       "      <td>[3311, 2943, 3521, 3859, 3428, 3601, 3985, 347...</td>\n",
       "      <td>[3113, 3582, 3242, 3502, 3391, 2973, 4389, 260...</td>\n",
       "      <td>105</td>\n",
       "      <td>latte</td>\n",
       "      <td>105</td>\n",
       "      <td>latte</td>\n",
       "      <td>153</td>\n",
       "      <td>friendly</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>0</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>[755, 740, 632, 600, 522, 662, 688, 769, 858, ...</td>\n",
       "      <td>[145, 152, 139, 144, 151, 187, 190, 207, 226, ...</td>\n",
       "      <td>[755, 765, 773, 371, 268, 246, 301, 249, 224, ...</td>\n",
       "      <td>[10, 8, 12, 8, 10, 11, 15, 15, 14, 14, 14]</td>\n",
       "      <td>[755, 795, 473, 424, 445, 873, 811, 815, 830, ...</td>\n",
       "      <td>[237, 245, 172, 164, 182, 304, 287, 278, 285, ...</td>\n",
       "      <td>115</td>\n",
       "      <td>lobster</td>\n",
       "      <td>223</td>\n",
       "      <td>general tao</td>\n",
       "      <td>27</td>\n",
       "      <td>bbq</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>1</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>[755, 784, 875, 945, 2129, 2197, 2027, 3059, 4...</td>\n",
       "      <td>[241, 256, 301, 325, 569, 588, 585, 2340, 2329...</td>\n",
       "      <td>[755, 774, 780, 765, 816, 939, 1076, 1842, 202...</td>\n",
       "      <td>[48, 60, 66, 77, 91, 108, 121, 143, 150, 2247,...</td>\n",
       "      <td>[755, 758, 732, 439, 434, 481, 515, 731, 387, ...</td>\n",
       "      <td>[256, 263, 269, 184, 207, 260, 294, 394, 256, ...</td>\n",
       "      <td>48</td>\n",
       "      <td>burger</td>\n",
       "      <td>73</td>\n",
       "      <td>congee</td>\n",
       "      <td>0</td>\n",
       "      <td>chinese</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>2</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>[755, 830, 928, 2045, 1916, 1962, 4303, 3101, ...</td>\n",
       "      <td>[171, 193, 213, 313, 301, 311, 2240, 2239, 224...</td>\n",
       "      <td>[755, 784, 783, 756, 885, 1874, 2967, 2934, 29...</td>\n",
       "      <td>[139, 160, 158, 157, 188, 293, 2205, 2264, 223...</td>\n",
       "      <td>[755, 772, 795, 882, 583, 575, 2054, 1798, 167...</td>\n",
       "      <td>[379, 385, 396, 447, 340, 350, 883, 804, 781, ...</td>\n",
       "      <td>108</td>\n",
       "      <td>duck</td>\n",
       "      <td>60</td>\n",
       "      <td>squid</td>\n",
       "      <td>51</td>\n",
       "      <td>pork</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>886</td>\n",
       "      <td>b'Wok &amp; Roast Chinese BBQ'</td>\n",
       "      <td>3</td>\n",
       "      <td>755</td>\n",
       "      <td>b'Khao San Road'</td>\n",
       "      <td>[755, 819, 906, 944, 1364, 2166, 2105, 1946, 1...</td>\n",
       "      <td>[141, 163, 179, 206, 255, 320, 316, 320, 328, ...</td>\n",
       "      <td>[755, 819, 906, 944, 1364, 2166, 2105, 1946, 1...</td>\n",
       "      <td>[141, 163, 179, 206, 255, 320, 316, 320, 328, ...</td>\n",
       "      <td>[755, 787, 747, 690, 976, 936, 990, 1173, 1484...</td>\n",
       "      <td>[478, 520, 494, 457, 623, 619, 641, 696, 836, ...</td>\n",
       "      <td>72</td>\n",
       "      <td>scallop</td>\n",
       "      <td>72</td>\n",
       "      <td>scallop</td>\n",
       "      <td>5</td>\n",
       "      <td>fried</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3486</td>\n",
       "      <td>3486</td>\n",
       "      <td>349</td>\n",
       "      <td>6210</td>\n",
       "      <td>b\"Osmow's\"</td>\n",
       "      <td>5</td>\n",
       "      <td>4687</td>\n",
       "      <td>b'Jim Chai Kee'</td>\n",
       "      <td>[4687, 4688, 4671, 4677, 4696, 4713, 4715, 471...</td>\n",
       "      <td>[3959, 3959, 3946, 3959, 3970, 3977, 3983, 398...</td>\n",
       "      <td>[4687, 4652, 4582, 4637, 4561, 4562, 4588, 455...</td>\n",
       "      <td>[4001, 3994, 3987, 3993, 3987, 3997, 3997, 399...</td>\n",
       "      <td>[4687, 4683, 4703, 4690, 4697, 4708, 4712, 475...</td>\n",
       "      <td>[4024, 4038, 4047, 4041, 4044, 4052, 4063, 407...</td>\n",
       "      <td>160</td>\n",
       "      <td>overpriced</td>\n",
       "      <td>164</td>\n",
       "      <td>quiet</td>\n",
       "      <td>178</td>\n",
       "      <td>honestly</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3487</td>\n",
       "      <td>3487</td>\n",
       "      <td>349</td>\n",
       "      <td>6210</td>\n",
       "      <td>b\"Osmow's\"</td>\n",
       "      <td>6</td>\n",
       "      <td>4687</td>\n",
       "      <td>b'Jim Chai Kee'</td>\n",
       "      <td>[4687, 4687, 4651, 4643, 4660, 4677, 4681, 470...</td>\n",
       "      <td>[3991, 3975, 3998, 3991, 3984, 3991, 3992, 397...</td>\n",
       "      <td>[4687, 4669, 4691, 4687, 4705, 4639, 4658, 467...</td>\n",
       "      <td>[3993, 3965, 3980, 3963, 3978, 3976, 3980, 400...</td>\n",
       "      <td>[4687, 4660, 4670, 1833, 1525, 1407, 1923, 187...</td>\n",
       "      <td>[3944, 3945, 3949, 420, 360, 349, 462, 465, 45...</td>\n",
       "      <td>91</td>\n",
       "      <td>wrap</td>\n",
       "      <td>173</td>\n",
       "      <td>dark</td>\n",
       "      <td>76</td>\n",
       "      <td>lettuce</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3488</td>\n",
       "      <td>3488</td>\n",
       "      <td>349</td>\n",
       "      <td>6210</td>\n",
       "      <td>b\"Osmow's\"</td>\n",
       "      <td>7</td>\n",
       "      <td>4687</td>\n",
       "      <td>b'Jim Chai Kee'</td>\n",
       "      <td>[4687, 4685, 1860, 1892, 1922, 1897, 1846, 472...</td>\n",
       "      <td>[3883, 3884, 231, 243, 250, 267, 284, 3915, 39...</td>\n",
       "      <td>[4687, 4683, 4703, 4690, 4697, 4708, 4712, 475...</td>\n",
       "      <td>[4024, 4038, 4047, 4041, 4044, 4052, 4063, 407...</td>\n",
       "      <td>[4687, 4696, 4821, 4804, 4803, 2550, 2545, 266...</td>\n",
       "      <td>[4004, 4010, 4010, 4008, 4012, 811, 818, 861, ...</td>\n",
       "      <td>64</td>\n",
       "      <td>olive</td>\n",
       "      <td>178</td>\n",
       "      <td>honestly</td>\n",
       "      <td>114</td>\n",
       "      <td>tomato</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3489</td>\n",
       "      <td>3489</td>\n",
       "      <td>349</td>\n",
       "      <td>6210</td>\n",
       "      <td>b\"Osmow's\"</td>\n",
       "      <td>8</td>\n",
       "      <td>4687</td>\n",
       "      <td>b'Jim Chai Kee'</td>\n",
       "      <td>[4687, 4678, 4691, 1884, 2057, 4734, 4739, 471...</td>\n",
       "      <td>[4053, 4055, 4063, 923, 1033, 4107, 4116, 4112...</td>\n",
       "      <td>[4687, 4687, 4651, 4643, 4660, 4677, 4681, 470...</td>\n",
       "      <td>[3991, 3975, 3998, 3991, 3984, 3991, 3992, 397...</td>\n",
       "      <td>[4687, 4696, 4697, 4744, 4736, 4715, 4741, 472...</td>\n",
       "      <td>[3967, 3985, 3992, 4006, 4014, 3962, 4012, 399...</td>\n",
       "      <td>50</td>\n",
       "      <td>salad</td>\n",
       "      <td>91</td>\n",
       "      <td>wrap</td>\n",
       "      <td>161</td>\n",
       "      <td>cheaper</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3490</td>\n",
       "      <td>3490</td>\n",
       "      <td>349</td>\n",
       "      <td>6210</td>\n",
       "      <td>b\"Osmow's\"</td>\n",
       "      <td>9</td>\n",
       "      <td>4687</td>\n",
       "      <td>b'Jim Chai Kee'</td>\n",
       "      <td>[4687, 4683, 4703, 4690, 4697, 4708, 4712, 475...</td>\n",
       "      <td>[4024, 4038, 4047, 4041, 4044, 4052, 4063, 407...</td>\n",
       "      <td>[4687, 4690, 4689, 4702, 4725, 4720, 2292, 233...</td>\n",
       "      <td>[4014, 4012, 4018, 4019, 4051, 4060, 1041, 106...</td>\n",
       "      <td>[4687, 4673, 4669, 4784, 4779, 4839, 4838, 476...</td>\n",
       "      <td>[4428, 4423, 4421, 4495, 4501, 4517, 4527, 451...</td>\n",
       "      <td>178</td>\n",
       "      <td>honestly</td>\n",
       "      <td>208</td>\n",
       "      <td>topped</td>\n",
       "      <td>7</td>\n",
       "      <td>dinner</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3491 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  user_id  target_item                   item_name  iter  \\\n",
       "0              0      300          101              b'Cabin Fever'     0   \n",
       "1              1      300          886  b'Wok & Roast Chinese BBQ'     0   \n",
       "2              2      300          886  b'Wok & Roast Chinese BBQ'     1   \n",
       "3              3      300          886  b'Wok & Roast Chinese BBQ'     2   \n",
       "4              4      300          886  b'Wok & Roast Chinese BBQ'     3   \n",
       "...          ...      ...          ...                         ...   ...   \n",
       "3486        3486      349         6210                  b\"Osmow's\"     5   \n",
       "3487        3487      349         6210                  b\"Osmow's\"     6   \n",
       "3488        3488      349         6210                  b\"Osmow's\"     7   \n",
       "3489        3489      349         6210                  b\"Osmow's\"     8   \n",
       "3490        3490      349         6210                  b\"Osmow's\"     9   \n",
       "\n",
       "      pre_rank top_prediction_item_name  \\\n",
       "0         3311         b'Khao San Road'   \n",
       "1          755         b'Khao San Road'   \n",
       "2          755         b'Khao San Road'   \n",
       "3          755         b'Khao San Road'   \n",
       "4          755         b'Khao San Road'   \n",
       "...        ...                      ...   \n",
       "3486      4687          b'Jim Chai Kee'   \n",
       "3487      4687          b'Jim Chai Kee'   \n",
       "3488      4687          b'Jim Chai Kee'   \n",
       "3489      4687          b'Jim Chai Kee'   \n",
       "3490      4687          b'Jim Chai Kee'   \n",
       "\n",
       "                                   post_rank_random_all  \\\n",
       "0     [3311, 3536, 2960, 3115, 3546, 3465, 3482, 345...   \n",
       "1     [755, 740, 632, 600, 522, 662, 688, 769, 858, ...   \n",
       "2     [755, 784, 875, 945, 2129, 2197, 2027, 3059, 4...   \n",
       "3     [755, 830, 928, 2045, 1916, 1962, 4303, 3101, ...   \n",
       "4     [755, 819, 906, 944, 1364, 2166, 2105, 1946, 1...   \n",
       "...                                                 ...   \n",
       "3486  [4687, 4688, 4671, 4677, 4696, 4713, 4715, 471...   \n",
       "3487  [4687, 4687, 4651, 4643, 4660, 4677, 4681, 470...   \n",
       "3488  [4687, 4685, 1860, 1892, 1922, 1897, 1846, 472...   \n",
       "3489  [4687, 4678, 4691, 1884, 2057, 4734, 4739, 471...   \n",
       "3490  [4687, 4683, 4703, 4690, 4697, 4708, 4712, 475...   \n",
       "\n",
       "                                 post_rank_random_upper  \\\n",
       "0     [2908, 2907, 2903, 2907, 2822, 2812, 2822, 282...   \n",
       "1     [145, 152, 139, 144, 151, 187, 190, 207, 226, ...   \n",
       "2     [241, 256, 301, 325, 569, 588, 585, 2340, 2329...   \n",
       "3     [171, 193, 213, 313, 301, 311, 2240, 2239, 224...   \n",
       "4     [141, 163, 179, 206, 255, 320, 316, 320, 328, ...   \n",
       "...                                                 ...   \n",
       "3486  [3959, 3959, 3946, 3959, 3970, 3977, 3983, 398...   \n",
       "3487  [3991, 3975, 3998, 3991, 3984, 3991, 3992, 397...   \n",
       "3488  [3883, 3884, 231, 243, 250, 267, 284, 3915, 39...   \n",
       "3489  [4053, 4055, 4063, 923, 1033, 4107, 4116, 4112...   \n",
       "3490  [4024, 4038, 4047, 4041, 4044, 4052, 4063, 407...   \n",
       "\n",
       "                                      post_rank_pop_all  \\\n",
       "0     [3311, 3536, 2960, 3115, 3546, 3465, 3482, 345...   \n",
       "1     [755, 765, 773, 371, 268, 246, 301, 249, 224, ...   \n",
       "2     [755, 774, 780, 765, 816, 939, 1076, 1842, 202...   \n",
       "3     [755, 784, 783, 756, 885, 1874, 2967, 2934, 29...   \n",
       "4     [755, 819, 906, 944, 1364, 2166, 2105, 1946, 1...   \n",
       "...                                                 ...   \n",
       "3486  [4687, 4652, 4582, 4637, 4561, 4562, 4588, 455...   \n",
       "3487  [4687, 4669, 4691, 4687, 4705, 4639, 4658, 467...   \n",
       "3488  [4687, 4683, 4703, 4690, 4697, 4708, 4712, 475...   \n",
       "3489  [4687, 4687, 4651, 4643, 4660, 4677, 4681, 470...   \n",
       "3490  [4687, 4690, 4689, 4702, 4725, 4720, 2292, 233...   \n",
       "\n",
       "                                    post_rank_pop_upper  \\\n",
       "0     [2908, 2907, 2903, 2907, 2822, 2812, 2822, 282...   \n",
       "1            [10, 8, 12, 8, 10, 11, 15, 15, 14, 14, 14]   \n",
       "2     [48, 60, 66, 77, 91, 108, 121, 143, 150, 2247,...   \n",
       "3     [139, 160, 158, 157, 188, 293, 2205, 2264, 223...   \n",
       "4     [141, 163, 179, 206, 255, 320, 316, 320, 328, ...   \n",
       "...                                                 ...   \n",
       "3486  [4001, 3994, 3987, 3993, 3987, 3997, 3997, 399...   \n",
       "3487  [3993, 3965, 3980, 3963, 3978, 3976, 3980, 400...   \n",
       "3488  [4024, 4038, 4047, 4041, 4044, 4052, 4063, 407...   \n",
       "3489  [3991, 3975, 3998, 3991, 3984, 3991, 3992, 397...   \n",
       "3490  [4014, 4012, 4018, 4019, 4051, 4060, 1041, 106...   \n",
       "\n",
       "                                     post_rank_diff_all  \\\n",
       "0     [3311, 2943, 3521, 3859, 3428, 3601, 3985, 347...   \n",
       "1     [755, 795, 473, 424, 445, 873, 811, 815, 830, ...   \n",
       "2     [755, 758, 732, 439, 434, 481, 515, 731, 387, ...   \n",
       "3     [755, 772, 795, 882, 583, 575, 2054, 1798, 167...   \n",
       "4     [755, 787, 747, 690, 976, 936, 990, 1173, 1484...   \n",
       "...                                                 ...   \n",
       "3486  [4687, 4683, 4703, 4690, 4697, 4708, 4712, 475...   \n",
       "3487  [4687, 4660, 4670, 1833, 1525, 1407, 1923, 187...   \n",
       "3488  [4687, 4696, 4821, 4804, 4803, 2550, 2545, 266...   \n",
       "3489  [4687, 4696, 4697, 4744, 4736, 4715, 4741, 472...   \n",
       "3490  [4687, 4673, 4669, 4784, 4779, 4839, 4838, 476...   \n",
       "\n",
       "                                   post_rank_diff_upper  \\\n",
       "0     [3113, 3582, 3242, 3502, 3391, 2973, 4389, 260...   \n",
       "1     [237, 245, 172, 164, 182, 304, 287, 278, 285, ...   \n",
       "2     [256, 263, 269, 184, 207, 260, 294, 394, 256, ...   \n",
       "3     [379, 385, 396, 447, 340, 350, 883, 804, 781, ...   \n",
       "4     [478, 520, 494, 457, 623, 619, 641, 696, 836, ...   \n",
       "...                                                 ...   \n",
       "3486  [4024, 4038, 4047, 4041, 4044, 4052, 4063, 407...   \n",
       "3487  [3944, 3945, 3949, 420, 360, 349, 462, 465, 45...   \n",
       "3488  [4004, 4010, 4010, 4008, 4012, 811, 818, 861, ...   \n",
       "3489  [3967, 3985, 3992, 4006, 4014, 3962, 4012, 399...   \n",
       "3490  [4428, 4423, 4421, 4495, 4501, 4517, 4527, 451...   \n",
       "\n",
       "      critiqued_keyphrase_random keyphrase_name_random  \\\n",
       "0                            105                 latte   \n",
       "1                            115               lobster   \n",
       "2                             48                burger   \n",
       "3                            108                  duck   \n",
       "4                             72               scallop   \n",
       "...                          ...                   ...   \n",
       "3486                         160            overpriced   \n",
       "3487                          91                  wrap   \n",
       "3488                          64                 olive   \n",
       "3489                          50                 salad   \n",
       "3490                         178              honestly   \n",
       "\n",
       "      critiqued_keyphrase_pop keyphrase_name_pop  critiqued_keyphrase_diff  \\\n",
       "0                         105              latte                       153   \n",
       "1                         223        general tao                        27   \n",
       "2                          73             congee                         0   \n",
       "3                          60              squid                        51   \n",
       "4                          72            scallop                         5   \n",
       "...                       ...                ...                       ...   \n",
       "3486                      164              quiet                       178   \n",
       "3487                      173               dark                        76   \n",
       "3488                      178           honestly                       114   \n",
       "3489                       91               wrap                       161   \n",
       "3490                      208             topped                         7   \n",
       "\n",
       "     keyphrase_name_diff  num_existing_keyphrases  \n",
       "0               friendly                      NaN  \n",
       "1                    bbq                      NaN  \n",
       "2                chinese                      NaN  \n",
       "3                   pork                      NaN  \n",
       "4                  fried                      NaN  \n",
       "...                  ...                      ...  \n",
       "3486            honestly                      NaN  \n",
       "3487             lettuce                      NaN  \n",
       "3488              tomato                      NaN  \n",
       "3489             cheaper                      NaN  \n",
       "3490              dinner                      NaN  \n",
       "\n",
       "[3491 rows x 20 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3311, 3536, 2960, 3115, 3546, 3465, 3482, 3457, 2670, 3471, 2642]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "literal_eval(df['post_rank_random_all'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[3311, 3536, 2960, 3115, 3546, 3465, 3482, 3457, 2670, 3471, 2642]',\n",
       " '[755, 740, 632, 600, 522, 662, 688, 769, 858, 929, 973]',\n",
       " '[755, 784, 875, 945, 2129, 2197, 2027, 3059, 4151, 4140, 2975]',\n",
       " '[755, 830, 928, 2045, 1916, 1962, 4303, 3101, 3083, 3054, 3098]',\n",
       " '[755, 819, 906, 944, 1364, 2166, 2105, 1946, 1955, 1933, 1923]',\n",
       " '[755, 765, 773, 371, 268, 246, 301, 249, 224, 196, 176]',\n",
       " '[755, 774, 780, 765, 816, 939, 1076, 1842, 2024, 4499, 3288]',\n",
       " '[755, 837, 906, 1024, 1097, 1511, 1428, 1404, 1497, 1555, 2936]',\n",
       " '[755, 784, 783, 756, 885, 1874, 2967, 2934, 2901, 2919, 2903]',\n",
       " '[755, 865, 1044, 1116, 2032, 3756, 3720, 3065, 3047, 3055, 3023]',\n",
       " '[755, 795, 473, 424, 445, 873, 811, 815, 830, 847, 866]',\n",
       " '[574, 586, 623, 686, 588, 607, 866, 818, 794, 797, 393]',\n",
       " '[574, 559, 548, 550, 492, 501, 533, 255, 254, 319, 320]',\n",
       " '[574, 372, 387, 430, 738, 1889, 3329, 3220, 3385, 2510, 3322]',\n",
       " '[574, 569, 566, 368, 272, 273, 203, 143, 175, 165, 146]',\n",
       " '[574, 625, 383, 436, 439, 420, 256, 263, 155, 133, 133]',\n",
       " '[574, 365, 356, 204, 146, 146, 108, 100, 125, 116, 115]',\n",
       " '[574, 407, 281, 136, 113, 154, 94, 105, 84, 75, 71]',\n",
       " '[574, 403, 416, 470, 1107, 2310, 2154, 2094, 3137, 3141, 3106]',\n",
       " '[574, 380, 406, 328, 282, 70, 44, 40, 32, 42, 40]',\n",
       " '[574, 600, 614, 547, 443, 311, 694, 620, 581, 534, 498]',\n",
       " '[29, 34, 33, 30, 22, 35, 87, 159, 167, 175, 283]',\n",
       " '[29, 31, 36, 31, 81, 136, 188, 196, 212, 148, 148]',\n",
       " '[29, 19, 17, 17, 32, 65, 212, 272, 278, 273, 283]',\n",
       " '[29, 47, 52, 54, 131, 301, 348, 984, 1896, 1886, 1893]',\n",
       " '[29, 24, 32, 62, 194, 537, 1291, 1208, 1110, 1056, 1022]',\n",
       " '[29, 26, 14, 20, 21, 46, 50, 76, 106, 165, 159]',\n",
       " '[29, 35, 39, 53, 65, 204, 656, 1301, 1446, 1436, 1591]',\n",
       " '[29, 45, 50, 58, 91, 137, 287, 403, 395, 390, 436]',\n",
       " '[29, 42, 41, 79, 146, 147, 193, 464, 518, 556, 615]',\n",
       " '[29, 25, 27, 51, 80, 213, 372, 4245, 4274, 4024, 4024]',\n",
       " '[130, 189, 199, 277, 394, 654, 268, 356, 340, 270, 256]',\n",
       " '[130, 184, 143, 155, 118, 64, 70, 86, 80, 76, 74]',\n",
       " '[130, 123, 122, 73, 46, 73, 219, 389, 427, 655, 1569]',\n",
       " '[130, 131, 187, 227, 299, 533, 229, 665, 645, 635, 642]',\n",
       " '[130, 174, 214, 201, 242, 616, 1412, 2651, 2226, 2213, 2077]',\n",
       " '[130, 132, 139, 116, 90, 133, 516, 496, 509, 498, 498]',\n",
       " '[130, 142, 107, 127, 152, 206, 323, 1503, 2843, 2592, 2258]',\n",
       " '[130, 137, 142, 134, 145, 143, 190, 237, 218, 192, 192]',\n",
       " '[130, 195, 198, 161, 309, 324, 549, 1026, 1016, 1036, 1032]',\n",
       " '[130, 173, 125, 108, 78, 127, 204, 342, 688, 731, 765]',\n",
       " '[7230, 7228, 7228, 7199, 6171, 6173, 6152, 6162, 6285, 6274, 6282]',\n",
       " '[7230, 7213, 5330, 7091, 6880, 1280, 1080, 949, 811, 738, 699]',\n",
       " '[1263, 1342, 765, 795, 766, 1703, 1664, 1648, 1648, 1630, 1740]',\n",
       " '[1263, 1364, 1388, 1461, 7034, 6785, 6796, 6796, 6787, 6750, 6725]',\n",
       " '[1263, 1183, 475, 371, 177, 153, 78, 70, 72, 64, 67]',\n",
       " '[1263, 1352, 1402, 1344, 6723, 6787, 6786, 6757, 6774, 6715, 6704]',\n",
       " '[1263, 1165, 1029, 511, 477, 476, 484, 494, 494, 497, 832]',\n",
       " '[1263, 1287, 744, 255, 339, 243, 379, 394, 440, 1315, 1385]',\n",
       " '[1263, 1361, 1423, 1554, 6746, 6716, 6706, 6775, 6706, 6713, 6845]',\n",
       " '[180, 211, 204, 209, 179, 102, 111, 93, 68, 60, 55]',\n",
       " '[180, 148, 171, 221, 156, 159, 186, 227, 272, 266, 251]',\n",
       " '[180, 222, 214, 209, 149, 139, 227, 343, 318, 298, 291]',\n",
       " '[180, 206, 207, 291, 271, 218, 206, 281, 271, 348, 357]',\n",
       " '[180, 255, 240, 222, 223, 470, 473, 518, 596, 1079, 1121]',\n",
       " '[180, 252, 215, 302, 232, 918, 955, 1914, 2010, 2061, 2029]',\n",
       " '[180, 228, 230, 141, 137, 156, 194, 271, 573, 560, 530]',\n",
       " '[180, 229, 222, 435, 402, 566, 363, 359, 572, 658, 302]',\n",
       " '[180, 243, 270, 318, 274, 248, 1430, 6745, 6728, 6726, 1463]',\n",
       " '[180, 253, 291, 238, 245, 236, 318, 300, 299, 281, 284]',\n",
       " '[49, 60, 99, 194, 163, 125, 86, 95, 108, 127, 121]',\n",
       " '[49, 69, 67, 81, 87, 196, 383, 652, 1448, 1365, 1330]',\n",
       " '[49, 80, 106, 87, 212, 190, 348, 596, 337, 671, 1468]',\n",
       " '[49, 41, 44, 65, 113, 589, 652, 709, 725, 1809, 5196]',\n",
       " '[49, 42, 47, 41, 56, 59, 68, 107, 119, 202, 212]',\n",
       " '[49, 64, 65, 60, 58, 99, 80, 79, 64, 55, 65]',\n",
       " '[49, 60, 76, 112, 276, 428, 588, 520, 294, 268, 264]',\n",
       " '[49, 38, 32, 33, 73, 63, 57, 83, 95, 140, 213]',\n",
       " '[49, 73, 97, 129, 177, 165, 109, 95, 76, 51, 52]',\n",
       " '[49, 49, 48, 49, 52, 121, 375, 592, 612, 663, 677]',\n",
       " '[134, 133, 146, 130, 301, 293, 207, 205, 208, 387, 385]',\n",
       " '[134, 174, 226, 315, 619, 430, 431, 863, 881, 1833, 1832]',\n",
       " '[134, 132, 129, 142, 257, 358, 331, 363, 649, 779, 748]',\n",
       " '[134, 146, 148, 184, 269, 650, 414, 590, 1616, 1645, 1679]',\n",
       " '[134, 138, 145, 108, 133, 178, 244, 491, 311, 569, 554]',\n",
       " '[134, 136, 102, 107, 116, 280, 511, 467, 488, 715, 716]',\n",
       " '[134, 159, 149, 94, 114, 130, 133, 96, 161, 252, 250]',\n",
       " '[134, 131, 126, 229, 246, 274, 200, 504, 708, 1079, 1052]',\n",
       " '[134, 180, 250, 365, 344, 622, 685, 2511, 1918, 2248, 2071]',\n",
       " '[134, 136, 103, 133, 141, 241, 260, 743, 2063, 2107, 1488]',\n",
       " '[117, 100, 115, 88, 78, 49, 142, 351, 386, 402, 427]',\n",
       " '[117, 123, 166, 168, 278, 1206, 2621, 1750, 2233, 2132, 2220]',\n",
       " '[117, 101, 114, 96, 110, 119, 277, 313, 362, 624, 631]',\n",
       " '[117, 127, 130, 194, 223, 573, 1510, 2507, 2581, 2772, 2295]',\n",
       " '[117, 112, 114, 144, 138, 201, 274, 273, 446, 448, 463]',\n",
       " '[117, 98, 89, 80, 58, 181, 1492, 2242, 2528, 2210, 2312]',\n",
       " '[117, 116, 110, 106, 157, 161, 395, 1637, 1628, 2706, 2143]',\n",
       " '[68, 59, 49, 40, 78, 96, 221, 121, 153, 142, 140]',\n",
       " '[68, 66, 80, 71, 92, 103, 29, 19, 37, 38, 38]',\n",
       " '[68, 72, 78, 65, 102, 148, 338, 192, 186, 76, 49]',\n",
       " '[68, 65, 59, 78, 126, 192, 198, 372, 462, 430, 453]',\n",
       " '[68, 54, 66, 93, 115, 150, 283, 247, 223, 195, 189]',\n",
       " '[68, 73, 63, 76, 37, 64, 145, 219, 232, 251, 261]',\n",
       " '[68, 65, 68, 88, 173, 290, 404, 408, 630, 670, 1335]',\n",
       " '[68, 65, 47, 55, 47, 91, 122, 147, 432, 432, 350]',\n",
       " '[68, 64, 59, 68, 78, 110, 102, 581, 769, 809, 509]',\n",
       " '[68, 65, 47, 38, 20, 14, 22, 123, 234, 196, 199]',\n",
       " '[211, 214, 339, 341, 361, 385, 653, 672, 629, 604, 602]',\n",
       " '[211, 214, 215, 209, 202, 204, 189, 143, 95, 85, 140]',\n",
       " '[211, 221, 236, 531, 616, 1397, 1690, 6163, 7201, 6162, 6149]',\n",
       " '[211, 213, 211, 211, 219, 223, 408, 428, 1773, 7341, 7343]',\n",
       " '[211, 312, 314, 179, 182, 175, 342, 294, 280, 166, 159]',\n",
       " '[211, 217, 163, 141, 129, 156, 89, 150, 139, 139, 145]',\n",
       " '[211, 220, 331, 497, 610, 1836, 6868, 6149, 7198, 7201, 6864]',\n",
       " '[211, 204, 314, 326, 359, 755, 1072, 914, 875, 861, 846]',\n",
       " '[211, 194, 173, 173, 102, 81, 81, 151, 131, 133, 134]',\n",
       " '[211, 210, 482, 249, 234, 282, 838, 843, 834, 866, 927]',\n",
       " '[174, 170, 122, 91, 180, 170, 111, 211, 138, 127, 117]',\n",
       " '[174, 182, 190, 199, 295, 773, 827, 479, 541, 1336, 1272]',\n",
       " '[174, 187, 182, 193, 502, 7124, 7123, 7116, 7153, 7153, 7109]',\n",
       " '[174, 172, 171, 115, 202, 225, 418, 1067, 1122, 1275, 1452]',\n",
       " '[174, 166, 155, 173, 348, 1258, 7074, 7063, 7076, 7055, 7076]',\n",
       " '[174, 155, 140, 95, 101, 130, 201, 304, 296, 284, 220]',\n",
       " '[174, 160, 125, 150, 121, 272, 243, 261, 267, 7260, 310]',\n",
       " '[174, 175, 168, 163, 117, 176, 575, 1162, 1164, 1207, 1249]',\n",
       " '[174, 186, 251, 239, 350, 969, 7142, 7147, 7157, 7147, 7137]',\n",
       " '[174, 125, 120, 116, 122, 312, 1260, 7125, 7125, 7149, 7153]',\n",
       " '[7024, 7014, 6977, 6779, 6745, 6758, 6706, 6674, 6740, 6672, 6677]',\n",
       " '[7024, 7012, 7015, 7010, 7008, 1631, 1362, 1296, 1004, 996, 983]',\n",
       " '[7024, 7012, 7008, 6878, 6763, 6882, 6745, 6722, 6725, 6731, 6734]',\n",
       " '[7024, 7010, 7028, 565, 342, 175, 139, 130, 120, 115, 139]',\n",
       " '[7024, 7005, 1103, 832, 646, 464, 372, 367, 398, 433, 447]',\n",
       " '[7024, 7020, 7008, 2062, 1944, 869, 498, 426, 275, 269, 258]',\n",
       " '[7024, 6990, 6992, 2418, 2368, 2048, 6748, 6741, 6735, 6700, 6671]',\n",
       " '[7024, 7020, 7009, 7017, 7007, 7018, 6752, 7000, 1759, 1582, 1414]',\n",
       " '[7024, 7015, 7001, 7005, 4714, 2281, 2159, 2121, 2178, 2193, 2196]',\n",
       " '[7024, 7029, 7014, 7027, 6998, 7003, 6742, 6724, 1709, 1653, 1583]',\n",
       " '[70, 67, 69, 113, 114, 283, 544, 555, 1287, 1414, 1508]',\n",
       " '[70, 67, 142, 157, 253, 459, 925, 427, 365, 325, 299]',\n",
       " '[70, 55, 53, 33, 47, 73, 73, 74, 69, 67, 96]',\n",
       " '[70, 65, 128, 95, 147, 94, 100, 144, 101, 100, 98]',\n",
       " '[70, 97, 169, 391, 417, 424, 495, 524, 602, 1433, 1546]',\n",
       " '[70, 92, 32, 27, 17, 15, 32, 46, 51, 55, 61]',\n",
       " '[70, 69, 60, 41, 52, 139, 211, 195, 322, 333, 334]',\n",
       " '[70, 91, 155, 219, 159, 497, 533, 604, 669, 1471, 1529]',\n",
       " '[70, 48, 38, 20, 11, 8, 6, 6, 6, 7, 10]',\n",
       " '[70, 90, 90, 141, 121, 209, 167, 255, 259, 256, 258]',\n",
       " '[132, 203, 205, 278, 293, 325, 209, 194, 188, 261, 180]',\n",
       " '[132, 69, 62, 112, 98, 93, 156, 160, 176, 108, 105]',\n",
       " '[132, 204, 258, 274, 347, 697, 679, 735, 477, 719, 729]',\n",
       " '[132, 318, 342, 895, 1115, 3902, 3448, 3874, 3504, 3869, 3886]',\n",
       " '[132, 214, 184, 293, 305, 254, 167, 154, 150, 149, 112]',\n",
       " '[132, 159, 206, 341, 334, 206, 204, 525, 978, 1001, 1050]',\n",
       " '[132, 107, 133, 240, 416, 1040, 4003, 4011, 4033, 4034, 4042]',\n",
       " '[132, 125, 185, 145, 87, 82, 112, 116, 169, 129, 127]',\n",
       " '[132, 208, 240, 204, 156, 23, 11, 12, 12, 8, 11]',\n",
       " '[132, 91, 47, 53, 33, 42, 56, 68, 66, 50, 55]',\n",
       " '[152, 85, 77, 73, 102, 212, 264, 276, 268, 273, 266]',\n",
       " '[152, 122, 135, 332, 290, 354, 305, 270, 155, 76, 69]',\n",
       " '[152, 127, 84, 124, 235, 334, 84, 82, 58, 41, 46]',\n",
       " '[152, 208, 235, 573, 996, 1077, 564, 1248, 1213, 1146, 1143]',\n",
       " '[152, 62, 50, 51, 55, 181, 160, 173, 215, 248, 311]',\n",
       " '[152, 120, 39, 26, 10, 11, 7, 7, 5, 5, 6]',\n",
       " '[152, 64, 78, 205, 352, 636, 1230, 1379, 1485, 1654, 6623]',\n",
       " '[152, 167, 64, 54, 64, 93, 94, 125, 120, 95, 101]',\n",
       " '[152, 130, 163, 64, 67, 38, 32, 32, 25, 24, 28]',\n",
       " '[152, 144, 88, 142, 145, 102, 83, 70, 69, 60, 61]',\n",
       " '[1404, 973, 1054, 1086, 1164, 2345, 2573, 7440, 7385, 7382, 7383]',\n",
       " '[1404, 948, 398, 155, 111, 78, 71, 456, 840, 792, 749]',\n",
       " '[1404, 490, 285, 240, 138, 88, 111, 185, 175, 179, 122]',\n",
       " '[1404, 963, 986, 998, 1051, 867, 948, 1517, 2210, 2270, 2216]',\n",
       " '[1404, 970, 569, 428, 427, 602, 492, 1162, 1020, 500, 494]',\n",
       " '[1404, 680, 610, 540, 628, 736, 1443, 1595, 1727, 5126, 5111]',\n",
       " '[1404, 602, 190, 84, 54, 48, 48, 100, 105, 104, 99]',\n",
       " '[1404, 580, 255, 197, 175, 104, 126, 108, 90, 85, 85]',\n",
       " '[37, 35, 36, 42, 137, 128, 147, 192, 373, 374, 414]',\n",
       " '[37, 50, 46, 29, 47, 68, 97, 83, 120, 176, 126]',\n",
       " '[37, 47, 56, 53, 86, 121, 147, 153, 163, 158, 173]',\n",
       " '[37, 37, 58, 47, 34, 71, 62, 74, 108, 131, 224]',\n",
       " '[37, 36, 29, 51, 128, 127, 266, 697, 338, 308, 270]',\n",
       " '[37, 29, 32, 18, 16, 14, 12, 16, 21, 30, 42]',\n",
       " '[37, 39, 55, 102, 180, 129, 111, 103, 172, 150, 145]',\n",
       " '[37, 25, 34, 17, 14, 17, 22, 27, 46, 48, 36]',\n",
       " '[37, 37, 31, 32, 44, 31, 63, 64, 65, 51, 50]',\n",
       " '[37, 35, 50, 50, 45, 26, 39, 39, 37, 36, 32]',\n",
       " '[338, 398, 429, 322, 487, 461, 742, 1209, 950, 4543, 4522]',\n",
       " '[338, 345, 368, 433, 903, 2137, 4952, 4975, 4940, 4908, 4854]',\n",
       " '[338, 387, 380, 362, 553, 1010, 1154, 1841, 1770, 696, 689]',\n",
       " '[338, 338, 336, 979, 1085, 4307, 4277, 4289, 4293, 4281, 4301]',\n",
       " '[338, 299, 270, 222, 329, 332, 338, 817, 887, 1002, 1076]',\n",
       " '[338, 311, 294, 266, 268, 1191, 1303, 1418, 1616, 4360, 4357]',\n",
       " '[338, 294, 302, 326, 748, 1913, 1135, 1072, 1032, 967, 976]',\n",
       " '[338, 303, 271, 380, 437, 523, 1544, 5446, 5398, 5608, 5989]',\n",
       " '[338, 351, 363, 405, 1634, 4317, 4323, 4361, 4337, 4320, 4322]',\n",
       " '[338, 330, 370, 558, 1309, 4339, 4388, 4375, 4368, 4364, 4369]',\n",
       " '[1543, 1441, 1418, 1351, 1336, 1404, 1446, 1567, 1592, 1702, 5176]',\n",
       " '[1543, 1578, 1745, 1785, 1779, 1717, 1687, 1612, 1684, 1644, 1654]',\n",
       " '[1543, 1692, 2128, 1129, 1143, 1241, 1320, 1414, 1464, 2591, 2543]',\n",
       " '[1543, 1637, 1712, 1815, 5221, 5250, 5259, 5295, 5289, 5292, 5269]',\n",
       " '[1543, 1603, 1740, 1793, 5177, 5185, 5163, 5173, 5177, 5169, 5184]',\n",
       " '[1543, 1614, 1720, 690, 912, 747, 721, 401, 398, 354, 373]',\n",
       " '[1543, 1740, 1025, 1041, 1556, 1623, 1687, 1794, 1919, 1952, 2087]',\n",
       " '[1543, 1425, 1354, 1146, 1017, 1067, 1207, 1341, 1494, 1601, 1652]',\n",
       " '[1543, 1705, 1745, 2098, 1014, 1834, 935, 870, 847, 834, 853]',\n",
       " '[1543, 1511, 1683, 1704, 692, 913, 908, 950, 1003, 1023, 1107]',\n",
       " '[503, 489, 460, 426, 425, 621, 624, 629, 1546, 1622, 1619]',\n",
       " '[503, 495, 503, 495, 472, 963, 4181, 4143, 3059, 4212, 4213]',\n",
       " '[503, 492, 471, 425, 494, 3238, 3553, 3429, 4248, 3428, 4247]',\n",
       " '[141, 135, 102, 62, 35, 27, 26, 28, 33, 55, 99]',\n",
       " '[141, 143, 134, 96, 73, 57, 143, 304, 359, 1563, 3896]',\n",
       " '[141, 148, 193, 320, 311, 215, 175, 185, 318, 305, 294]',\n",
       " '[141, 146, 205, 207, 146, 285, 1727, 2662, 3130, 3084, 3080]',\n",
       " '[141, 124, 98, 96, 49, 59, 20, 14, 17, 16, 20]',\n",
       " '[141, 136, 144, 104, 72, 63, 104, 267, 187, 245, 246]',\n",
       " '[141, 92, 42, 7, 4, 2, 1, 1, 1, 0, 0]',\n",
       " '[141, 162, 150, 124, 272, 375, 647, 871, 1963, 1939, 1938]',\n",
       " '[496, 484, 501, 475, 538, 665, 698, 1347, 6406, 2456, 2456]',\n",
       " '[496, 500, 538, 586, 687, 843, 1195, 2253, 2031, 2517, 2193]',\n",
       " '[496, 477, 484, 295, 263, 293, 331, 388, 479, 881, 849]',\n",
       " '[57, 37, 25, 15, 9, 9, 10, 6, 4, 8, 8]',\n",
       " '[57, 42, 30, 23, 24, 23, 16, 16, 11, 9, 9]',\n",
       " '[57, 52, 60, 70, 77, 56, 65, 77, 108, 108, 93]',\n",
       " '[57, 33, 21, 17, 23, 22, 18, 19, 19, 19, 27]',\n",
       " '[57, 47, 34, 27, 19, 18, 10, 42, 34, 26, 45]',\n",
       " '[57, 54, 55, 58, 46, 68, 56, 53, 90, 57, 91]',\n",
       " '[57, 52, 48, 31, 19, 24, 10, 10, 8, 8, 7]',\n",
       " '[57, 48, 18, 21, 21, 20, 25, 22, 26, 28, 35]',\n",
       " '[57, 52, 63, 57, 46, 107, 621, 1448, 6962, 7009, 5226]',\n",
       " '[57, 77, 87, 76, 65, 95, 164, 137, 127, 116, 114]',\n",
       " '[83, 76, 74, 92, 93, 86, 199, 375, 191, 201, 1131]',\n",
       " '[83, 49, 41, 36, 45, 79, 111, 323, 183, 114, 235]',\n",
       " '[83, 75, 38, 27, 23, 6, 4, 2, 3, 2, 1]',\n",
       " '[83, 53, 53, 61, 60, 55, 28, 36, 32, 29, 28]',\n",
       " '[83, 88, 94, 85, 176, 283, 1144, 1346, 1753, 1721, 1648]',\n",
       " '[83, 91, 101, 146, 199, 555, 560, 611, 624, 696, 739]',\n",
       " '[83, 72, 78, 72, 74, 61, 30, 22, 54, 53, 47]',\n",
       " '[83, 122, 123, 102, 155, 124, 182, 206, 257, 1069, 1090]',\n",
       " '[83, 45, 37, 19, 13, 6, 3, 3, 4, 4, 6]',\n",
       " '[83, 103, 94, 138, 383, 1144, 6989, 7061, 7068, 7064, 1897]',\n",
       " '[82, 86, 72, 54, 51, 57, 47, 60, 58, 72, 46]',\n",
       " '[82, 74, 64, 64, 31, 54, 66, 116, 262, 377, 379]',\n",
       " '[82, 104, 99, 101, 199, 200, 302, 649, 678, 1446, 1428]',\n",
       " '[82, 67, 59, 55, 25, 70, 45, 42, 42, 40, 40]',\n",
       " '[82, 40, 21, 4, 3, 1, 0, 0, 0, 0, 0]',\n",
       " '[82, 99, 94, 226, 333, 178, 177, 173, 335, 288, 272]',\n",
       " '[82, 124, 165, 119, 139, 143, 389, 359, 170, 156, 144]',\n",
       " '[82, 80, 99, 126, 161, 402, 890, 4671, 4675, 1594, 1589]',\n",
       " '[82, 68, 75, 112, 102, 117, 104, 171, 108, 101, 112]',\n",
       " '[82, 68, 49, 41, 33, 35, 24, 23, 29, 40, 50]',\n",
       " '[336, 321, 301, 303, 442, 380, 203, 293, 333, 399, 377]',\n",
       " '[336, 325, 415, 404, 178, 151, 158, 268, 685, 713, 528]',\n",
       " '[336, 319, 304, 294, 302, 466, 868, 954, 559, 554, 536]',\n",
       " '[336, 319, 430, 426, 268, 133, 134, 118, 141, 129, 123]',\n",
       " '[336, 504, 593, 1097, 2290, 2004, 1750, 600, 263, 260, 134]',\n",
       " '[336, 352, 514, 512, 533, 1082, 944, 962, 946, 520, 508]',\n",
       " '[336, 343, 562, 958, 411, 332, 126, 105, 94, 81, 70]',\n",
       " '[336, 307, 182, 153, 93, 86, 83, 62, 65, 134, 127]',\n",
       " '[336, 312, 308, 413, 219, 211, 205, 215, 212, 309, 581]',\n",
       " '[336, 335, 335, 366, 513, 702, 659, 653, 490, 466, 435]',\n",
       " '[689, 678, 689, 693, 553, 605, 754, 1097, 5790, 5791, 5760]',\n",
       " '[689, 679, 460, 427, 355, 385, 671, 1056, 5791, 5780, 5781]',\n",
       " '[689, 660, 598, 619, 595, 642, 918, 1486, 2038, 5476, 5517]',\n",
       " '[689, 677, 455, 456, 440, 381, 1001, 1432, 1744, 6963, 6980]',\n",
       " '[689, 680, 452, 459, 444, 792, 937, 1362, 1709, 5786, 5780]',\n",
       " '[689, 665, 660, 620, 618, 662, 722, 872, 1228, 1605, 5793]',\n",
       " '[689, 684, 706, 751, 747, 428, 405, 414, 440, 597, 532]',\n",
       " '[689, 674, 635, 659, 734, 941, 1190, 2229, 2216, 2202, 2197]',\n",
       " '[689, 673, 662, 714, 432, 426, 431, 453, 701, 5803, 5793]',\n",
       " '[689, 650, 650, 635, 650, 674, 772, 1016, 1401, 1632, 5796]',\n",
       " '[41, 32, 58, 63, 205, 209, 232, 388, 352, 304, 276]',\n",
       " '[41, 42, 61, 65, 81, 218, 351, 242, 168, 171, 111]',\n",
       " '[41, 31, 27, 42, 31, 86, 183, 216, 486, 472, 467]',\n",
       " '[41, 31, 32, 46, 62, 127, 197, 1370, 1381, 6427, 6394]',\n",
       " '[41, 31, 48, 36, 30, 20, 23, 60, 88, 77, 73]',\n",
       " '[41, 30, 29, 56, 37, 41, 63, 54, 46, 51, 49]',\n",
       " '[41, 38, 81, 89, 205, 234, 284, 1343, 1504, 2410, 2005]',\n",
       " '[41, 36, 36, 58, 144, 456, 527, 554, 569, 573, 612]',\n",
       " '[41, 31, 38, 39, 165, 378, 541, 1394, 1371, 1330, 1285]',\n",
       " '[41, 52, 63, 73, 169, 309, 377, 757, 726, 709, 698]',\n",
       " '[78, 73, 66, 64, 91, 193, 295, 222, 229, 212, 210]',\n",
       " '[78, 84, 77, 95, 67, 81, 99, 130, 135, 129, 130]',\n",
       " '[78, 77, 50, 52, 53, 45, 46, 27, 28, 26, 20]',\n",
       " '[78, 94, 117, 126, 270, 838, 2333, 2575, 2539, 2297, 2526]',\n",
       " '[78, 57, 64, 112, 109, 137, 137, 183, 379, 417, 587]',\n",
       " '[78, 75, 103, 108, 118, 142, 120, 88, 164, 151, 138]',\n",
       " '[78, 72, 74, 94, 155, 234, 759, 1892, 1982, 1965, 1918]',\n",
       " '[78, 68, 86, 57, 31, 20, 20, 24, 25, 32, 38]',\n",
       " '[78, 79, 78, 87, 98, 122, 138, 173, 321, 300, 291]',\n",
       " '[78, 75, 130, 150, 676, 622, 697, 652, 1498, 1442, 1414]',\n",
       " '[4446, 4185, 1571, 757, 2063, 1873, 1729, 1695, 518, 499, 480]',\n",
       " '[4446, 4182, 4135, 3977, 3930, 3920, 3876, 3870, 3805, 3818, 3797]',\n",
       " '[4446, 4208, 1484, 1308, 1113, 1079, 681, 648, 395, 385, 376]',\n",
       " '[4446, 4229, 1077, 821, 667, 542, 459, 442, 411, 405, 397]',\n",
       " '[4446, 2041, 1883, 1855, 1615, 1608, 1630, 1699, 1694, 1740, 1841]',\n",
       " '[4446, 4439, 1763, 1683, 1730, 4471, 4099, 1953, 1858, 1638, 1463]',\n",
       " '[4446, 3731, 1803, 652, 613, 541, 1558, 1724, 4044, 1634, 1561]',\n",
       " '[4446, 4502, 4105, 4495, 1686, 1533, 1439, 1422, 883, 882, 858]',\n",
       " '[4446, 4189, 1455, 678, 345, 312, 283, 269, 430, 270, 272]',\n",
       " '[4446, 4226, 4206, 4301, 3962, 3924, 3794, 3748, 3747, 4145, 3715]',\n",
       " '[265, 265, 195, 225, 88, 63, 65, 60, 42, 38, 50]',\n",
       " '[265, 267, 219, 353, 365, 337, 445, 768, 712, 684, 595]',\n",
       " '[265, 294, 471, 379, 383, 1361, 484, 226, 372, 210, 201]',\n",
       " '[265, 242, 320, 190, 127, 101, 134, 165, 376, 394, 400]',\n",
       " '[265, 265, 257, 190, 197, 401, 338, 339, 543, 501, 487]',\n",
       " '[265, 233, 209, 211, 54, 60, 48, 106, 59, 38, 58]',\n",
       " '[265, 219, 117, 41, 38, 15, 8, 7, 3, 3, 3]',\n",
       " '[265, 252, 227, 135, 88, 88, 98, 161, 251, 150, 135]',\n",
       " '[265, 250, 136, 76, 52, 34, 26, 18, 16, 10, 9]',\n",
       " '[265, 257, 271, 171, 147, 141, 188, 238, 287, 505, 1121]',\n",
       " '[1384, 1518, 1313, 992, 930, 852, 886, 917, 1001, 1058, 1190]',\n",
       " '[1384, 1357, 1378, 680, 360, 336, 485, 440, 250, 252, 238]',\n",
       " '[1384, 1378, 1600, 1369, 1314, 685, 774, 1609, 2033, 2793, 2441]',\n",
       " '[1384, 1355, 685, 980, 1174, 1186, 2458, 2435, 2714, 2761, 2499]',\n",
       " '[1384, 1335, 1282, 1249, 1424, 1442, 1652, 2589, 1320, 1234, 1173]',\n",
       " '[1384, 1640, 736, 593, 581, 422, 222, 206, 367, 353, 340]',\n",
       " '[1384, 1670, 512, 511, 454, 610, 331, 576, 603, 1477, 1483]',\n",
       " '[1384, 1480, 768, 1281, 1285, 1325, 2540, 3289, 3003, 2789, 2826]',\n",
       " '[1384, 742, 474, 253, 297, 521, 459, 1213, 1224, 1230, 1232]',\n",
       " '[1384, 1402, 1463, 1227, 340, 144, 67, 33, 25, 21, 26]',\n",
       " '[267, 166, 30, 7, 3, 0, 0, 0, 0, 0, 0]',\n",
       " '[267, 263, 277, 239, 151, 179, 198, 261, 382, 384, 376]',\n",
       " '[267, 295, 456, 364, 385, 438, 1286, 1358, 1438, 2468, 2851]',\n",
       " '[267, 262, 329, 340, 363, 419, 1783, 1743, 1691, 2073, 2117]',\n",
       " '[267, 265, 156, 168, 183, 127, 199, 101, 88, 86, 70]',\n",
       " '[267, 283, 280, 383, 413, 385, 822, 1140, 1938, 1934, 1907]',\n",
       " '[267, 257, 188, 84, 35, 16, 16, 22, 20, 16, 17]',\n",
       " '[267, 231, 210, 149, 131, 96, 111, 124, 120, 113, 107]',\n",
       " '[267, 272, 248, 204, 210, 239, 557, 353, 428, 463, 680]',\n",
       " '[267, 297, 308, 336, 472, 531, 547, 1418, 1511, 1558, 1618]',\n",
       " '[347, 341, 503, 684, 662, 1018, 1020, 1079, 1161, 1284, 756]',\n",
       " '[347, 304, 123, 118, 80, 41, 25, 22, 24, 29, 18]',\n",
       " '[347, 473, 809, 695, 1202, 2031, 6480, 6489, 6494, 6489, 6499]',\n",
       " '[347, 320, 229, 96, 58, 48, 48, 47, 36, 31, 33]',\n",
       " '[347, 452, 437, 268, 201, 127, 94, 96, 96, 89, 99]',\n",
       " '[347, 345, 357, 1154, 1323, 2314, 6061, 6060, 7213, 7267, 7212]',\n",
       " '[347, 447, 860, 914, 890, 503, 504, 446, 422, 240, 239]',\n",
       " '[347, 462, 486, 471, 493, 687, 1582, 792, 702, 689, 660]',\n",
       " '[347, 476, 992, 2470, 2539, 5496, 5506, 7149, 6972, 6960, 6964]',\n",
       " '[347, 324, 409, 277, 257, 195, 156, 192, 185, 145, 138]',\n",
       " '[1732, 1718, 739, 500, 220, 163, 141, 100, 88, 102, 95]',\n",
       " '[1732, 1715, 1574, 1303, 170, 105, 98, 141, 113, 107, 76]',\n",
       " '[1732, 1722, 1827, 1916, 5920, 5893, 1110, 1004, 927, 879, 843]',\n",
       " '[1732, 1905, 6005, 6014, 2018, 2078, 1996, 6021, 6023, 5995, 6017]',\n",
       " '[1732, 1806, 1689, 1802, 5989, 1157, 949, 401, 384, 359, 344]',\n",
       " '[1732, 1850, 2022, 6012, 5926, 5971, 5907, 6014, 6036, 6038, 6046]',\n",
       " '[1732, 1685, 1675, 1015, 488, 312, 268, 238, 204, 170, 162]',\n",
       " '[1732, 1684, 1802, 1780, 1839, 6008, 6009, 5918, 5913, 5898, 5902]',\n",
       " '[1732, 1767, 1746, 1851, 1899, 6005, 5977, 5995, 5987, 5988, 6007]',\n",
       " '[1732, 1754, 1445, 715, 860, 352, 301, 111, 104, 89, 87]',\n",
       " '[7097, 7127, 2037, 7114, 7115, 7241, 7268, 7251, 7222, 7189, 6127]',\n",
       " '[7097, 7116, 7132, 7242, 7180, 6131, 6138, 6152, 6098, 6097, 6132]',\n",
       " '[7097, 7099, 1972, 1871, 1763, 1551, 1474, 1598, 1649, 1579, 1717]',\n",
       " '[7097, 7130, 7134, 7098, 7258, 7193, 7192, 7196, 7201, 7192, 7208]',\n",
       " '[7097, 7117, 7135, 7173, 7213, 6134, 1893, 1915, 1929, 1935, 1932]',\n",
       " '[7097, 7101, 1962, 1687, 1565, 655, 545, 526, 532, 536, 549]',\n",
       " '[7097, 7134, 7130, 753, 1203, 828, 690, 660, 659, 629, 612]',\n",
       " '[7097, 7115, 7118, 7184, 7170, 7182, 5533, 5299, 6961, 6984, 7084]',\n",
       " '[7097, 7100, 7135, 7123, 7126, 7263, 7265, 6136, 6128, 6117, 6115]',\n",
       " '[7097, 7116, 1753, 1507, 1501, 1492, 7195, 1470, 1381, 1277, 1241]',\n",
       " '[1897, 1955, 1933, 7197, 7119, 7228, 7222, 7103, 7110, 7224, 7225]',\n",
       " '[1897, 1848, 1636, 1523, 1499, 1400, 1419, 1376, 7163, 7158, 7183]',\n",
       " '[370, 324, 317, 313, 643, 654, 657, 702, 708, 549, 582]',\n",
       " '[370, 379, 551, 829, 760, 671, 706, 1206, 1285, 6806, 6800]',\n",
       " '[370, 349, 187, 168, 206, 98, 189, 159, 89, 116, 99]',\n",
       " '[370, 248, 529, 495, 311, 564, 339, 314, 306, 298, 208]',\n",
       " '[370, 341, 132, 87, 63, 37, 39, 37, 39, 31, 29]',\n",
       " '[370, 381, 626, 1127, 1181, 2685, 2535, 6891, 6886, 6817, 6799]',\n",
       " '[370, 355, 348, 188, 136, 160, 133, 141, 130, 129, 156]',\n",
       " '[370, 343, 330, 321, 619, 954, 385, 348, 316, 305, 305]',\n",
       " '[370, 363, 407, 642, 1045, 1071, 1991, 6901, 6900, 6902, 6801]',\n",
       " '[370, 346, 474, 499, 526, 1563, 1636, 1787, 1760, 1923, 2052]',\n",
       " '[119, 124, 93, 83, 46, 71, 93, 92, 58, 131, 130]',\n",
       " '[119, 196, 360, 279, 273, 313, 1677, 1056, 1116, 1175, 1190]',\n",
       " '[119, 135, 99, 131, 74, 68, 51, 68, 63, 60, 61]',\n",
       " '[119, 119, 98, 126, 119, 254, 241, 431, 275, 248, 207]',\n",
       " '[119, 109, 88, 53, 24, 20, 6, 9, 8, 9, 9]',\n",
       " '[119, 115, 90, 69, 38, 53, 88, 102, 79, 86, 85]',\n",
       " '[119, 134, 82, 78, 33, 78, 103, 167, 183, 356, 610]',\n",
       " '[119, 192, 187, 175, 168, 177, 372, 324, 312, 297, 287]',\n",
       " '[119, 105, 110, 54, 15, 15, 11, 8, 8, 8, 8]',\n",
       " '[119, 151, 209, 387, 804, 2432, 2450, 4976, 4987, 4967, 4952]',\n",
       " '[406, 440, 1028, 2808, 3374, 3101, 3426, 3719, 3591, 3998, 3463]',\n",
       " '[406, 403, 387, 399, 812, 1909, 1868, 1826, 1810, 1800, 1784]',\n",
       " '[406, 407, 377, 377, 354, 362, 296, 311, 356, 748, 732]',\n",
       " '[406, 353, 308, 254, 214, 121, 127, 227, 264, 294, 603]',\n",
       " '[406, 425, 417, 492, 273, 511, 411, 241, 263, 268, 427]',\n",
       " '[406, 455, 489, 1004, 1157, 1637, 1417, 1289, 585, 563, 546]',\n",
       " '[406, 412, 421, 445, 469, 521, 489, 427, 389, 366, 365]',\n",
       " '[406, 432, 850, 841, 375, 355, 347, 289, 287, 285, 283]',\n",
       " '[406, 411, 421, 116, 136, 99, 112, 104, 109, 109, 114]',\n",
       " '[406, 377, 338, 337, 348, 360, 843, 1018, 2687, 2716, 2712]',\n",
       " '[282, 175, 155, 241, 224, 225, 219, 4236, 4248, 4253, 1181]',\n",
       " '[282, 294, 320, 633, 343, 413, 347, 148, 132, 128, 243]',\n",
       " '[282, 199, 283, 452, 755, 743, 362, 228, 206, 192, 174]',\n",
       " '[282, 163, 103, 28, 6, 1, 1, 1, 3, 5, 7]',\n",
       " '[282, 211, 299, 303, 304, 496, 543, 1263, 1352, 4210, 3591]',\n",
       " '[282, 173, 155, 218, 183, 102, 66, 52, 36, 29, 27]',\n",
       " '[282, 185, 262, 239, 135, 78, 52, 60, 58, 58, 55]',\n",
       " '[282, 285, 101, 60, 68, 73, 63, 58, 68, 75, 71]',\n",
       " '[282, 292, 493, 513, 1134, 1388, 4294, 4345, 4298, 4299, 4324]',\n",
       " '[282, 326, 537, 348, 371, 373, 197, 191, 188, 178, 234]',\n",
       " '[4140, 1333, 1238, 1155, 706, 670, 696, 1606, 1623, 1614, 1633]',\n",
       " '[4140, 3350, 1629, 1529, 1369, 1199, 1298, 4143, 4132, 4129, 4130]',\n",
       " '[4140, 4167, 3535, 3518, 4153, 4208, 4227, 4277, 3492, 3757, 4273]',\n",
       " '[4140, 1652, 1394, 1222, 1114, 1113, 1147, 1263, 1439, 1585, 1649]',\n",
       " '[4140, 4170, 1791, 1797, 1761, 3981, 4238, 4202, 4193, 4191, 4176]',\n",
       " '[4140, 3336, 3580, 4140, 4088, 3960, 4098, 4237, 3979, 4196, 4109]',\n",
       " '[4140, 4163, 4174, 4166, 4217, 4228, 4145, 4110, 3959, 3028, 4159]',\n",
       " '[4140, 4129, 3190, 4205, 3471, 4256, 4199, 4157, 3195, 3334, 4219]',\n",
       " '[4140, 1394, 1414, 1503, 1582, 1627, 3868, 3404, 2750, 2946, 2722]',\n",
       " '[4140, 4161, 4171, 4161, 4208, 4233, 4158, 1846, 1832, 1842, 1836]',\n",
       " '[135, 122, 143, 182, 71, 95, 86, 94, 122, 122, 126]',\n",
       " '[135, 130, 139, 206, 302, 353, 526, 1045, 1105, 1135, 1142]',\n",
       " '[135, 186, 263, 258, 461, 736, 1694, 1721, 1788, 4131, 3937]',\n",
       " '[135, 97, 76, 129, 124, 170, 695, 689, 740, 830, 868]',\n",
       " '[135, 96, 75, 87, 77, 74, 53, 69, 96, 105, 110]',\n",
       " '[135, 98, 88, 96, 118, 124, 122, 122, 140, 264, 581]',\n",
       " '[135, 84, 74, 57, 67, 68, 111, 116, 179, 255, 253]',\n",
       " '[135, 190, 196, 270, 281, 280, 218, 155, 239, 232, 348]',\n",
       " '[135, 81, 50, 37, 49, 50, 79, 64, 102, 95, 96]',\n",
       " '[135, 128, 77, 89, 75, 75, 75, 139, 174, 294, 608]',\n",
       " '[940, 957, 710, 702, 695, 678, 841, 1864, 1854, 1844, 1794]',\n",
       " '[940, 564, 571, 591, 397, 630, 642, 606, 618, 633, 667]',\n",
       " '[940, 566, 542, 548, 565, 951, 913, 951, 1099, 1202, 1322]',\n",
       " '[940, 942, 1019, 1137, 1223, 1343, 1497, 1755, 2061, 3358, 3358]',\n",
       " '[940, 987, 1035, 1174, 1337, 3854, 3672, 3720, 1887, 3711, 2019]',\n",
       " '[940, 937, 941, 1033, 660, 1697, 1668, 1660, 1658, 1684, 3863]',\n",
       " '[940, 927, 923, 1055, 533, 1266, 1215, 1210, 1227, 1345, 1371]',\n",
       " '[940, 507, 480, 451, 375, 399, 513, 1038, 1391, 1437, 1516]',\n",
       " '[940, 840, 797, 744, 807, 816, 866, 995, 1153, 1232, 1306]',\n",
       " '[940, 1059, 411, 547, 502, 478, 374, 324, 290, 279, 275]',\n",
       " '[860, 831, 1191, 749, 764, 1923, 1503, 1503, 1438, 890, 879]',\n",
       " '[860, 415, 346, 329, 315, 459, 392, 355, 363, 217, 219]',\n",
       " '[860, 798, 1013, 889, 457, 349, 328, 210, 200, 189, 132]',\n",
       " '[860, 842, 2054, 2009, 547, 425, 403, 350, 361, 354, 357]',\n",
       " '[860, 876, 6640, 6627, 6653, 6641, 6625, 2106, 2054, 2063, 2054]',\n",
       " '[860, 819, 787, 1109, 6648, 6644, 6630, 6509, 6517, 6502, 6513]',\n",
       " '[860, 6510, 6506, 6636, 2035, 1902, 1874, 1821, 1841, 824, 690]',\n",
       " '[860, 826, 776, 531, 1187, 1273, 1295, 1356, 1728, 1728, 1747]',\n",
       " '[860, 862, 788, 1856, 1607, 1935, 1973, 6529, 6489, 6514, 6514]',\n",
       " '[860, 2066, 954, 1256, 984, 916, 497, 1018, 1085, 1139, 1228]',\n",
       " '[14, 16, 14, 15, 9, 5, 4, 3, 4, 3, 3]',\n",
       " '[14, 15, 20, 19, 28, 52, 72, 65, 120, 121, 118]',\n",
       " '[14, 12, 4, 2, 1, 0, 0, 0, 0, 0, 0]',\n",
       " '[14, 18, 16, 19, 15, 20, 48, 62, 97, 99, 163]',\n",
       " '[14, 13, 12, 27, 20, 23, 24, 27, 55, 103, 98]',\n",
       " '[14, 10, 5, 2, 1, 1, 1, 0, 0, 0, 0]',\n",
       " '[14, 17, 14, 8, 8, 3, 3, 2, 2, 2, 2]',\n",
       " '[14, 16, 8, 6, 6, 5, 6, 10, 7, 7, 9]',\n",
       " '[14, 13, 13, 10, 11, 5, 11, 11, 15, 14, 15]',\n",
       " '[14, 14, 10, 10, 11, 23, 48, 106, 64, 157, 173]',\n",
       " '[1952, 820, 769, 820, 811, 1300, 5863, 5862, 5846, 5817, 5814]',\n",
       " '[1952, 856, 792, 811, 1950, 5804, 5734, 5767, 5770, 5781, 5757]',\n",
       " '[1952, 1952, 1849, 1852, 1914, 1890, 5691, 5801, 5813, 5794, 5817]',\n",
       " '[1952, 1878, 1759, 1784, 5794, 5801, 5793, 5799, 5799, 5793, 5789]',\n",
       " '[1952, 1951, 1928, 1844, 745, 395, 404, 224, 203, 189, 125]',\n",
       " '[1952, 808, 807, 819, 2133, 2199, 5856, 5831, 5842, 5835, 5840]',\n",
       " '[1952, 797, 815, 832, 560, 598, 1190, 435, 400, 377, 360]',\n",
       " '[1952, 1848, 1800, 1613, 750, 657, 401, 365, 359, 342, 330]',\n",
       " '[1952, 816, 816, 818, 1030, 1026, 1192, 5870, 5895, 5882, 5874]',\n",
       " '[1952, 801, 741, 961, 954, 432, 373, 358, 342, 339, 360]',\n",
       " '[60, 93, 93, 79, 97, 200, 205, 627, 288, 287, 281]',\n",
       " '[60, 55, 167, 180, 277, 286, 334, 380, 394, 728, 743]',\n",
       " '[60, 62, 63, 110, 76, 265, 284, 849, 841, 496, 494]',\n",
       " '[60, 62, 45, 44, 39, 49, 73, 132, 916, 1635, 1613]',\n",
       " '[60, 80, 100, 92, 89, 92, 74, 81, 124, 225, 205]',\n",
       " '[60, 59, 46, 63, 71, 132, 124, 115, 61, 56, 53]',\n",
       " '[60, 59, 180, 157, 99, 170, 40, 35, 34, 33, 32]',\n",
       " '[60, 62, 77, 62, 142, 192, 215, 227, 244, 242, 250]',\n",
       " '[60, 58, 93, 135, 132, 117, 166, 293, 296, 204, 414]',\n",
       " '[60, 97, 125, 75, 68, 64, 53, 117, 86, 96, 80]',\n",
       " '[299, 292, 285, 408, 437, 1519, 3339, 4364, 4362, 4369, 4381]',\n",
       " '[299, 305, 485, 552, 1700, 1941, 4406, 3600, 4238, 3720, 3307]',\n",
       " '[299, 394, 160, 143, 88, 57, 42, 35, 28, 32, 24]',\n",
       " '[299, 271, 250, 131, 124, 126, 97, 77, 133, 138, 138]',\n",
       " '[643, 597, 598, 362, 380, 374, 382, 662, 1075, 1119, 1141]',\n",
       " '[643, 1046, 983, 376, 335, 203, 178, 119, 162, 452, 349]',\n",
       " '[643, 666, 677, 765, 795, 492, 346, 543, 534, 871, 913]',\n",
       " '[643, 634, 626, 282, 119, 44, 39, 36, 30, 35, 39]',\n",
       " '[303, 450, 455, 1910, 1892, 1908, 1924, 1880, 1865, 1944, 2369]',\n",
       " '[303, 193, 250, 343, 337, 343, 361, 272, 997, 1646, 1582]',\n",
       " '[303, 290, 790, 380, 350, 324, 327, 349, 380, 395, 420]',\n",
       " '[303, 190, 178, 259, 268, 1603, 1761, 2059, 3049, 3186, 2931]',\n",
       " '[303, 290, 875, 1871, 875, 916, 1367, 1460, 2546, 2730, 2544]',\n",
       " '[303, 282, 167, 632, 571, 575, 783, 373, 347, 338, 345]',\n",
       " '[303, 757, 796, 831, 2191, 2056, 2452, 1514, 1405, 1343, 1282]',\n",
       " '[303, 431, 838, 804, 796, 880, 1082, 3233, 3189, 2758, 3110]',\n",
       " '[303, 281, 246, 252, 247, 109, 84, 92, 71, 80, 75]',\n",
       " '[303, 211, 292, 171, 152, 470, 569, 499, 477, 460, 617]',\n",
       " '[6919, 6895, 6916, 6927, 6931, 6927, 6924, 6900, 6933, 6928, 6920]',\n",
       " '[827, 888, 1044, 1058, 1244, 4927, 6939, 6947, 6944, 6952, 6961]',\n",
       " '[827, 773, 516, 528, 502, 431, 393, 383, 307, 333, 579]',\n",
       " '[827, 481, 293, 141, 105, 116, 110, 79, 81, 116, 120]',\n",
       " '[827, 542, 343, 462, 446, 449, 503, 519, 548, 615, 679]',\n",
       " '[827, 854, 1048, 1249, 6939, 6930, 6949, 6952, 6945, 6947, 6942]',\n",
       " '[827, 956, 925, 338, 318, 312, 317, 344, 586, 567, 581]',\n",
       " '[827, 795, 952, 1123, 6912, 6862, 6888, 1641, 1634, 1617, 1691]',\n",
       " '[827, 738, 641, 589, 401, 367, 349, 445, 515, 794, 955]',\n",
       " '[827, 786, 497, 427, 453, 1022, 773, 792, 803, 819, 843]',\n",
       " '[3104, 3532, 3485, 3436, 3384, 2884, 3149, 3169, 3190, 3190, 3187]',\n",
       " '[3104, 3596, 3493, 4222, 3687, 3387, 3496, 3227, 3914, 3125, 2939]',\n",
       " '[3104, 3109, 3315, 3554, 3565, 3114, 3284, 1607, 1601, 3229, 3200]',\n",
       " '[3104, 3595, 1274, 591, 378, 236, 177, 160, 135, 122, 116]',\n",
       " '[3104, 3552, 3517, 2318, 2182, 1658, 1519, 1407, 696, 386, 358]',\n",
       " '[3104, 3560, 3562, 2112, 1641, 1329, 1214, 1085, 970, 903, 772]',\n",
       " '[55, 61, 59, 131, 234, 324, 3510, 3521, 3523, 3530, 3483]',\n",
       " '[55, 60, 56, 71, 132, 266, 432, 3707, 2659, 3664, 3634]',\n",
       " '[55, 40, 39, 31, 34, 72, 134, 224, 327, 302, 204]',\n",
       " '[55, 62, 75, 78, 88, 351, 1446, 1388, 1388, 1402, 1420]',\n",
       " '[55, 73, 48, 74, 111, 216, 678, 660, 1235, 1176, 1169]',\n",
       " '[55, 71, 79, 46, 39, 166, 362, 259, 255, 282, 285]',\n",
       " '[55, 47, 45, 47, 37, 31, 32, 48, 44, 27, 25]',\n",
       " '[55, 34, 29, 26, 22, 19, 18, 18, 14, 12, 5]',\n",
       " '[55, 55, 60, 52, 36, 57, 50, 46, 45, 50, 54]',\n",
       " '[55, 50, 38, 58, 65, 75, 114, 177, 455, 434, 453]',\n",
       " '[562, 362, 350, 263, 232, 291, 297, 271, 266, 827, 842]',\n",
       " '[562, 547, 519, 486, 646, 720, 838, 1702, 1904, 2096, 2314]',\n",
       " '[562, 577, 731, 748, 828, 900, 2974, 2953, 2175, 4278, 4274]',\n",
       " '[562, 584, 583, 662, 647, 588, 1548, 4334, 3919, 2907, 2919]',\n",
       " '[562, 568, 592, 557, 672, 802, 2986, 2962, 2911, 2903, 2897]',\n",
       " '[562, 600, 601, 486, 820, 1610, 1752, 3037, 2991, 2932, 2960]',\n",
       " '[243, 204, 96, 62, 52, 55, 39, 67, 44, 40, 32]',\n",
       " '[243, 193, 175, 105, 90, 68, 84, 53, 52, 36, 34]',\n",
       " '[243, 242, 183, 167, 206, 2753, 2787, 2791, 2763, 3723, 2714]',\n",
       " '[243, 236, 155, 132, 221, 261, 125, 267, 298, 621, 1743]',\n",
       " '[243, 236, 213, 186, 156, 199, 296, 507, 785, 747, 750]',\n",
       " '[243, 267, 366, 378, 1207, 3282, 3004, 2968, 2973, 2915, 2860]',\n",
       " '[243, 253, 356, 310, 193, 395, 629, 536, 505, 476, 456]',\n",
       " '[243, 251, 208, 260, 442, 379, 611, 917, 816, 810, 739]',\n",
       " '[243, 183, 104, 91, 66, 50, 56, 279, 786, 370, 343]',\n",
       " '[1888, 2057, 2038, 2126, 2128, 2203, 2284, 3343, 3314, 3299, 3292]',\n",
       " '[1888, 1941, 1977, 1858, 1928, 1932, 2008, 3162, 3619, 3186, 3388]',\n",
       " '[1888, 2107, 2148, 1689, 564, 421, 207, 222, 206, 201, 204]',\n",
       " '[35, 46, 33, 41, 24, 24, 18, 13, 15, 18, 15]',\n",
       " '[35, 51, 66, 76, 86, 79, 90, 84, 299, 513, 511]',\n",
       " '[35, 41, 50, 50, 30, 22, 30, 28, 36, 35, 36]',\n",
       " '[35, 49, 67, 111, 364, 916, 1356, 1314, 1313, 1289, 1346]',\n",
       " '[35, 35, 39, 41, 41, 77, 193, 503, 602, 836, 834]',\n",
       " '[35, 48, 36, 37, 70, 92, 90, 100, 94, 109, 110]',\n",
       " '[35, 38, 31, 22, 12, 11, 6, 3, 4, 2, 3]',\n",
       " '[35, 34, 36, 26, 33, 39, 39, 54, 49, 42, 47]',\n",
       " '[35, 30, 35, 50, 50, 104, 222, 521, 872, 915, 995]',\n",
       " '[35, 30, 28, 24, 26, 25, 32, 25, 28, 73, 67]',\n",
       " '[337, 338, 197, 161, 103, 79, 54, 40, 36, 34, 27]',\n",
       " '[337, 275, 262, 218, 296, 378, 908, 1007, 1730, 1766, 1820]',\n",
       " '[337, 454, 473, 475, 399, 213, 314, 293, 420, 405, 406]',\n",
       " '[337, 427, 402, 516, 549, 2283, 4028, 4029, 3503, 3060, 3057]',\n",
       " '[337, 438, 410, 421, 498, 682, 1004, 4019, 3595, 4009, 3056]',\n",
       " '[337, 346, 349, 415, 295, 232, 183, 261, 361, 214, 218]',\n",
       " '[337, 428, 400, 468, 659, 3935, 3962, 3941, 3926, 3342, 3897]',\n",
       " '[337, 339, 331, 476, 356, 281, 459, 769, 786, 745, 734]',\n",
       " '[337, 362, 398, 483, 656, 1045, 1157, 2376, 2751, 3626, 3625]',\n",
       " '[337, 276, 164, 145, 123, 166, 185, 201, 199, 184, 190]',\n",
       " '[20, 19, 18, 17, 18, 28, 139, 377, 588, 979, 1697]',\n",
       " '[20, 22, 19, 22, 20, 35, 27, 56, 172, 272, 328]',\n",
       " '[20, 18, 17, 18, 10, 9, 10, 9, 41, 38, 108]',\n",
       " '[20, 17, 17, 21, 18, 17, 29, 66, 154, 278, 349]',\n",
       " '[20, 20, 22, 17, 16, 49, 180, 486, 691, 665, 665]',\n",
       " '[20, 19, 19, 15, 13, 42, 81, 130, 119, 120, 123]',\n",
       " '[20, 20, 17, 19, 20, 24, 29, 38, 72, 128, 119]',\n",
       " '[20, 20, 24, 38, 50, 45, 145, 171, 131, 161, 170]',\n",
       " '[20, 21, 33, 44, 95, 306, 701, 1677, 3102, 4088, 4089]',\n",
       " '[20, 20, 18, 28, 61, 48, 349, 699, 1747, 3690, 3784]',\n",
       " '[219, 172, 131, 62, 35, 28, 23, 24, 29, 28, 21]',\n",
       " '[219, 169, 153, 51, 14, 13, 6, 6, 6, 6, 6]',\n",
       " '[219, 202, 167, 122, 61, 151, 179, 373, 655, 656, 682]',\n",
       " '[219, 227, 357, 352, 320, 724, 1603, 1682, 1761, 1811, 4232]',\n",
       " '[219, 205, 122, 57, 13, 8, 6, 6, 8, 7, 7]',\n",
       " '[219, 234, 249, 229, 366, 967, 393, 351, 311, 302, 266]',\n",
       " '[219, 218, 274, 216, 193, 236, 131, 266, 180, 171, 173]',\n",
       " '[219, 223, 219, 220, 251, 239, 176, 225, 132, 119, 143]',\n",
       " '[219, 227, 278, 289, 161, 68, 45, 49, 45, 45, 70]',\n",
       " '[4297, 4063, 2371, 1242, 1101, 277, 237, 147, 123, 209, 191]',\n",
       " '[4297, 4084, 4084, 4334, 2511, 784, 656, 486, 468, 526, 537]',\n",
       " '[4297, 4478, 4039, 4048, 4004, 3668, 1648, 1509, 1509, 1455, 1387]',\n",
       " '[4297, 4047, 4234, 3963, 4096, 3985, 808, 642, 606, 532, 268]',\n",
       " '[4297, 4132, 4240, 4102, 4088, 2545, 1962, 1956, 1872, 1801, 1793]',\n",
       " '[4297, 4071, 4299, 1339, 1271, 1686, 291, 424, 591, 1170, 1186]',\n",
       " '[4297, 4069, 4068, 4067, 4415, 2627, 2174, 2159, 2094, 1983, 1984]',\n",
       " '[4297, 4453, 4299, 1650, 1579, 469, 331, 529, 508, 480, 477]',\n",
       " '[4297, 4075, 4075, 1847, 1520, 593, 399, 387, 393, 402, 403]',\n",
       " '[4297, 4084, 4037, 4041, 751, 319, 146, 121, 83, 65, 58]',\n",
       " '[255, 268, 362, 678, 840, 1296, 1014, 861, 793, 743, 697]',\n",
       " '[255, 335, 238, 301, 259, 270, 333, 503, 812, 819, 752]',\n",
       " '[255, 275, 371, 606, 558, 2042, 2078, 2046, 2007, 3881, 3657]',\n",
       " '[255, 208, 76, 56, 42, 29, 27, 31, 27, 15, 10]',\n",
       " '[255, 158, 138, 98, 79, 91, 136, 60, 55, 38, 35]',\n",
       " '[255, 194, 130, 118, 88, 73, 73, 1623, 3683, 1292, 1248]',\n",
       " '[255, 251, 188, 162, 239, 500, 436, 408, 389, 358, 348]',\n",
       " '[311, 312, 300, 393, 1279, 2876, 3072, 3305, 2827, 2730, 3242]',\n",
       " '[311, 224, 126, 106, 84, 62, 59, 89, 216, 308, 301]',\n",
       " '[311, 315, 344, 351, 184, 328, 155, 145, 145, 136, 124]',\n",
       " '[311, 307, 508, 335, 735, 525, 498, 486, 503, 507, 540]',\n",
       " '[311, 322, 329, 349, 708, 1501, 1805, 3313, 2637, 2941, 3205]',\n",
       " '[311, 289, 279, 239, 226, 241, 322, 564, 499, 474, 440]',\n",
       " '[311, 296, 212, 269, 304, 296, 536, 1435, 1463, 1511, 1544]',\n",
       " '[311, 305, 314, 273, 271, 1232, 3536, 3552, 1976, 3287, 1783]',\n",
       " '[311, 229, 231, 349, 288, 377, 1065, 1025, 969, 975, 991]',\n",
       " '[311, 463, 334, 382, 394, 442, 950, 904, 788, 840, 1726]',\n",
       " '[26, 24, 15, 7, 5, 2, 3, 3, 5, 5, 6]',\n",
       " '[26, 27, 32, 36, 24, 43, 111, 101, 133, 186, 196]',\n",
       " '[26, 26, 18, 16, 15, 27, 61, 175, 364, 362, 357]',\n",
       " '[26, 38, 69, 259, 1385, 1425, 2813, 2745, 2699, 4139, 2712]',\n",
       " '[26, 25, 30, 28, 27, 77, 106, 93, 352, 338, 183]',\n",
       " '[26, 29, 31, 23, 29, 25, 39, 50, 40, 44, 44]',\n",
       " '[26, 23, 23, 44, 62, 101, 312, 244, 163, 238, 198]',\n",
       " '[26, 15, 12, 11, 4, 3, 5, 3, 5, 6, 6]',\n",
       " '[26, 20, 17, 16, 17, 44, 141, 194, 2844, 2860, 3521]',\n",
       " '[26, 31, 24, 38, 53, 75, 160, 181, 298, 317, 331]',\n",
       " '[139, 134, 212, 262, 400, 1819, 3757, 3723, 4230, 3762, 3875]',\n",
       " '[139, 166, 162, 395, 655, 1646, 3730, 3715, 3723, 3729, 3732]',\n",
       " '[139, 104, 84, 75, 46, 79, 108, 78, 69, 55, 58]',\n",
       " '[139, 180, 273, 178, 243, 431, 508, 261, 247, 227, 219]',\n",
       " '[139, 130, 144, 100, 55, 228, 236, 428, 400, 261, 251]',\n",
       " '[139, 200, 162, 263, 294, 257, 341, 345, 389, 956, 996]',\n",
       " '[139, 90, 82, 97, 67, 96, 292, 460, 751, 791, 816]',\n",
       " '[139, 134, 88, 57, 36, 26, 20, 17, 20, 18, 19]',\n",
       " '[139, 139, 136, 273, 487, 418, 440, 758, 1610, 3730, 1523]',\n",
       " '[139, 149, 140, 291, 269, 357, 1573, 1621, 930, 1673, 1635]',\n",
       " '[1285, 1279, 1284, 2276, 2218, 2234, 2017, 2632, 2299, 2072, 2111]',\n",
       " '[1285, 1317, 1350, 1437, 1534, 1119, 954, 888, 1509, 1420, 1344]',\n",
       " '[1285, 1286, 1324, 1358, 2202, 2000, 2599, 2087, 2159, 2287, 2260]',\n",
       " '[1285, 1282, 803, 807, 888, 1366, 1334, 1433, 1488, 2077, 2719]',\n",
       " '[1285, 1225, 1228, 1813, 1831, 2375, 2143, 2159, 2038, 2232, 1872]',\n",
       " '[1285, 1275, 1290, 1712, 2057, 2750, 1293, 1153, 1106, 1045, 973]',\n",
       " '[1285, 1325, 791, 714, 1336, 1246, 1248, 1429, 1488, 1590, 1696]',\n",
       " '[1285, 1259, 1275, 1292, 1382, 2358, 2103, 2471, 2326, 2373, 2000]',\n",
       " '[1285, 1246, 1272, 1248, 1775, 2084, 1732, 1607, 690, 643, 625]',\n",
       " '[1285, 1277, 745, 975, 573, 499, 448, 462, 438, 444, 479]',\n",
       " '[354, 340, 312, 258, 189, 113, 243, 282, 304, 420, 423]',\n",
       " '[354, 223, 213, 173, 156, 112, 132, 170, 165, 165, 167]',\n",
       " '[354, 351, 359, 688, 778, 682, 417, 354, 323, 260, 240]',\n",
       " '[354, 424, 425, 547, 609, 623, 590, 1037, 1922, 1935, 1863]',\n",
       " '[354, 216, 208, 192, 163, 283, 221, 152, 150, 224, 163]',\n",
       " '[354, 392, 367, 250, 191, 239, 252, 381, 385, 402, 429]',\n",
       " '[354, 435, 470, 704, 813, 801, 1713, 1714, 1703, 1723, 2097]',\n",
       " '[354, 439, 441, 669, 723, 434, 613, 548, 537, 550, 565]',\n",
       " '[354, 265, 260, 319, 380, 340, 1860, 4313, 6418, 6431, 6431]',\n",
       " '[354, 229, 283, 249, 313, 931, 1003, 1759, 2136, 2258, 2190]',\n",
       " '[1255, 1255, 709, 648, 571, 585, 557, 892, 934, 964, 1100]',\n",
       " '[1255, 1270, 1318, 2019, 1166, 1032, 926, 1116, 776, 742, 721]',\n",
       " '[1255, 1245, 1310, 2107, 6430, 6444, 6357, 6390, 6376, 6359, 6362]',\n",
       " '[1255, 1229, 846, 792, 732, 1110, 1134, 1506, 1573, 1618, 1655]',\n",
       " '[1255, 1260, 786, 503, 547, 456, 457, 457, 432, 410, 418]',\n",
       " '[1255, 1263, 1286, 1332, 2185, 2160, 6297, 6455, 6452, 6468, 6461]',\n",
       " '[1255, 1231, 1218, 1209, 1233, 1277, 873, 1901, 1818, 1794, 1775]',\n",
       " '[1255, 1264, 1267, 2072, 2059, 2148, 6330, 6457, 6451, 6294, 6453]',\n",
       " '[1255, 1192, 1166, 1131, 1307, 1248, 1305, 1298, 1361, 1444, 1550]',\n",
       " '[1255, 689, 368, 387, 312, 239, 192, 160, 126, 412, 407]',\n",
       " '[71, 56, 45, 23, 29, 26, 24, 25, 23, 26, 24]',\n",
       " '[71, 53, 50, 39, 41, 35, 110, 109, 125, 153, 160]',\n",
       " '[71, 55, 57, 52, 33, 37, 22, 20, 26, 22, 23]',\n",
       " '[71, 46, 48, 56, 81, 177, 252, 259, 179, 332, 749]',\n",
       " '[71, 47, 45, 40, 42, 28, 65, 81, 171, 340, 386]',\n",
       " '[71, 75, 92, 105, 188, 538, 809, 713, 685, 652, 619]',\n",
       " '[71, 65, 68, 79, 69, 136, 705, 673, 1496, 6259, 6247]',\n",
       " '[71, 74, 60, 78, 135, 108, 146, 273, 546, 1348, 6255]',\n",
       " '[71, 68, 66, 134, 598, 1546, 6153, 6154, 6143, 6144, 6259]',\n",
       " '[71, 63, 50, 143, 148, 155, 233, 290, 513, 548, 588]',\n",
       " '[549, 553, 324, 313, 423, 822, 1749, 1782, 1947, 2001, 7358]',\n",
       " '[549, 542, 542, 555, 490, 537, 1387, 1211, 1116, 1027, 617]',\n",
       " '[549, 553, 600, 631, 507, 688, 991, 1477, 1422, 1361, 1316]',\n",
       " '[549, 415, 409, 390, 352, 275, 320, 468, 214, 181, 178]',\n",
       " '[549, 566, 605, 639, 511, 326, 377, 432, 418, 382, 347]',\n",
       " '[549, 564, 594, 522, 604, 1655, 7283, 7266, 7279, 7351, 7355]',\n",
       " '[549, 535, 515, 322, 173, 86, 124, 135, 200, 307, 301]',\n",
       " '[549, 534, 532, 559, 546, 854, 719, 688, 679, 657, 655]',\n",
       " '[549, 558, 441, 420, 306, 364, 485, 668, 969, 944, 439]',\n",
       " '[549, 530, 575, 590, 673, 1094, 1907, 1039, 940, 894, 835]',\n",
       " '[369, 363, 370, 462, 451, 446, 1301, 6765, 6781, 6856, 6864]',\n",
       " '[369, 346, 326, 241, 220, 136, 134, 113, 175, 178, 180]',\n",
       " '[369, 323, 287, 136, 93, 87, 73, 62, 62, 66, 57]',\n",
       " '[369, 375, 490, 485, 869, 1640, 1810, 709, 1130, 1038, 1006]',\n",
       " '[369, 359, 470, 564, 775, 862, 947, 1128, 990, 943, 925]',\n",
       " '[369, 349, 436, 405, 863, 895, 949, 970, 7326, 7321, 7327]',\n",
       " '[369, 568, 581, 559, 838, 1870, 6777, 6776, 6773, 6842, 6835]',\n",
       " '[369, 575, 562, 558, 371, 428, 188, 130, 122, 105, 101]',\n",
       " '[369, 582, 595, 784, 799, 6808, 6809, 6779, 6822, 6848, 6821]',\n",
       " '[369, 562, 435, 415, 398, 417, 875, 931, 984, 1893, 6773]',\n",
       " '[119, 117, 85, 67, 43, 45, 101, 204, 219, 324, 348]',\n",
       " '[119, 119, 132, 102, 118, 82, 49, 86, 83, 120, 168]',\n",
       " '[119, 124, 159, 213, 318, 571, 1284, 1524, 4968, 4960, 4963]',\n",
       " '[119, 119, 124, 181, 189, 679, 1425, 1533, 1421, 1322, 1322]',\n",
       " '[119, 139, 118, 121, 150, 397, 743, 1575, 1619, 1639, 1678]',\n",
       " '[119, 116, 124, 74, 69, 54, 68, 84, 84, 68, 82]',\n",
       " '[119, 119, 91, 43, 30, 13, 13, 17, 16, 17, 21]',\n",
       " '[119, 111, 94, 101, 176, 160, 278, 301, 415, 393, 599]',\n",
       " '[119, 111, 83, 138, 154, 155, 340, 553, 510, 746, 765]',\n",
       " '[119, 142, 98, 98, 147, 119, 123, 130, 142, 137, 152]',\n",
       " '[4836, 2000, 874, 366, 96, 42, 25, 24, 27, 23, 22]',\n",
       " '[4836, 4859, 4863, 4969, 4780, 777, 668, 627, 603, 558, 833]',\n",
       " '[4836, 4839, 1394, 1335, 1338, 1409, 4940, 4755, 4724, 4723, 4706]',\n",
       " '[4836, 4831, 1387, 1053, 804, 543, 417, 353, 132, 96, 72]',\n",
       " '[4836, 1942, 1851, 2078, 4815, 4851, 4848, 1767, 1480, 515, 459]',\n",
       " '[4836, 4849, 4865, 4864, 1712, 1199, 1010, 903, 539, 320, 307]',\n",
       " '[4836, 1483, 383, 172, 51, 19, 11, 10, 8, 6, 5]',\n",
       " '[4836, 4843, 4843, 4827, 4839, 4843, 4839, 4867, 4749, 4753, 4748]',\n",
       " '[4836, 832, 704, 592, 120, 33, 40, 36, 34, 39, 32]',\n",
       " '[4836, 1962, 1865, 1058, 959, 1507, 1322, 1242, 1062, 1034, 1008]',\n",
       " '[31, 30, 24, 11, 8, 9, 20, 26, 31, 41, 48]',\n",
       " '[31, 20, 21, 18, 25, 46, 86, 238, 226, 236, 231]',\n",
       " '[31, 26, 32, 27, 45, 222, 559, 582, 622, 1493, 1533]',\n",
       " '[31, 22, 19, 16, 10, 16, 54, 99, 352, 345, 357]',\n",
       " '[31, 29, 24, 23, 33, 79, 276, 486, 727, 723, 732]',\n",
       " '[31, 24, 21, 26, 26, 27, 28, 39, 46, 55, 56]',\n",
       " '[31, 30, 19, 14, 20, 11, 19, 22, 30, 40, 42]',\n",
       " '[31, 27, 26, 25, 35, 59, 150, 338, 325, 283, 275]',\n",
       " '[31, 26, 20, 10, 5, 3, 3, 4, 4, 6, 6]',\n",
       " '[31, 21, 20, 17, 13, 11, 12, 29, 35, 77, 78]',\n",
       " '[1650, 1608, 1515, 1562, 772, 731, 1650, 1610, 1625, 1618, 1668]',\n",
       " '[1650, 1629, 1648, 1615, 1590, 1761, 2633, 2131, 2133, 2360, 2335]',\n",
       " '[1650, 1615, 1647, 580, 449, 211, 193, 222, 1401, 1309, 1295]',\n",
       " '[1650, 1632, 1866, 1802, 2128, 6432, 6429, 6448, 6449, 6449, 6414]',\n",
       " '[1650, 1579, 1510, 1469, 1457, 1624, 1704, 1701, 2290, 2080, 2167]',\n",
       " '[1650, 1716, 1706, 1726, 1861, 1914, 1879, 1841, 2412, 2673, 2482]',\n",
       " '[1650, 1582, 1467, 1393, 1545, 1667, 1746, 1970, 2333, 2195, 2134]',\n",
       " '[5933, 5949, 5931, 6235, 6238, 6229, 5953, 726, 668, 633, 600]',\n",
       " '[5933, 5944, 5924, 6230, 6232, 2141, 1848, 1769, 1545, 1462, 1304]',\n",
       " '[5933, 5950, 5926, 5937, 5937, 1240, 984, 897, 840, 799, 772]',\n",
       " '[5933, 5941, 6170, 1254, 996, 858, 1518, 1366, 1229, 6206, 6103]',\n",
       " '[5933, 5911, 5932, 5922, 5927, 5914, 6222, 6234, 1966, 1946, 1891]',\n",
       " '[5933, 5947, 5926, 5923, 5901, 5940, 5929, 5940, 5946, 5932, 5930]',\n",
       " '[5933, 5923, 5931, 5935, 6206, 946, 788, 451, 428, 420, 389]',\n",
       " '[5933, 5915, 1862, 907, 852, 756, 651, 381, 355, 471, 455]',\n",
       " '[5933, 6073, 6206, 5923, 6213, 2220, 1468, 1311, 1262, 1198, 1196]',\n",
       " '[42, 42, 35, 40, 41, 190, 258, 1203, 1350, 7417, 7410]',\n",
       " '[42, 46, 53, 63, 81, 101, 122, 119, 225, 319, 418]',\n",
       " '[42, 40, 59, 106, 200, 173, 431, 427, 228, 217, 210]',\n",
       " '[42, 39, 43, 60, 123, 191, 262, 438, 614, 599, 607]',\n",
       " '[42, 41, 42, 77, 184, 219, 470, 688, 662, 682, 697]',\n",
       " '[42, 34, 24, 29, 43, 37, 116, 105, 168, 169, 109]',\n",
       " '[42, 20, 12, 7, 4, 4, 3, 5, 4, 4, 3]',\n",
       " '[42, 25, 29, 27, 37, 39, 74, 103, 172, 237, 240]',\n",
       " '[42, 39, 47, 48, 51, 59, 53, 55, 56, 44, 42]',\n",
       " '[42, 41, 52, 104, 203, 762, 627, 2247, 2325, 1157, 2220]',\n",
       " '[131, 122, 94, 113, 446, 912, 958, 839, 443, 424, 404]',\n",
       " '[131, 93, 46, 59, 51, 36, 48, 43, 44, 85, 119]',\n",
       " '[131, 130, 163, 311, 632, 738, 759, 705, 1358, 1270, 1092]',\n",
       " '[131, 69, 55, 37, 19, 41, 57, 54, 57, 61, 73]',\n",
       " '[131, 128, 101, 174, 264, 500, 1198, 1857, 1686, 1673, 1639]',\n",
       " '[131, 133, 91, 175, 381, 903, 584, 890, 912, 907, 622]',\n",
       " '[131, 134, 173, 295, 953, 6810, 6799, 6783, 4693, 4802, 5879]',\n",
       " '[131, 125, 86, 73, 96, 69, 35, 58, 37, 28, 28]',\n",
       " '[131, 101, 117, 97, 129, 204, 169, 179, 183, 180, 103]',\n",
       " '[131, 121, 118, 141, 187, 193, 328, 689, 1040, 723, 1842]',\n",
       " '[79, 77, 60, 89, 142, 406, 243, 448, 470, 342, 351]',\n",
       " '[79, 73, 53, 46, 60, 73, 97, 96, 115, 151, 155]',\n",
       " '[79, 77, 63, 77, 58, 65, 99, 107, 118, 161, 310]',\n",
       " '[79, 73, 100, 108, 402, 534, 1264, 581, 1700, 1687, 5093]',\n",
       " '[79, 55, 50, 62, 87, 111, 147, 96, 93, 127, 132]',\n",
       " '[79, 78, 125, 137, 175, 557, 1143, 1099, 1073, 1091, 1074]',\n",
       " '[79, 85, 99, 79, 90, 125, 198, 332, 332, 343, 350]',\n",
       " '[79, 85, 80, 91, 79, 54, 30, 28, 47, 42, 48]',\n",
       " '[79, 41, 30, 19, 12, 7, 1, 0, 5, 5, 4]',\n",
       " '[79, 71, 66, 87, 50, 62, 210, 226, 202, 193, 182]',\n",
       " '[5337, 5344, 5324, 5339, 5367, 1512, 1123, 1073, 1073, 1066, 1084]',\n",
       " '[5337, 5314, 5303, 5412, 5395, 5402, 5321, 5313, 5324, 5335, 5321]',\n",
       " '[5337, 5306, 5301, 5406, 5417, 5412, 5422, 5427, 5441, 5349, 1513]',\n",
       " '[5337, 5318, 5301, 5289, 5284, 5300, 5317, 1599, 1578, 1551, 1539]',\n",
       " '[5337, 5327, 5309, 5312, 5306, 5311, 5299, 5300, 5299, 1411, 1364]',\n",
       " '[5337, 5320, 5308, 5304, 5308, 1278, 1003, 875, 819, 829, 800]',\n",
       " '[980, 1015, 1072, 1203, 3361, 3407, 3415, 1499, 1398, 1346, 1284]',\n",
       " '[980, 884, 813, 782, 737, 725, 835, 948, 1292, 1817, 1795]',\n",
       " '[980, 946, 905, 879, 890, 874, 967, 1241, 3457, 3457, 4220]',\n",
       " '[980, 966, 1012, 1068, 1194, 1231, 986, 407, 357, 303, 263]',\n",
       " '[980, 885, 814, 745, 682, 358, 569, 588, 619, 324, 318]',\n",
       " '[980, 925, 903, 897, 1077, 3191, 2994, 2649, 3934, 3926, 3921]',\n",
       " '[980, 900, 862, 787, 723, 363, 294, 311, 311, 681, 646]',\n",
       " '[980, 980, 1034, 1188, 3365, 1451, 1212, 1147, 1073, 1051, 982]',\n",
       " '[980, 1006, 1009, 1012, 1112, 1145, 1567, 1453, 1445, 1413, 1421]',\n",
       " '[980, 922, 900, 834, 483, 471, 482, 478, 1130, 493, 470]',\n",
       " '[24, 29, 56, 93, 132, 284, 929, 3717, 2096, 3891, 3657]',\n",
       " '[24, 24, 18, 30, 20, 18, 84, 313, 722, 792, 1293]',\n",
       " '[24, 17, 20, 17, 22, 37, 83, 100, 1665, 1751, 3809]',\n",
       " '[24, 13, 9, 4, 4, 3, 1, 1, 1, 1, 0]',\n",
       " '[24, 22, 15, 16, 17, 15, 17, 18, 15, 8, 10]',\n",
       " '[24, 30, 30, 56, 63, 307, 898, 1211, 2260, 2042, 2098]',\n",
       " '[24, 11, 8, 6, 2, 1, 0, 0, 0, 0, 0]',\n",
       " '[24, 15, 10, 8, 9, 7, 5, 6, 6, 6, 5]',\n",
       " '[24, 24, 21, 23, 34, 28, 99, 172, 532, 557, 571]',\n",
       " '[24, 24, 32, 35, 49, 136, 388, 1257, 1952, 1954, 2022]',\n",
       " '[6388, 6386, 6379, 6379, 1898, 2140, 6389, 6384, 6419, 6419, 6392]',\n",
       " '[6388, 6391, 6383, 6384, 1766, 2071, 1860, 2225, 6394, 6396, 6382]',\n",
       " '[6388, 6399, 1878, 961, 877, 753, 715, 698, 715, 761, 796]',\n",
       " '[6388, 6395, 6400, 6385, 6391, 6366, 1338, 1232, 1067, 1037, 996]',\n",
       " '[6388, 6393, 6401, 1768, 1720, 1845, 1785, 1772, 776, 790, 771]',\n",
       " '[6388, 6387, 2014, 2015, 1400, 1293, 528, 485, 457, 438, 424]',\n",
       " '[6388, 6378, 6380, 6379, 6388, 1855, 1757, 1705, 653, 659, 646]',\n",
       " '[6388, 6395, 6394, 4227, 6373, 6465, 2117, 2128, 2104, 2240, 2264]',\n",
       " '[6388, 6399, 6404, 6388, 6441, 6329, 6340, 6404, 6814, 7255, 7245]',\n",
       " '[6388, 6401, 6399, 1922, 2216, 2569, 1883, 6380, 4228, 6385, 6394]',\n",
       " '[117, 116, 92, 76, 91, 83, 51, 48, 68, 102, 158]',\n",
       " '[117, 207, 203, 296, 405, 515, 5582, 5583, 5562, 1901, 1912]',\n",
       " '[117, 220, 332, 303, 492, 1099, 1465, 1355, 1356, 1367, 1406]',\n",
       " '[117, 149, 185, 171, 269, 972, 425, 353, 331, 296, 291]',\n",
       " '[117, 145, 138, 175, 117, 129, 98, 94, 93, 70, 102]',\n",
       " '[117, 145, 133, 122, 103, 119, 94, 314, 323, 335, 338]',\n",
       " '[117, 213, 214, 222, 535, 1246, 1438, 1615, 5602, 5603, 5596]',\n",
       " '[5798, 5762, 1442, 1268, 1146, 1090, 1058, 1177, 1359, 1466, 1617]',\n",
       " '[5798, 5751, 5748, 5764, 5778, 351, 311, 297, 273, 180, 177]',\n",
       " '[5798, 5828, 5778, 5804, 5846, 5852, 1549, 1363, 513, 465, 442]',\n",
       " '[5798, 1459, 1245, 1062, 995, 907, 866, 858, 865, 523, 525]',\n",
       " '[5798, 5835, 5830, 5758, 5754, 5761, 5759, 5764, 5757, 5762, 5768]',\n",
       " '[5798, 5819, 1417, 1357, 1313, 1351, 1423, 1446, 1287, 1247, 1220]',\n",
       " '[5798, 1545, 1461, 1446, 797, 858, 2047, 2096, 5897, 5899, 5875]',\n",
       " '[5798, 5802, 5778, 5847, 1801, 1586, 1296, 1204, 1101, 1028, 1007]',\n",
       " '[5798, 5792, 5798, 5807, 5814, 1327, 1041, 864, 331, 287, 285]',\n",
       " '[5798, 5801, 1466, 741, 198, 110, 70, 36, 24, 111, 108]',\n",
       " '[148, 210, 206, 225, 558, 463, 813, 854, 951, 1006, 500]',\n",
       " '[148, 164, 209, 210, 325, 873, 5327, 571, 482, 215, 198]',\n",
       " '[148, 169, 171, 132, 76, 57, 54, 42, 58, 131, 151]',\n",
       " '[148, 156, 206, 202, 320, 1385, 272, 178, 122, 91, 82]',\n",
       " '[148, 161, 129, 98, 66, 56, 40, 41, 61, 83, 111]',\n",
       " '[148, 272, 230, 166, 217, 316, 344, 259, 268, 216, 217]',\n",
       " '[148, 145, 139, 149, 108, 142, 331, 319, 243, 338, 343]',\n",
       " '[148, 199, 157, 146, 86, 83, 77, 59, 60, 60, 83]',\n",
       " '[148, 174, 249, 321, 480, 584, 693, 670, 400, 628, 624]',\n",
       " '[148, 143, 118, 127, 108, 101, 115, 130, 187, 193, 287]',\n",
       " '[10, 8, 9, 9, 14, 13, 11, 21, 16, 20, 18]',\n",
       " '[10, 7, 12, 14, 27, 68, 113, 191, 191, 116, 168]',\n",
       " '[10, 10, 10, 8, 8, 11, 30, 22, 23, 18, 20]',\n",
       " '[10, 13, 10, 20, 11, 11, 11, 14, 13, 11, 8]',\n",
       " '[10, 7, 10, 10, 22, 33, 67, 44, 34, 40, 116]',\n",
       " '[10, 10, 13, 14, 24, 79, 76, 160, 249, 342, 352]',\n",
       " '[10, 8, 7, 9, 6, 5, 4, 1, 0, 0, 0]',\n",
       " '[10, 10, 12, 15, 16, 27, 29, 59, 214, 213, 218]',\n",
       " '[10, 10, 8, 16, 27, 30, 34, 69, 98, 80, 81]',\n",
       " '[10, 14, 12, 10, 10, 18, 17, 19, 17, 16, 19]',\n",
       " '[711, 759, 806, 1354, 593, 569, 566, 564, 615, 647, 1555]',\n",
       " '[711, 774, 414, 1052, 1043, 1137, 1239, 3438, 2020, 3432, 3460]',\n",
       " '[711, 761, 706, 709, 785, 785, 465, 971, 1456, 614, 574]',\n",
       " '[711, 730, 747, 764, 775, 1792, 1839, 1901, 2846, 3467, 3987]',\n",
       " '[711, 374, 375, 371, 777, 351, 329, 364, 364, 405, 436]',\n",
       " '[711, 701, 327, 253, 80, 73, 71, 72, 41, 34, 26]',\n",
       " '[711, 650, 602, 590, 365, 244, 348, 245, 155, 167, 231]',\n",
       " '[711, 683, 645, 636, 592, 477, 444, 413, 406, 389, 402]',\n",
       " '[711, 391, 667, 955, 875, 849, 905, 957, 519, 1269, 1237]',\n",
       " '[711, 758, 946, 837, 355, 397, 320, 299, 293, 288, 272]',\n",
       " '[2543, 2260, 2223, 2708, 2433, 2678, 2579, 2631, 2515, 2552, 2014]',\n",
       " '[2543, 2281, 1737, 1651, 1638, 1721, 1932, 808, 786, 766, 746]',\n",
       " '[2543, 2636, 2494, 2657, 1864, 1709, 1536, 1480, 662, 400, 375]',\n",
       " '[2543, 2593, 2714, 2059, 1990, 2016, 2045, 2105, 2125, 2098, 2099]',\n",
       " '[2543, 2327, 1654, 1442, 1358, 1281, 1366, 1481, 1575, 2756, 1527]',\n",
       " '[2543, 1542, 1257, 1011, 200, 111, 91, 71, 79, 74, 68]',\n",
       " '[2543, 2783, 2429, 2676, 2594, 2354, 3181, 3155, 1539, 1481, 1409]',\n",
       " '[2543, 2081, 2435, 2025, 2591, 2362, 2366, 1720, 1703, 1637, 741]',\n",
       " '[2543, 2589, 2246, 2697, 2550, 2313, 2656, 2724, 2527, 2309, 2104]',\n",
       " '[2543, 2458, 2112, 1604, 627, 558, 527, 527, 912, 857, 870]',\n",
       " '[1373, 1364, 1294, 1281, 662, 632, 1577, 1451, 1458, 1501, 1549]',\n",
       " '[1373, 1497, 6008, 5997, 5978, 6015, 6461, 6519, 6519, 6528, 6514]',\n",
       " '[1373, 1644, 5997, 6011, 6262, 6530, 6525, 6530, 6542, 6538, 6533]',\n",
       " '[1373, 1520, 6030, 6017, 6025, 1529, 1309, 496, 432, 399, 363]',\n",
       " '[1373, 1357, 1308, 1278, 1285, 1424, 6518, 6523, 6510, 6515, 6507]',\n",
       " '[1373, 1535, 5998, 5997, 1138, 1116, 1148, 5996, 6020, 6015, 6002]',\n",
       " '[1373, 1469, 1663, 6020, 6007, 6361, 6530, 6529, 6517, 6513, 6514]',\n",
       " '[1373, 1408, 1563, 1686, 1584, 1424, 1343, 1312, 1280, 1288, 1289]',\n",
       " '[1373, 1346, 1346, 654, 1319, 1419, 1550, 6018, 6266, 6518, 6532]',\n",
       " '[1373, 1575, 6011, 6000, 6034, 6532, 6023, 6025, 6468, 6023, 6029]',\n",
       " '[617, 426, 761, 736, 738, 690, 1262, 1314, 1444, 625, 1570]',\n",
       " '[617, 408, 254, 242, 506, 504, 550, 343, 626, 637, 1308]',\n",
       " '[617, 1258, 1345, 1592, 7056, 7064, 7027, 1435, 1324, 1268, 1244]',\n",
       " '[617, 426, 1741, 1345, 1310, 913, 408, 362, 327, 185, 148]',\n",
       " '[617, 404, 244, 222, 152, 138, 299, 307, 322, 325, 440]',\n",
       " '[617, 330, 330, 229, 178, 124, 135, 132, 97, 93, 96]',\n",
       " '[617, 565, 308, 114, 83, 64, 74, 64, 146, 228, 220]',\n",
       " '[617, 1198, 1216, 1323, 1576, 1934, 1979, 2056, 2085, 2097, 2066]',\n",
       " '[617, 606, 597, 628, 727, 2148, 1975, 1910, 1842, 1833, 1811]',\n",
       " '[1123, 677, 624, 478, 442, 485, 874, 647, 384, 392, 379]',\n",
       " '[1123, 1101, 1067, 1097, 1122, 1256, 1489, 616, 649, 1183, 1246]',\n",
       " '[1123, 1154, 698, 515, 784, 912, 633, 543, 550, 508, 1359]',\n",
       " '[1123, 1189, 633, 647, 1617, 1798, 4919, 4877, 4873, 4874, 4885]',\n",
       " '[1123, 1186, 812, 745, 736, 780, 1740, 1743, 1720, 1760, 804]',\n",
       " '[1123, 721, 754, 732, 648, 1194, 931, 875, 809, 771, 749]',\n",
       " '[1123, 818, 799, 746, 728, 733, 1666, 1618, 1570, 1512, 1517]',\n",
       " '[1123, 1157, 1131, 1196, 666, 207, 116, 135, 124, 121, 110]',\n",
       " '[1123, 1071, 1026, 1056, 560, 555, 591, 648, 411, 439, 703]',\n",
       " '[1123, 1215, 1268, 1374, 1606, 1659, 1418, 1409, 4767, 4722, 4724]',\n",
       " '[419, 404, 226, 194, 126, 210, 353, 429, 404, 298, 268]',\n",
       " '[419, 445, 576, 837, 468, 634, 607, 608, 649, 385, 199]',\n",
       " '[419, 679, 1781, 1851, 494, 595, 574, 527, 503, 502, 479]',\n",
       " '[419, 690, 1792, 1915, 2018, 1394, 905, 645, 614, 340, 315]',\n",
       " '[419, 410, 236, 208, 95, 77, 74, 43, 34, 30, 56]',\n",
       " '[419, 430, 529, 883, 638, 559, 192, 90, 75, 67, 65]',\n",
       " '[419, 610, 255, 208, 100, 94, 118, 123, 141, 133, 177]',\n",
       " '[419, 464, 421, 95, 42, 27, 20, 14, 24, 20, 18]',\n",
       " '[419, 444, 349, 315, 406, 261, 202, 184, 212, 180, 154]',\n",
       " '[419, 373, 365, 465, 479, 797, 1313, 801, 821, 886, 917]',\n",
       " '[96, 59, 33, 22, 13, 9, 6, 4, 9, 9, 6]',\n",
       " '[96, 74, 129, 54, 42, 33, 38, 42, 67, 72, 69]',\n",
       " '[96, 87, 62, 90, 123, 277, 250, 727, 710, 653, 658]',\n",
       " '[96, 114, 86, 80, 52, 121, 231, 202, 187, 185, 190]',\n",
       " '[96, 133, 131, 250, 276, 542, 518, 236, 300, 309, 330]',\n",
       " '[96, 75, 91, 139, 207, 368, 736, 2001, 2047, 2112, 2168]',\n",
       " '[96, 101, 131, 118, 145, 102, 191, 232, 367, 300, 317]',\n",
       " '[96, 100, 111, 169, 545, 597, 857, 979, 1221, 5571, 5574]',\n",
       " '[96, 111, 209, 276, 199, 242, 274, 583, 678, 1414, 1348]',\n",
       " '[96, 103, 117, 131, 210, 394, 652, 662, 689, 523, 507]',\n",
       " '[38, 31, 17, 14, 32, 14, 11, 20, 19, 24, 25]',\n",
       " '[38, 52, 86, 86, 59, 51, 52, 75, 71, 74, 77]',\n",
       " '[38, 35, 57, 388, 430, 958, 2075, 2046, 2079, 2110, 2120]',\n",
       " '[38, 40, 60, 55, 67, 35, 93, 62, 91, 89, 78]',\n",
       " '[38, 33, 52, 63, 41, 65, 122, 161, 579, 760, 392]',\n",
       " '[38, 57, 111, 104, 129, 77, 63, 62, 73, 57, 72]',\n",
       " '[38, 27, 28, 57, 32, 56, 95, 40, 37, 28, 39]',\n",
       " '[38, 23, 18, 15, 13, 9, 8, 11, 11, 10, 8]',\n",
       " '[38, 44, 191, 326, 329, 985, 993, 1007, 1059, 1092, 1116]',\n",
       " '[38, 30, 39, 35, 23, 25, 23, 14, 9, 7, 6]',\n",
       " '[4482, 3391, 3669, 3364, 3341, 3109, 3117, 3095, 3112, 3095, 3097]',\n",
       " '[4482, 3366, 3458, 3415, 3297, 3061, 3135, 3088, 3064, 3028, 3006]',\n",
       " '[515, 488, 465, 466, 491, 1703, 1915, 3495, 3548, 3547, 3708]',\n",
       " '[515, 495, 493, 473, 492, 678, 716, 721, 806, 893, 1920]',\n",
       " '[515, 499, 458, 799, 787, 360, 381, 644, 699, 1439, 1397]',\n",
       " '[515, 494, 513, 834, 860, 1607, 4299, 2964, 3152, 3994, 4212]',\n",
       " '[515, 519, 579, 946, 1950, 3621, 4368, 4295, 4265, 4253, 1644]',\n",
       " '[515, 529, 567, 1763, 1880, 1798, 1257, 459, 447, 443, 440]',\n",
       " '[515, 573, 635, 1178, 2257, 2287, 2173, 2089, 1981, 1967, 1936]',\n",
       " '[515, 484, 739, 413, 460, 1035, 969, 590, 595, 604, 617]',\n",
       " '[515, 504, 723, 690, 365, 588, 569, 349, 379, 398, 627]',\n",
       " '[515, 466, 474, 296, 284, 197, 160, 135, 180, 79, 56]',\n",
       " '[49, 95, 100, 115, 150, 312, 484, 1007, 318, 297, 296]',\n",
       " '[49, 79, 105, 59, 65, 79, 94, 385, 425, 369, 352]',\n",
       " '[49, 65, 59, 72, 43, 47, 67, 104, 86, 86, 98]',\n",
       " '[49, 55, 80, 133, 224, 243, 297, 806, 3114, 3957, 3609]',\n",
       " '[49, 51, 69, 42, 40, 36, 90, 68, 55, 63, 99]',\n",
       " '[49, 53, 66, 46, 41, 52, 87, 234, 230, 221, 229]',\n",
       " '[49, 46, 59, 43, 33, 29, 74, 178, 329, 337, 353]',\n",
       " '[49, 56, 72, 84, 133, 83, 410, 393, 767, 744, 722]',\n",
       " '[49, 62, 180, 123, 56, 43, 33, 37, 120, 107, 316]',\n",
       " '[49, 65, 59, 39, 86, 90, 172, 130, 250, 259, 261]',\n",
       " '[38, 40, 50, 67, 117, 653, 1175, 2122, 1675, 761, 716]',\n",
       " '[38, 34, 33, 25, 28, 35, 67, 84, 287, 410, 419]',\n",
       " '[38, 33, 30, 52, 58, 71, 559, 572, 602, 624, 665]',\n",
       " '[38, 41, 56, 56, 90, 142, 284, 945, 538, 979, 978]',\n",
       " '[38, 36, 35, 47, 53, 58, 67, 303, 343, 686, 1670]',\n",
       " '[38, 52, 53, 95, 301, 404, 818, 2283, 6807, 6772, 6776]',\n",
       " '[38, 44, 59, 45, 109, 239, 601, 587, 1070, 1021, 1010]',\n",
       " '[38, 34, 48, 54, 84, 212, 270, 429, 476, 266, 284]',\n",
       " '[38, 51, 131, 147, 164, 263, 331, 261, 263, 150, 147]',\n",
       " '[38, 38, 57, 47, 40, 49, 51, 57, 60, 99, 155]',\n",
       " '[412, 419, 413, 475, 984, 1038, 2253, 4907, 4752, 4744, 4779]',\n",
       " '[412, 246, 219, 149, 102, 46, 40, 34, 26, 26, 16]',\n",
       " '[412, 402, 375, 518, 611, 4755, 4789, 4808, 4841, 4818, 4848]',\n",
       " '[412, 399, 386, 394, 417, 240, 699, 4750, 1337, 1280, 1234]',\n",
       " '[412, 405, 223, 270, 163, 203, 169, 251, 255, 267, 281]',\n",
       " '[412, 401, 407, 262, 231, 227, 202, 115, 215, 251, 241]',\n",
       " '[412, 439, 452, 471, 945, 919, 2292, 2061, 1834, 1852, 1828]',\n",
       " '[412, 357, 196, 71, 37, 11, 11, 13, 16, 11, 15]',\n",
       " '[412, 360, 347, 325, 407, 248, 226, 119, 260, 250, 252]',\n",
       " '[412, 383, 355, 154, 94, 58, 32, 39, 36, 39, 44]',\n",
       " '[266, 185, 175, 216, 215, 301, 332, 624, 1458, 5797, 5795]',\n",
       " '[266, 242, 242, 247, 332, 557, 1320, 5791, 5783, 5764, 5763]',\n",
       " '[266, 336, 162, 189, 150, 138, 133, 147, 212, 487, 513]',\n",
       " '[266, 248, 252, 238, 384, 688, 829, 1163, 1632, 5790, 5779]',\n",
       " '[266, 266, 274, 295, 261, 336, 647, 1908, 1851, 1822, 1844]',\n",
       " '[266, 257, 168, 218, 244, 241, 437, 492, 546, 1097, 1071]',\n",
       " '[266, 266, 305, 274, 347, 415, 471, 887, 1108, 1440, 1744]',\n",
       " '[266, 260, 192, 189, 259, 740, 822, 1114, 1587, 5780, 5784]',\n",
       " '[266, 285, 296, 262, 344, 450, 761, 5957, 5956, 5948, 5949]',\n",
       " '[266, 416, 249, 252, 221, 193, 320, 1245, 1279, 1376, 2067]',\n",
       " '[1151, 1134, 1755, 1677, 1628, 1608, 1754, 2616, 2722, 2567, 2653]',\n",
       " '[1151, 1242, 2409, 3733, 3604, 3539, 3285, 3238, 3225, 3246, 3239]',\n",
       " '[1151, 1056, 1042, 1893, 1876, 3485, 3345, 3353, 3388, 3381, 3344]',\n",
       " '[1151, 1137, 1170, 2334, 3610, 3461, 3374, 3282, 2170, 2619, 3232]',\n",
       " '[1151, 1188, 2112, 2195, 3611, 2741, 2603, 3234, 3226, 2639, 3202]',\n",
       " '[1151, 1128, 2172, 2119, 3384, 3316, 3293, 3240, 2553, 3226, 3224]',\n",
       " '[1151, 1117, 1053, 2078, 2118, 1521, 1377, 1252, 1150, 1160, 1058]',\n",
       " '[1151, 1157, 1095, 1067, 1076, 1188, 1149, 1141, 1049, 1947, 1895]',\n",
       " '[1151, 1096, 966, 2694, 3430, 3341, 2546, 1093, 1003, 924, 848]',\n",
       " '[380, 247, 308, 276, 434, 759, 3284, 3270, 3241, 3239, 3213]',\n",
       " '[380, 922, 936, 1989, 2032, 3725, 3693, 3669, 3090, 2597, 3356]',\n",
       " '[380, 252, 333, 340, 639, 1023, 1715, 1600, 1578, 1467, 1456]',\n",
       " '[380, 386, 553, 523, 800, 1316, 1297, 1288, 1159, 657, 657]',\n",
       " '[804, 747, 645, 626, 656, 2508, 2404, 1793, 1740, 1648, 1613]',\n",
       " '[804, 793, 720, 699, 446, 728, 712, 705, 674, 652, 647]',\n",
       " '[804, 800, 752, 1317, 745, 1595, 1283, 1232, 1251, 1170, 1190]',\n",
       " '[804, 429, 370, 297, 297, 289, 423, 366, 366, 348, 333]',\n",
       " '[804, 784, 421, 364, 538, 802, 822, 751, 842, 843, 921]',\n",
       " '[804, 789, 491, 324, 316, 700, 677, 673, 629, 646, 1209]',\n",
       " '[804, 735, 676, 594, 789, 732, 770, 840, 911, 1046, 1122]',\n",
       " '[804, 745, 760, 802, 1553, 3525, 3391, 3401, 3434, 3410, 3390]',\n",
       " '[804, 730, 590, 600, 618, 1528, 1695, 3302, 3329, 2713, 2632]',\n",
       " '[804, 781, 744, 522, 703, 957, 781, 720, 720, 634, 613]',\n",
       " '[615, 917, 798, 1193, 1076, 782, 1892, 1882, 845, 799, 764]',\n",
       " '[615, 606, 842, 817, 1601, 1531, 1310, 1426, 3017, 2960, 2946]',\n",
       " '[615, 596, 1488, 1445, 1435, 2871, 2981, 4333, 1654, 1548, 1471]',\n",
       " '[615, 623, 231, 258, 326, 240, 327, 271, 519, 484, 480]',\n",
       " '[615, 581, 848, 689, 667, 1253, 1377, 2996, 2881, 2989, 3113]',\n",
       " '[615, 660, 1960, 1810, 1688, 1691, 1748, 3291, 4099, 3828, 2946]',\n",
       " '[615, 922, 903, 1486, 1449, 1416, 1292, 1408, 1476, 1501, 1502]',\n",
       " '[615, 689, 2237, 3341, 3284, 4396, 3173, 2845, 3747, 3962, 4006]',\n",
       " '[615, 587, 574, 1909, 1826, 2998, 1145, 1106, 1167, 1125, 1175]',\n",
       " '[615, 1684, 1752, 3363, 3221, 3087, 4038, 2984, 2960, 2943, 2931]',\n",
       " '[151, 212, 467, 1008, 1136, 1827, 3099, 3114, 3053, 3050, 3024]',\n",
       " '[151, 141, 172, 275, 409, 992, 464, 678, 595, 580, 248]',\n",
       " '[151, 315, 271, 638, 993, 965, 1847, 1773, 1886, 1891, 3099]',\n",
       " '[151, 146, 98, 115, 119, 174, 118, 133, 117, 149, 144]',\n",
       " '[151, 160, 188, 168, 184, 188, 276, 131, 264, 252, 349]',\n",
       " '[151, 154, 234, 250, 204, 471, 418, 384, 319, 307, 282]',\n",
       " '[151, 202, 347, 371, 718, 1058, 749, 646, 413, 373, 357]',\n",
       " '[151, 157, 79, 72, 64, 62, 41, 51, 53, 34, 48]',\n",
       " '[151, 148, 118, 119, 180, 235, 573, 603, 696, 691, 718]',\n",
       " '[151, 129, 86, 43, 34, 39, 48, 82, 95, 98, 171]',\n",
       " '[2473, 1984, 1828, 1407, 1192, 1036, 1076, 1145, 1309, 1368, 1453]',\n",
       " '[2473, 3781, 3687, 3450, 3365, 3263, 3180, 4233, 3150, 3142, 3404]',\n",
       " '[2473, 2465, 3778, 3642, 3582, 3531, 3763, 3208, 1749, 1697, 1662]',\n",
       " '[2473, 2438, 3592, 1900, 1540, 681, 736, 755, 717, 685, 659]',\n",
       " '[2473, 3820, 3469, 2925, 3244, 2369, 3623, 3069, 3392, 3521, 3177]',\n",
       " '[2473, 2332, 2062, 1854, 1818, 1895, 3280, 4022, 3277, 3268, 3067]',\n",
       " '[2473, 2401, 2417, 3783, 4466, 3442, 3412, 3365, 3622, 3372, 3347]',\n",
       " '[2473, 2374, 2284, 2253, 3366, 3268, 3273, 3215, 3182, 3167, 3068]',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['post_rank_random_all'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot HR vs. Performance for 3 keyphrase selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]),)\n",
      "[ 755  830  928 2045 1916 1962 4303 3101 3083 3054 3098]\n"
     ]
    }
   ],
   "source": [
    "print (np.where(get_post_rank(df, rank_type='random_all', line = 3) < [831]*11))\n",
    "print (get_post_rank(df, rank_type='random_all', line = 3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 25\n",
    "lams = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "hr = len(np.where(get_post_rank(df, rank_type='random_all', line = 3) < [k]*len(lams))[0])\n",
    "hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-fa79334efa14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_post_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random_all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "a = get_post_rank(df, rank_type='random_all', line = i)\n",
    "np.concatenate(np.array(np.array([1,2])),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-83d72dd44c43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_post_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random_all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_post_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random_all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "#first concat, then \n",
    "res = np.array([get_post_rank(df, rank_type='random_all', line = 3)])\n",
    "for i in range(len(df)):\n",
    "    np.concatenate(res, get_post_rank(df, rank_type='random_all', line = i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_rank(df,rank_type = 'random_all', line = 0):\n",
    "    \"\"\"\n",
    "    get one line of the post_rank res\n",
    "    \"\"\"\n",
    "    return np.array(literal_eval(df['post_rank_'+rank_type][line]))\n",
    "\n",
    "\n",
    "def get_hr(df, k, rank_type = 'random_all', lams = [0,1,2,3,4,5,6,7,8,9,10]):\n",
    "    \"\"\"\n",
    "    return one line of the plots \n",
    "    i.e. hr@k for random_all over all lams\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for lam in lams:\n",
    "        res.append(hr_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lambda_vs_performance(df, k = 25):\n",
    "    \"\"\"get hr@k graph for all 6 options\n",
    "    x_axis: lambda from all initial to all modified matrix\n",
    "    y_axis: performance (hr@k)\"\"\"\n",
    "    plt.figure(figsize=(12,12))\n",
    "    # Get lines\n",
    "    pre_hr = get_hr(df, k= k)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), pre_hr_list)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a1)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a2)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a3)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a4)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a5)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a6)\n",
    "\n",
    "    # plt.xticks(np.arange(len(k_list)), k_list)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('HR@K')\n",
    "    plt.legend(['Pre-Critiquing','Random', 'Random_Upper','Pop','Pop_Upper','Diff','Diff_Upper'])\n",
    "    plt.show()\n",
    "    plt.savefig('../figs/three_keyphrase_selection_methods_with_upper_bound_0104')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def hr_at_k(df, k):\n",
    "    pre_hit = np.where(df['pre_rank']<k)[0]\n",
    "    post_hit = np.where(df['post_rank']<k)[0]\n",
    "\n",
    "\n",
    "def hr_at_k(df, k):\n",
    "    \"\"\"\n",
    "    Given the above dataframe, calculate the avg pre and post hit rate at k \n",
    "    \"\"\"\n",
    "    pre_hit = np.where(df['pre_rank']<k)[0]\n",
    "    post_hit = np.where(df['post_rank']<k)[0]\n",
    "    pre_hr = len(pre_hit)/len(df)\n",
    "    post_hr = len(post_hit)/len(df)\n",
    "    return pre_hr, post_hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utlilty func\n",
    "def hr_at_k(df, k):\n",
    "    \"\"\"\n",
    "    Given the above dataframe, calculate the avg pre and post hit rate at k \n",
    "    \"\"\"\n",
    "    pre_hit = np.where(df['pre_rank']<k)[0]\n",
    "    post_hit = np.where(df['post_rank']<k)[0]\n",
    "    pre_hr = len(pre_hit)/len(df)\n",
    "    post_hr = len(post_hit)/len(df)\n",
    "    return pre_hr, post_hr\n",
    "\n",
    "def get_hr(l=5, rang = 200):\n",
    "    \"\"\"\n",
    "    Get the hit rate at different rang, with different lambda value\n",
    "    Output in the form of list\n",
    "    \"\"\"\n",
    "    pre_hr_list = []\n",
    "    post_hr_list = []\n",
    "    for k in range(1,rang):\n",
    "        pre_hr,post_hr = hr_at_k(df,4, k)\n",
    "        pre_hr_list.append(pre_hr)\n",
    "        post_hr_list.append(post_hr)\n",
    "    return pre_hr_list,post_hr_list\n",
    "\n",
    "def get_hr_of_all_lambda(methods):\n",
    "    for method in methods:\n",
    "        post_rates = []\n",
    "        for i in range(9):\n",
    "            _,a = get_hr(l=i)\n",
    "            diff.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pre_hr_list,b = get_hr(l=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_hr_performance():\n",
    "    plt.figure(figsize=(12,12))\n",
    "    for \n",
    "    plt.plot(np.arange(len(pre_hr_list)), pre_hr_list)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a1)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a2)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a3)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a4)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a5)\n",
    "    plt.plot(np.arange(len(pre_hr_list)), a6)\n",
    "\n",
    "    # plt.xticks(np.arange(len(k_list)), k_list)\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('HR@K')\n",
    "    plt.legend(['Pre-Critiquing','Random', 'Random_Upper','Pop','Pop_Upper','Diff','Diff_Upper'])\n",
    "    plt.show()\n",
    "    plt.savefig('../figs/three_keyphrase_selection_methods_with_upper_bound_0104')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
